<!DOCTYPE html><html>
  <head>
    <title>Mastering Kafka Streams and ksqlDB</title>
    <meta charset="UTF-8">
    <style>:root:root {
  --pre-background-color: #eef2f6;
}
a { text-decoration: none }
div.chapter > h1:first-child { font-size: 1.8rem; font-weight: 300; color: #777 }
h1 { font-size: 1.6rem; font-weight: 400; color: #a16223 }
h2 { font-size: 1.4rem; font-weight: 500; color: #2e2bcc }
h3 { font-size: 1.2rem; font-weight: 500; color: #f235e2 }
h6 { font-size: .9rem; font-weight: 400; color: #222 }
thead, tr:nth-child(2n) { background: #EEF2F6 } td { vertical-align: top; padding: .5rem }
td ul, td p { margin: 0 }
blockquote { font-style: italic; color: #555 }
blockquote p[data-type="attribution"] { text-align: right; font-style: normal; width: 82% }
blockquote p[data-type="attribution"]::before { content: "—" }
div[data-type="note"], div[data-type="tip"], aside {
  border-top: .1rem solid #8b889a; border-bottom: .1rem solid #8b889a; padding-left: 1rem; padding-right: 1rem;
  font-family: 'Noto Sans' }
aside h5 { color: #018C8C; font-family: 'Noto Sans'; text-transform: uppercase }
div[data-type="note"] h6, [data-type="tip"] h6 {
  margin-bottom: 0; margin-top: .5rem; color: #018c8c; text-transform: uppercase }
dl { display: grid; grid-template-columns: max-content auto }
dt { display: grid; align-content: center; grid-column-start: 1 }
dd { margin-left: 1rem; grid-column-start: 2 }
code { background: var(--pre-background-color); padding: .25rem .25rem .2rem; font-family: 'Fira Code' }
pre { background: var(--pre-background-color) } pre code { padding: 0 }
code.c1 { color: #999; font-style: italic }
code.k { color: #069 } code.kd { color: #069 } code.kt { color: #078 } code.n { color: #008 }
code.nc { color: #0a8 } code.nf { color: #c0f } code.nl { color: #99f } code.o { color: #666 }
code.s { color: #c30 }
table > caption { white-space: nowrap }
div#toc {
  position: fixed; top: 0; right: 4rem; background: #fff; opacity: .9; z-index: 9;
  max-height: 82vh; overflow: auto; padding-left: 1rem; padding-right: 1rem }
div#toc a { text-decoration: none; color: #333; display: block; }
div#toc a:hover { text-decoration: underline; color: #26f }
div#toc a.h2 { margin-left: 1rem }
div#toc a.h3 { margin-left: 2rem }
div#toc > [type="checkbox"] + div { display: none }
div#toc > [type="checkbox"]:checked + div { display: block }</style>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;family=Noto+Serif:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;family=Open+Sans:wght@300;400;500;600&amp;display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif:ital,wght@0,100;0,800;0,900;1,100;1,800;1,900&amp;family=Open+Sans:wght@300;400;500;600&amp;display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&amp;display=swap" rel="stylesheet">
  </head>
  <body>
    <div class="chapter" id="ch2">
      <h1><span class="label">Chapter 2. </span>Getting Started with Kafka Streams</h1>
      <p>Kafka Streams is a lightweight, yet powerful Java library for enriching, transforming, and processing real-time streams of data.<a data-type="indexterm" data-primary="Kafka Streams" id="ix_KafStr" target="_blank" rel="noopener noreferrer"></a> In this chapter, you will be introduced to Kafka Streams at a high level. Think of it as a first date, where you will learn a little about Kafka Streams’ background and get an initial glance at its features.<a data-type="indexterm" data-primary="streams" data-seealso="Kafka Streams" id="idm46281564870808" target="_blank" rel="noopener noreferrer"></a></p>
      <p>By the end of this date, er…I mean <em>chapter</em>, you will understand the following:</p>
      <ul>
        <li>
          <p>Where Kafka Streams fits in the Kafka ecosystem</p></li>
        <li>
          <p>Why Kafka Streams was built in the first place</p></li>
        <li>
          <p>What kinds of features and operational characteristics are present in this library</p></li>
        <li>
          <p>Who Kafka Streams is appropriate for</p></li>
        <li>
          <p>How Kafka Streams compares to other stream processing solutions</p></li>
        <li>
          <p>How to create and run a basic Kafka Streams application</p></li>
      </ul>
      <p>So without further ado, let’s get our metaphorical date started with a simple question for Kafka Streams: <em>where do you live</em> (…in the Kafka ecosystem)?</p>
      <section data-type="sect1" data-pdf-bookmark="The Kafka Ecosystem">
        <div class="sect1" id="idm46281564862120">
          <h1>The Kafka Ecosystem</h1>
          <p>Kafka Streams lives among a group of technologies that are collectively referred to as the <em>Kafka ecosystem</em>. In <a data-type="xref" href="ch01.html#ch1" target="_blank" rel="noopener noreferrer">Chapter&nbsp;1</a>, we learned that at the heart of Apache Kafka is a distributed, append-only log that we can produce messages to and read messages from.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Kafka ecosystem" id="idm46281564858776" target="_blank" rel="noopener noreferrer"></a> Furthermore, the core Kafka code base includes some important APIs for interacting with this log (which is separated into categories of messages called <em>topics</em>).<a data-type="indexterm" data-primary="APIs" data-secondary="moving data to and from Kafka" id="idm46281564857144" target="_blank" rel="noopener noreferrer"></a> Three APIs in the Kafka ecosystem, which are summarized in <a data-type="xref" href="#table-kafka-ecosystem-apis" target="_blank" rel="noopener noreferrer">Table&nbsp;2-1</a>, are concerned with the <em>movement</em> of data to and from Kafka.</p>
          <table id="table-kafka-ecosystem-apis" class="fix-table-list">
            <caption>
              <span class="label">Table 2-1. </span>APIs for moving data to and from Kafka
            </caption>
            <thead>
              <tr>
                <th>API</th>
                <th>Topic interaction</th>
                <th>Examples</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div>
                    <p>Producer API</p>
                  </div></td>
                <td>
                  <div>
                    <p><em>Writing</em> messages to <a data-type="indexterm" data-primary="Producer API" id="idm46281564811144" target="_blank" rel="noopener noreferrer"></a>Kafka topics.</p>
                  </div></td>
                <td>
                  <div>
                    <ul>
                      <li>
                        <p>Filebeat</p></li>
                      <li>
                        <p>rsyslog</p></li>
                      <li>
                        <p>Custom producers</p></li>
                    </ul>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>Consumer API</p>
                  </div></td>
                <td>
                  <div>
                    <p><em>Reading</em> messages <a data-type="indexterm" data-primary="Consumer API" id="idm46281564804408" target="_blank" rel="noopener noreferrer"></a>from Kafka topics.</p>
                  </div></td>
                <td>
                  <div>
                    <ul>
                      <li>
                        <p>Logstash</p></li>
                      <li>
                        <p>kafkacat</p></li>
                      <li>
                        <p>Custom consumers</p></li>
                    </ul>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>Connect API</p>
                  </div></td>
                <td>
                  <div>
                    <p><em>Connecting</em> external data stores, APIs, and <a data-type="indexterm" data-primary="Connect API" id="idm46281564797512" target="_blank" rel="noopener noreferrer"></a>filesystems to Kafka topics.</p>
                    <p>Involves both <em>reading</em> from topics (sink connectors) and <em>writing</em> to topics (source connectors).</p>
                  </div></td>
                <td>
                  <div>
                    <ul>
                      <li>
                        <p>JDBC source connector</p></li>
                      <li>
                        <p>Elasticsearch sink connector</p></li>
                      <li>
                        <p>Custom connectors</p></li>
                    </ul>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>However, while moving data through Kafka is certainly important for creating data pipelines, some business problems require us to also <em>process</em> and <em>react</em> to data as it becomes available in Kafka.<a data-type="indexterm" data-primary="stream processing" id="idm46281564789976" target="_blank" rel="noopener noreferrer"></a> This is referred to as <em>stream processing</em>, and there are multiple ways of building stream processing applications with Kafka. Therefore, let’s take a look at how stream processing applications were implemented before Kafka Streams was introduced, and how a dedicated stream processing library came to exist alongside the other APIs in the Kafka ecosystem.</p>
          <section data-type="sect2" data-pdf-bookmark="Before Kafka Streams">
            <div class="sect2" id="idm46281564788248">
              <h2>Before Kafka Streams</h2>
              <p>Before Kafka Streams existed, there was a void in the Kafka ecosystem.<sup><a data-type="noteref" id="idm46281564786680-marker" href="ch02.html#idm46281564786680" target="_blank" rel="noopener noreferrer">1</a></sup> Not the kind of void you <a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Kafka ecosystem" data-tertiary="before Kafka Streams" id="idm46281564785784" target="_blank" rel="noopener noreferrer"></a>might encounter during your morning meditation that makes you feel refreshed and enlightened, but the kind of void that made building stream processing applications more difficult than it needed to be. I’m talking about the lack of library support for processing data in Kafka topics.</p>
              <p>During these early days of the Kafka ecosystem, there were two main options for building<a data-type="indexterm" data-primary="stream processing" data-secondary="before Kafka Streams" id="idm46281564783640" target="_blank" rel="noopener noreferrer"></a> Kafka-based stream processing applications:</p>
              <ul>
                <li>
                  <p>Use the Consumer and Producer APIs directly</p></li>
                <li>
                  <p>Use another stream processing framework (e.g., Apache Spark Streaming, Apache Flink)</p></li>
              </ul>
              <p>With the Consumer and Producer APIs, you can read from and write to the event stream directly using a number of programming languages (Python, Java, Go, C/C++, Node.js, etc.) and perform <a data-type="indexterm" data-primary="programming languages supported by Kafka Consumer and Producer APIs" id="idm46281564779480" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Consumer API" data-secondary="using directly before Kafka Streams" id="idm46281564778808" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Producer API" data-secondary="using directly before Kafka Streams" id="idm46281564777848" target="_blank" rel="noopener noreferrer"></a>any kind of data processing logic you’d like, as long <span class="keep-together">as you’re</span> willing to write a lot of code from scratch. These APIs are very basic and <span class="keep-together">lack many</span> of the primitives that would qualify them as a stream processing API, <span class="keep-together">including</span>:</p>
              <ul>
                <li>
                  <p>Local and fault-tolerant state<sup><a data-type="noteref" id="idm46281564773768-marker" href="ch02.html#idm46281564773768" target="_blank" rel="noopener noreferrer">2</a></sup></p></li>
                <li>
                  <p>A rich set of operators for transforming streams of data</p></li>
                <li>
                  <p>More advanced representations of streams<sup><a data-type="noteref" id="idm46281564770600-marker" href="ch02.html#idm46281564770600" target="_blank" rel="noopener noreferrer">3</a></sup></p></li>
                <li>
                  <p>Sophisticated handling of time<sup><a data-type="noteref" id="idm46281564769208-marker" href="ch02.html#idm46281564769208" target="_blank" rel="noopener noreferrer">4</a></sup></p></li>
              </ul>
              <p>Therefore, if you want to do anything nontrivial, like aggregate records, join separate streams of data, group events into windowed time buckets, or run ad hoc queries against your stream, you will hit a wall of complexity pretty quickly. The Producer and Consumer APIs do not contain any abstractions to help you with these kinds of use cases, so you will be left to your own devices as soon as it’s time to do something more advanced with your event stream.</p>
              <p>The second option, which <a data-type="indexterm" data-primary="stream processing" data-secondary="using another stream processing framework" id="idm46281564766216" target="_blank" rel="noopener noreferrer"></a>involves adopting a full-blown streaming platform like Apache Spark or Apache Flink, introduces a lot of unneeded complexity.<a data-type="indexterm" data-primary="Apache Spark" id="idm46281564764872" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Apache Flink" id="idm46281564764200" target="_blank" rel="noopener noreferrer"></a> We talk about the downsides of this approach in <a data-type="xref" href="#comparison" target="_blank" rel="noopener noreferrer">“Comparison to Other Systems”</a>, but the short version is that if we’re optimizing for simplicity <em>and</em> power, then we need a solution that gives us the stream processing primitives without the overhead of a processing cluster. We also need better integration with Kafka, especially when it comes to working with intermediate representations of data outside of the source and sink topics.</p>
              <p>Fortunately for us, the Kafka community recognized the need for a stream processing API in the Kafka ecosystem and decided to build it.<sup><a data-type="noteref" id="idm46281564761352-marker" href="ch02.html#idm46281564761352" target="_blank" rel="noopener noreferrer">5</a></sup></p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Enter Kafka Streams">
            <div class="sect2" id="idm46281564759336">
              <h2>Enter Kafka Streams</h2>
              <p>In 2016, the Kafka ecosystem was forever changed when the first version of Kafka Streams (also called the <em>Streams API</em>) was released.<a data-type="indexterm" data-primary="Streams API" data-seealso="Kafka Streams" id="idm46281564757256" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Kafka ecosystem" data-tertiary="inception of Kafka Streams" id="idm46281564756248" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stream processing" data-secondary="using Kafka Streams" id="idm46281564755016" target="_blank" rel="noopener noreferrer"></a> With its inception, the landscape of stream processing applications that relied so heavily on hand-rolled features gave way to more advanced applications, which leveraged community-developed patterns and abstractions for transforming and processing real-time event streams.</p>
              <p>Unlike the Producer, Consumer, and Connect APIs, Kafka Streams is dedicated to helping you <em>process</em> real-time data streams, not just <em>move</em> data to and from Kafka.<sup><a data-type="noteref" id="idm46281564752104-marker" href="ch02.html#idm46281564752104" target="_blank" rel="noopener noreferrer">6</a></sup> <span class="keep-together">It makes</span> it easy to consume real-time streams of events as they move through our data pipeline, apply data transformation logic using a rich set of stream processing operators and primitives, and optionally write new representations of the data back <span class="keep-together">to Kafka</span> (i.e., if we want to make the transformed or enriched events available to <span class="keep-together">downstream</span> systems).</p>
              <p><a data-type="xref" href="#C2_ARCH2" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-1</a> depicts the previously discussed APIs in the Kafka ecosystem, with Kafka Streams operating at the stream processing layer.</p>
              <figure>
                <div id="C2_ARCH2" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0201.png" style="width: 36rem">
                  <h6><span class="label">Figure 2-1. </span>Kafka Streams is the “brain” of the Kafka ecosystem, consuming records from the event stream, processing the data, and optionally writing enriched or transformed records back to Kafka</h6>
                </div>
              </figure>
              <p>As you can see from the diagram in <a data-type="xref" href="#C2_ARCH2" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-1</a>, Kafka Streams operates at an exciting layer of the Kafka ecosystem: the place where data from many sources converges. This is the layer where sophisticated data <em>enrichment</em>, <em>transformation</em>, and <em>processing</em> can happen. It’s the same place where, in a pre–Kafka Streams world, we would have tediously written our own stream processing abstractions (using the Consumer/Producer API approach) or absorbed a complexity hit by using another framework. Now, let’s get a first look at the features of Kafka Streams that allow us to operate at this layer in a fun and efficient way.</p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Features at a Glance">
        <div class="sect1" id="idm46281564742024">
          <h1>Features at a Glance</h1>
          <p>Kafka Streams offers many features that make it an excellent choice for modern stream processing applications.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="features" id="idm46281564740488" target="_blank" rel="noopener noreferrer"></a> We’ll be going over these in detail in the following chapters, but here are just a few of the features you can look forward to:</p>
          <ul>
            <li>
              <p>A high-level DSL that looks and feels like Java’s streaming API. The DSL provides a fluent and functional approach to processing data streams that is easy to learn and use.</p></li>
            <li>
              <p>A low-level Processor API that gives developers fine-grained control when they need it.</p></li>
            <li>
              <p>Convenient abstractions for modeling data as either streams or tables.</p></li>
            <li>
              <p>The ability to join streams and tables, which is useful for data transformation and enrichment.</p></li>
            <li>
              <p>Operators and utilities for building both stateless and stateful stream processing applications.</p></li>
            <li>
              <p>Support for time-based operations, including windowing and periodic functions.</p></li>
            <li>
              <p>Easy installation. It’s just a library, so you can add Kafka Streams to any Java application.<sup><a data-type="noteref" id="idm46281564732280-marker" href="ch02.html#idm46281564732280" target="_blank" rel="noopener noreferrer">7</a></sup></p></li>
            <li>
              <p>Scalability, reliability, maintainability.</p></li>
          </ul>
          <p>As you begin exploring these features in this book, you will quickly see why this library is so widely used and loved. Both the high-level DSL and low-level Processor API are not only easy to learn, but are also extremely powerful. Advanced stream processing tasks (such as joining live, moving streams of data) can be accomplished with very little code, which makes the development experience truly <span class="keep-together">enjoyable</span>.</p>
          <p>Now, the last bullet point pertains to the long-term stability of our stream processing applications. After all, many technologies are exciting to learn in the beginning, but what really counts is whether or not Kafka Streams will continue to be a good choice as our relationship gets more complicated through real-world, long-term usage of this library. Therefore, it makes sense to evaluate the long-term viability of Kafka Streams before getting too far down the rabbit hole. So how should we go about doing this? Let’s start by looking at Kafka Streams’ operational characteristics.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Operational Characteristics">
        <div class="sect1" id="idm46281564727832">
          <h1>Operational Characteristics</h1>
          <p>In Martin Kleppmann’s excellent book, <em>Designing Data-Intensive Applications</em> (O’Reilly), the author<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="operational characteristics" id="idm46281564725736" target="_blank" rel="noopener noreferrer"></a> highlights three important goals for data systems:</p>
          <ul>
            <li>
              <p>Scalability</p></li>
            <li>
              <p>Reliability</p></li>
            <li>
              <p>Maintainability</p></li>
          </ul>
          <p>These goals provide a useful framework for evaluating Kafka Streams, so in this <span class="keep-together">section</span>, we will define these terms and discover how Kafka Streams achieves each <span class="keep-together">of them.</span></p>
          <section data-type="sect2" data-pdf-bookmark="Scalability">
            <div class="sect2" id="idm46281564719640">
              <h2>Scalability</h2>
              <p>Systems are considered <em>scalable</em> when they can cope and remain performant as load increases.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="operational characteristics" data-tertiary="scalability" id="idm46281564717624" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="scalability of Kafka Streams" id="idm46281564716360" target="_blank" rel="noopener noreferrer"></a> In <a data-type="xref" href="ch01.html#ch1" target="_blank" rel="noopener noreferrer">Chapter&nbsp;1</a>, we learned that scaling Kafka topics involves adding more partitions and, when needed, more Kafka brokers (the latter is only needed if the topic grows beyond the existing capacity of your Kafka cluster).</p>
              <p>Similarly, in Kafka Streams, the unit of work is a single topic-partition, and Kafka automatically distributes work to groups of cooperating consumers called consumer groups.<sup><a data-type="noteref" id="idm46281564713912-marker" href="ch02.html#idm46281564713912" target="_blank" rel="noopener noreferrer">8</a></sup> This has two important implications:</p>
              <ul>
                <li>
                  <p>Since the unit of work in Kafka Streams is a single topic-partition, and since <span class="keep-together">topics</span> can be expanded by adding more partitions, the amount of work a <span class="keep-together">Kafka</span> Streams application can undertake can be scaled by increasing the number of partitions on the source topics.<sup><a data-type="noteref" id="idm46281564710536-marker" href="ch02.html#idm46281564710536" target="_blank" rel="noopener noreferrer">9</a></sup></p></li>
                <li>
                  <p>By leveraging consumer groups, the total amount of work being handled by a Kafka Streams application can be distributed across multiple, cooperating instances of your application.</p></li>
              </ul>
              <p class="pagebreak-before">A quick note about the second point. When you deploy a Kafka Streams application, you will almost always deploy multiple application instances, each handling a subset of the work (e.g., if your source topic has 32 partitions, then you have 32 units of work that can be distributed across all cooperating consumers). For example, you could deploy four application instances, each handling eight partitions (<code>4 * 8 = 32</code>), or you could just as easily deploy sixteen instances of your application, each handling two partitions (<code>16 * 2 = 32</code>).</p>
              <p>However, regardless of how many application instances you end up deploying, Kafka Streams’ ability to cope with increased load by adding more partitions (units of work) and application instances (workers) makes Kafka Streams <em>scalable</em>.</p>
              <p>On a similar note, Kafka Streams is also <em>elastic</em>, allowing you to seamlessly (albeit manually) scale the <a data-type="indexterm" data-primary="elasticity (Kafka Streams)" id="idm46281564704440" target="_blank" rel="noopener noreferrer"></a>number of application instances in or out, with a limit on the scale-out path being the number of tasks that are created for your topology. We’ll discuss tasks in more detail in <a data-type="xref" href="#C2_STREAM_THREADS" target="_blank" rel="noopener noreferrer">“Tasks and Stream Threads”</a>.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Reliability">
            <div class="sect2" id="idm46281564702424">
              <h2>Reliability</h2>
              <p>Reliability is an <a data-type="indexterm" data-primary="reliability of Kafka Streams" id="idm46281564701096" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="operational characteristics" data-tertiary="reliability" id="idm46281564700392" target="_blank" rel="noopener noreferrer"></a>important feature of data systems, not only from an engineering perspective (we don’t want to be woken up at 3 a.m. due to some fault in the system), but also from our customers’ perspective (we don’t want the system to go offline in any noticeable way, and we certainly can’t tolerate data loss or corruption). Kafka Streams comes with a few fault-tolerant features,<sup><a data-type="noteref" id="idm46281564698648-marker" href="ch02.html#idm46281564698648" target="_blank" rel="noopener noreferrer">10</a></sup> but the most obvious one is something we’ve already touched on in <a data-type="xref" href="ch01.html#C1_CG" target="_blank" rel="noopener noreferrer">“Consumer Groups”</a>: automatic failovers and partition rebalancing via consumer groups.</p>
              <p>If you deploy multiple instances of your Kafka Streams application and one goes offline due to some fault in the system (e.g., a hardware failure), then Kafka will automatically redistribute the load to the other healthy instances.<a data-type="indexterm" data-primary="faults, handling by Kafka Streams" id="idm46281564695256" target="_blank" rel="noopener noreferrer"></a> When the failure is resolved (or, in more modern architectures that leverage an orchestration system like Kubernetes, when the application is moved to a healthy node), then Kafka will rebalance the work again. This ability to gracefully handle faults makes Kafka Streams <em>reliable</em>.</p>
            </div>
          </section>
          <section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Maintainability">
            <div class="sect2" id="idm46281564693432">
              <h2>Maintainability</h2>
              <blockquote>
                <p>It is well known that the majority of the cost of software is not in its initial development, but in its<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="operational characteristics" data-tertiary="maintainability" id="idm46281564691160" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="maintainability of Kafka Streams" id="idm46281564689896" target="_blank" rel="noopener noreferrer"></a> ongoing maintenance—fixing bugs, keeping its systems operational, investigating failures…</p>
                <p data-type="attribution">Martin Kleppmann</p>
              </blockquote>
              <p>Since Kafka Streams is a Java library, troubleshooting<a data-type="indexterm" data-primary="Java" data-secondary="Kafka Streams library" id="idm46281564687672" target="_blank" rel="noopener noreferrer"></a> and fixing bugs is relatively straightforward since we’re working with standalone applications, and patterns for both troubleshooting and monitoring Java applications are well established and may already be in use at your organization (collecting and analyzing application logs, capturing application and JVM metrics, profiling and tracing, etc.).</p>
              <p>Furthermore, since the Kafka Streams API is succinct and intuitive, code-level maintenance is less time-consuming than one would expect with more complicated libraries, and is very easy to understand for beginners and experts alike. If you build a Kafka Streams application and then don’t touch it for months, you likely won’t suffer the usual project amnesia and require a lot of ramp-up time to understand the previous code you’ve written. For the same reasons, new project maintainers can typically get up to speed pretty quickly with a Kafka Streams application, which improves maintainability even further.</p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Comparison to Other Systems">
        <div class="sect1" id="comparison">
          <h1>Comparison to Other Systems</h1>
          <p>By this point, you should be starting to feel comfortable about starting a long-term relationship with Kafka Streams.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="comparison to other systems" id="idm46281564683112" target="_blank" rel="noopener noreferrer"></a> But before things get too serious, isn’t it natural to see if there are other fish in the sea?</p>
          <p>Actually, it is sometimes difficult to evaluate how good a technology is without knowing how it stacks up against its competitors. So let’s take a look at how Kafka Streams compares to some other popular technologies in the stream processing space.<sup><a data-type="noteref" id="idm46281564681272-marker" href="ch02.html#idm46281564681272" target="_blank" rel="noopener noreferrer">11</a></sup> We’ll start by comparing Kafka Streams’ deployment model with other popular systems.</p>
          <section data-type="sect2" data-pdf-bookmark="Deployment Model">
            <div class="sect2" id="idm46281564680088">
              <h2>Deployment Model</h2>
              <p>Kafka Streams takes a different approach to stream processing than technologies like Apache Flink and Apache Spark Streaming.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="comparison to other systems" data-tertiary="deployment model" id="idm46281564678648" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="deployments" data-secondary="comparison of Kafka Streams to other systems" id="idm46281564677384" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Apache Flink" id="idm46281564676408" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Apache Spark" id="idm46281564675736" target="_blank" rel="noopener noreferrer"></a> The latter systems require you to set up a dedicated processing cluster for submitting and running your stream processing program. This can introduce a lot of complexity and overhead. Even experienced engineers at well-established companies have conceded that the overhead of a processing cluster is nontrivial. In an interview with Nitin Sharma at Netflix, I learned that it took around six months to adapt to the nuances of Apache Flink and build a highly reliable production Apache Flink application and cluster.</p>
              <p>On the other hand, Kafka Streams is implemented as a Java <em>library</em>, so getting started is much easier since you don’t need to worry about a cluster manager; you simply need to add the Kafka Streams dependency to your Java application. Being able to build a stream processing application as a standalone program also means you have a lot of freedom in terms of how you monitor, package, and deploy your code. For example, at Mailchimp, our Kafka Streams applications are deployed using the same patterns and tooling we use for other internal Java applications. This ability to immediately integrate into your company’s systems is a huge advantage for Kafka Streams.</p>
              <p>Next, let’s explore how Kafka Streams’ processing model compares to other systems in this space.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Processing Model">
            <div class="sect2" id="idm46281564672200">
              <h2>Processing Model</h2>
              <p>Another key differentiator between Kafka Streams and systems like Apache Spark Streaming or Trident <a data-type="indexterm" data-primary="Kafka Streams" data-secondary="comparison to other systems" data-tertiary="processing model" id="idm46281564670296" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing model, Kafka Streams" id="idm46281564669080" target="_blank" rel="noopener noreferrer"></a>is that Kafka Streams implements <em>event-at-a-time processing</em>, so events are processed immediately, one at a time, as they come in.<a data-type="indexterm" data-primary="latency, event-at-a-time processing versus micro-batching" id="idm46281564667832" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="events" data-secondary="event-at-a-time processing with Kafka Streams" id="idm46281564667192" target="_blank" rel="noopener noreferrer"></a> This is considered true streaming and provides lower latency than the alternative approach, which is called <em>micro-batching</em>.<a data-type="indexterm" data-primary="micro-batching" id="idm46281564665640" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="batch processing, micro-batching" id="idm46281564664904" target="_blank" rel="noopener noreferrer"></a> Micro-batching involves grouping records into small groups (or <em>batches</em>), which are buffered in memory and later emitted at some interval (e.g., every 500 milliseconds). <a data-type="xref" href="#C2_MICRO" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-2</a> depicts the difference between event-at-a-time and micro-batch processing.</p>
              <figure>
                <div id="C2_MICRO" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0202.png" style="width: 27rem">
                  <h6><span class="label">Figure 2-2. </span>Micro-batching involves grouping records into small batches and emitting them to downstream processors at a fixed interval; event-at-a-time processing allows each event to be processed at soon as it comes in, instead of waiting for a batch to <span class="keep-together">materialize</span></h6>
                </div>
              </figure>
              <div data-type="note" epub:type="note">
                <h6>Note</h6>
                <p>Frameworks that use micro-batching are often optimized for greater <em>throughput</em> at the cost of higher <em>latency</em>. <a data-type="indexterm" data-primary="throughput and latency in micro-batching frameworks" id="idm46281564658216" target="_blank" rel="noopener noreferrer"></a>In Kafka Streams, you can achieve extremely low latency while also maintaining high throughput by splitting data across many partitions.</p>
              </div>
              <p>Finally, let’s take a look at Kafka Streams’ data processing architecture, and see how its focus on streaming differs from other systems.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Kappa Architecture">
            <div class="sect2" id="idm46281564656360">
              <h2>Kappa Architecture</h2>
              <p>Another important consideration when comparing Kafka Streams to other solutions is whether or not your use case requires support for both batch and stream processing.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="comparison to other systems" data-tertiary="Kappa architecture" id="idm46281564654392" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kappa architecture" id="idm46281564653176" target="_blank" rel="noopener noreferrer"></a> At the time of this writing, Kafka Streams focuses solely on streaming use cases<sup><a data-type="noteref" id="idm46281564652376-marker" href="ch02.html#idm46281564652376" target="_blank" rel="noopener noreferrer">12</a></sup> (this is called a <em>Kappa architecture</em>), while frameworks like Apache Flink and Apache Spark support both batch and stream processing (this is called a <em>Lambda architecture</em>).<a data-type="indexterm" data-primary="Lambda architecture" id="idm46281564650072" target="_blank" rel="noopener noreferrer"></a> However, architectures that support both batch and streaming use cases aren’t without their drawbacks. Jay Kreps <a href="https://oreil.ly/RwkNi" target="_blank" rel="noopener noreferrer">discussed some of the disadvantages of a hybrid system</a> nearly two years before Kafka Streams was introduced into the Kafka <span class="keep-together">ecosystem</span>:</p>
              <blockquote>
                <p>The operational burden of running and debugging two systems is going to be very high. And any new abstraction can only provide the features supported by the intersection of the two systems.</p>
              </blockquote>
              <p>These challenges didn’t stop projects like Apache Beam, which defines a unified programming model for batch and stream processing, from gaining popularity in recent years. But Apache Beam isn’t comparable to Kafka Streams in the same way that Apache Flink is.<a data-type="indexterm" data-primary="Beam" id="idm46281564646264" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Apache Beam" id="idm46281564645560" target="_blank" rel="noopener noreferrer"></a> Instead, Apache Beam is an API layer that relies on an execution engine to do most of the work. For example, both Apache Flink and Apache Spark can be used as execution engines (often referred to as <em>runners</em>) in Apache Beam.<a data-type="indexterm" data-primary="Apache Spark" id="idm46281564644152" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Apache Flink" id="idm46281564643416" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="runners (execution engines) in Apache Beam" id="idm46281564642744" target="_blank" rel="noopener noreferrer"></a> So when you compare Kafka Streams to Apache Beam, you must also consider the execution engine that you plan on using in addition to the Beam API itself.</p>
              <p>Furthermore, Apache Beam–driven pipelines lack some important features that <span class="keep-together">are offered</span> in Kafka Streams. Robert Yokota, who created an experimental <a href="https://oreil.ly/h0Hdz" target="_blank" rel="noopener noreferrer">Kafka Streams Beam Runner</a> and who maintains several innovative projects in the Kafka ecosystem,<sup><a data-type="noteref" id="idm46281564639976-marker" href="ch02.html#idm46281564639976" target="_blank" rel="noopener noreferrer">13</a></sup> puts it this way in his <a href="https://oreil.ly/24zG9" target="_blank" rel="noopener noreferrer">comparison of different streaming frameworks</a>:</p>
              <blockquote class="pagebreak-before">
                <p>One way to state the differences between the two systems is as follows:</p>
                <ul>
                  <li>
                    <p>Kafka Streams is a <a data-type="indexterm" data-primary="Apache Beam" data-secondary="comparison to Kafka Streams" id="idm46281564635288" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="comparison to other systems" data-tertiary="Apache Beam" id="idm46281564634216" target="_blank" rel="noopener noreferrer"></a><em>stream-relational</em> processing platform.</p></li>
                  <li>
                    <p>Apache Beam is a <em>stream-only</em> processing platform.</p></li>
                </ul>
                <p>A stream-relational processing platform has the following capabilities which are typically missing in a stream-only processing platform:</p>
                <ul>
                  <li>
                    <p>Relations (or tables) are first-class citizens, i.e., each has an independent identity.</p></li>
                  <li>
                    <p>Relations can be transformed into other relations.</p></li>
                  <li>
                    <p>Relations can be queried in an ad-hoc manner.</p></li>
                </ul>
              </blockquote>
              <p>We will demonstrate each of the bulleted features over the next several chapters, but for now, suffice it to say that many of Kafka Streams’ most powerful features (including the ability to query the state of a stream) are not available in Apache Beam or other more generalized frameworks.<sup><a data-type="noteref" id="idm46281564626536-marker" href="ch02.html#idm46281564626536" target="_blank" rel="noopener noreferrer">14</a></sup> Furthermore, the Kappa architecture offers a simpler and more specialized approach for working with streams of data, which can improve the development experience and simplify the operation and maintenance of your software. So if your use case doesn’t require batch processing, then hybrid systems will introduce unnecessary complexity.</p>
              <p>Now that we’ve looked at the competition, let’s look at some Kafka Streams use cases.</p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Use Cases">
        <div class="sect1" id="idm46281564624232">
          <h1>Use Cases</h1>
          <p>Kafka Streams is optimized for processing unbounded datasets quickly and efficiently, and is therefore a great solution for problems in low-latency, time-critical domains.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="use cases" id="idm46281564622472" target="_blank" rel="noopener noreferrer"></a> A few example use cases include:</p>
          <ul>
            <li>
              <p>Financial data processing (<a href="https://oreil.ly/dAcbY" target="_blank" rel="noopener noreferrer">Flipkart</a>), purchase monitoring, fraud detection</p></li>
            <li>
              <p>Algorithmic trading</p></li>
            <li>
              <p>Stock market/crypto exchange monitoring</p></li>
            <li>
              <p>Real-time inventory tracking and replenishment (<a href="https://oreil.ly/VoF76" target="_blank" rel="noopener noreferrer">Walmart</a>)</p></li>
            <li>
              <p>Event booking, seat selection (<a href="https://oreil.ly/V4t1h" target="_blank" rel="noopener noreferrer">Ticketmaster</a>)</p></li>
            <li>
              <p>Email delivery tracking and monitoring (Mailchimp)</p></li>
            <li>
              <p>Video game telemetry processing (Activision, the publisher of <a href="https://oreil.ly/Skan3" target="_blank" rel="noopener noreferrer"><em>Call of Duty</em></a>)</p></li>
            <li>
              <p>Search indexing (<a href="https://oreil.ly/IhCnC" target="_blank" rel="noopener noreferrer">Yelp</a>)</p></li>
            <li>
              <p>Geospatial tracking/calculations (e.g., distance comparison, arrival projections)</p></li>
            <li>
              <p>Smart Home/IoT sensor processing (sometimes called AIOT, or the Artificial Intelligence of Things)</p></li>
            <li>
              <p>Change data capture (<a href="https://oreil.ly/INs3z" target="_blank" rel="noopener noreferrer">Redhat</a>)</p></li>
            <li>
              <p>Sports broadcasting/real-time widgets (<a href="https://oreil.ly/YeX33" target="_blank" rel="noopener noreferrer">Gracenote</a>)</p></li>
            <li>
              <p>Real-time ad platforms (<a href="https://oreil.ly/cBgSG" target="_blank" rel="noopener noreferrer">Pinterest</a>)</p></li>
            <li>
              <p>Predictive healthcare, vitals monitoring (<a href="https://oreil.ly/4MYLc" target="_blank" rel="noopener noreferrer">Children’s Healthcare of Atlanta</a>)</p></li>
            <li>
              <p>Chat infrastructure (<a href="https://oreil.ly/_n7sZ" target="_blank" rel="noopener noreferrer">Slack</a>), chat bots, virtual assistants</p></li>
            <li>
              <p>Machine learning pipelines (<a href="https://oreil.ly/RuPPV" target="_blank" rel="noopener noreferrer">Twitter</a>) and platforms (<a href="https://oreil.ly/8IHKT" target="_blank" rel="noopener noreferrer">Kafka Graphs</a>)</p></li>
          </ul>
          <p>The list goes on and on, but the common characteristic across all of these examples is that they require (or at least benefit from) <em>real-time decision making</em> or data processing. The spectrum of these use cases, and others you will encounter in the wild, is really quite fascinating. On one end of the spectrum, you may be processing streams at the hobbyist level by analyzing sensor output from a Smart Home device. However, you could also use Kafka Streams in a healthcare setting to monitor and react to changes in a trauma victim’s condition, as Children’s Healthcare of Atlanta has done.</p>
          <p>Kafka Streams is also a great choice for building microservices on top of real-time event streams.<a data-type="indexterm" data-primary="microservices" data-secondary="building using Kafka Streams" id="idm46281564595784" target="_blank" rel="noopener noreferrer"></a> It not only simplifies typical stream processing operations (filtering, joining, windowing, and transforming data), but as you will see in <a data-type="xref" href="ch04.html#C4_IQ" target="_blank" rel="noopener noreferrer">“Interactive Queries”</a>, it is also capable of exposing the state of a stream using a feature called <em>interactive queries</em>. The state of a stream could be an aggregation of some kind (e.g., the total number of views for each video in a streaming platform) or even the latest representation for a rapidly changing entity in your event stream (e.g., the latest stock price for a given stock symbol).</p>
          <p>Now that you have some idea of who is using Kafka Streams and what kinds of use cases it is well suited for, let’s take a quick look at Kafka Streams’ architecture before we start writing any code.</p>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Processor Topologies">
        <div class="sect1" id="C2_SECTION_PROCESSOR">
          <h1>Processor Topologies</h1>
          <p>Kafka Streams leverages a programming paradigm called <em>dataflow programming</em> (DFP), which is a data-centric method of representing programs as a series of inputs, outputs, and processing stages.<a data-type="indexterm" data-primary="dataflow programming (DFP)" id="idm46281564589432" target="_blank" rel="noopener noreferrer"></a> This leads to a very natural and intuitive way of creating stream processing programs and is one of the many reasons I think Kafka Streams is easy to pick up for beginners.</p>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p>This section will dive a little deeper into Kafka Streams architecture. If you prefer to get your feet <a data-type="indexterm" data-primary="processor topologies" id="ix_proctop" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="processor topologies" id="ix_KafStrprc" target="_blank" rel="noopener noreferrer"></a>wet with Kafka Streams and revisit this section later, feel free to proceed to <a data-type="xref" href="#C2_TUTORIAL_HELLO_STREAMS" target="_blank" rel="noopener noreferrer">“Introducing Our Tutorial: Hello, Streams”</a>.</p>
          </div>
          <p>Instead of building a program as a sequence of<a data-type="indexterm" data-primary="directed acyclic graph (DAG), Kafka Streams processing" id="idm46281564583560" target="_blank" rel="noopener noreferrer"></a> steps, the stream processing logic in a Kafka Streams application is structured as a directed acyclic graph (DAG). <a data-type="xref" href="#C2_PROCESSOR" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-3</a> shows an example DAG that depicts how data flows through a set of stream processors. The nodes (the rectangles in the diagram) represent a processing step, or processor, and the edges (the lines connecting the nodes in the diagram) represent input and output streams (where data flows from one processor to another).</p>
          <figure>
            <div id="C2_PROCESSOR" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0203.png" style="width: 22rem">
              <h6><span class="label">Figure 2-3. </span>Kafka Streams borrows some of its design from dataflow programming, and structures stream processing programs as a graph of processors through which data flows</h6>
            </div>
          </figure>
          <p class="pagebreak-before">There are <a data-type="indexterm" data-primary="processor topologies" data-secondary="types of processors in Kafka Streams" id="idm46281564578568" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="processor topologies" data-tertiary="types of processors in Kafka Streams" id="idm46281564577544" target="_blank" rel="noopener noreferrer"></a>three basic kinds of processors in Kafka Streams:</p>
          <dl>
            <dt>
              Source processors
            </dt>
            <dd>
              <p>Sources are where information flows into the Kafka Streams application. <a data-type="indexterm" data-primary="source processors" id="idm46281564574488" target="_blank" rel="noopener noreferrer"></a>Data is read from a Kafka topic and sent to one or more <em>stream processors</em>.</p>
            </dd>
            <dt>
              Stream processors
            </dt>
            <dd>
              <p>These processors are responsible for applying data processing/transformation logic on the input stream. <a data-type="indexterm" data-primary="stream processors" id="idm46281564571576" target="_blank" rel="noopener noreferrer"></a>In the high-level DSL, these processors are defined using a set of built-in <em>operators</em> that are exposed by the <a data-type="indexterm" data-primary="operators" data-secondary="in Kafka Streams" data-secondary-sortas="Kafka" id="idm46281564570360" target="_blank" rel="noopener noreferrer"></a>Kafka Streams library, which we will be going over in detail in the following chapters. Some example operators are <code>filter</code>, <code>map</code>, <code>flatMap</code>, and <code>join</code>.</p>
            </dd>
            <dt>
              Sink processors
            </dt>
            <dd>
              <p>Sinks are where enriched, transformed, filtered, or otherwise processed records are written <em>back</em> to Kafka, either to be handled by another stream processing application or to be sent to a downstream data store via something like Kafka Connect.<a data-type="indexterm" data-primary="sink processors" id="idm46281564565224" target="_blank" rel="noopener noreferrer"></a> Like source processors, sink processors are connected to a Kafka topic.</p>
            </dd>
          </dl>
          <p>A collection of processors forms a <em>processor topology</em>, which is often referred to as simply <em>the topology</em> in both this book and the wider Kafka Streams community. <a data-type="indexterm" data-primary="topology (processor)" id="idm46281564562952" target="_blank" rel="noopener noreferrer"></a>As we go through each tutorial in this part of the book, we will first design the topology by creating a DAG that connects the source, stream, and sink processors. Then, we will simply implement the topology by writing some Java code. To demonstrate this, let’s go through the exercise of translating some project requirements into a processor topology (represented by a DAG). This will help you learn how to <em>think</em> like a Kafka Streams developer.</p>
          <aside data-type="sidebar" epub:type="sidebar">
            <div class="sidebar" id="idm46281564561128">
              <h5>Scenario</h5>
              <p>Say we are building a chatbot with Kafka Streams, and we have a topic named <code>slack-mentions</code> that contains every <a data-type="indexterm" data-primary="chatbot example using Kafka Streams" id="idm46281564559144" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="chatbot example" id="idm46281564558344" target="_blank" rel="noopener noreferrer"></a>Slack message that mentions our bot, <code>@StreamsBot</code>. We will design our bot so that it expects each mention to be followed by a command, like <code>@StreamsBot restart myservice</code>.</p>
              <p>We want to implement a basic processor topology that does some preprocessing/validation of these Slack messages. First, we need to consume each message in the source topic, determine if the command is valid, and if so, write the Slack message to a topic called <code>valid-mentions</code>. If the command is not valid (e.g., someone makes a spelling error when mentioning our bot, such as <code>@StreamsBot restart serverrr</code>), then we will write to a topic named <code>invalid-mentions</code>).</p>
              <p>In this case, we would translate these requirements to the topology shown in <a data-type="xref" href="#C2_TOPOLOGY_1" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-4</a>.</p>
              <figure>
                <div id="C2_TOPOLOGY_1" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0204.png" style="width: 24rem">
                  <h6><span class="label">Figure 2-4. </span>An example processor topology that contains a single source processor for reading Slack messages from Kafka (<code>slack-mentions</code>), a single stream processor that checks the validity of each message (<code>isValid</code>), and two sink processors that route the message to one of two output topics based on the previous check (<code>valid-mentions</code>, <code>invalid-mentions</code>)</h6>
                </div>
              </figure>
            </div>
          </aside>
          <p>Starting with the tutorial in the next chapter, we will then begin to implement any topology we design using the Kafka Streams API. But first, let’s take a look at a related concept: sub-topologies.</p>
          <section data-type="sect2" data-pdf-bookmark="Sub-Topologies">
            <div class="sect2" id="C2_SUBTOPOLOGIES">
              <h2>Sub-Topologies</h2>
              <p>Kafka Streams also has the notion of sub-topologies.<a data-type="indexterm" data-primary="processor topologies" data-secondary="sub-topologies" id="idm46281564546456" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="processor topologies" data-tertiary="sub-topologies" id="idm46281564545480" target="_blank" rel="noopener noreferrer"></a> In the previous example, we designed a processor topology that consumes events from a single source topic (<code>slack-mentions</code>) and performs some preprocessing on a stream of raw chat messages. However, if our application needs to consume from multiple source topics, then Kafka Streams will (under most circumstances<sup><a data-type="noteref" id="idm46281564543416-marker" href="ch02.html#idm46281564543416" target="_blank" rel="noopener noreferrer">15</a></sup>) divide our topology into smaller sub-topologies to parallelize the work even further. This division of work is possible since operations on one input stream can be executed independently of operations on another input stream.</p>
              <p>For example, let’s keep building our chatbot by adding <a data-type="indexterm" data-primary="chatbot example using Kafka Streams" data-secondary="adding stream processors" id="idm46281564540792" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stream processors" data-secondary="adding to chatbox example" id="idm46281564539784" target="_blank" rel="noopener noreferrer"></a>two new stream processors: one that consumes from the <code>valid-mentions</code> topic and performs whatever command was issued to <code>StreamsBot</code> (e.g., <code>restart server</code>), and another processor that consumes from the <code>invalid-mentions</code> topic and posts an error response back to Slack.<sup><a data-type="noteref" id="idm46281564536776-marker" href="ch02.html#idm46281564536776" target="_blank" rel="noopener noreferrer">16</a></sup></p>
              <p>As you can see in <a data-type="xref" href="#C2_SUBTOPOLOGY_1" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-5</a>, our topology now has three Kafka topics it reads from: <code>slack-mentions</code>, <code>valid-mentions</code>, and <code>invalid-mentions</code>. Each time we read from a new source topic, Kafka Streams divides the topology into smaller sections that it can execute independently. In this example, we end up with three sub-topologies for our chatbot application, each denoted by a star in the figure.</p>
              <figure>
                <div id="C2_SUBTOPOLOGY_1" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0205.png" style="width: 22rem">
                  <h6><span class="label">Figure 2-5. </span>A processor topology, subdivided into sub-topologies (demarcated by dotted lines)</h6>
                </div>
              </figure>
              <p>Notice that both the <code>valid-mentions</code> and <code>invalid-mentions</code> topics serve as a sink processor in the first sub-topology, but as a source processor in the second and third sub-topologies.<a data-type="indexterm" data-primary="sink processors" id="idm46281564528968" target="_blank" rel="noopener noreferrer"></a> When this occurs, there is no direct data exchange between sub-topologies. <a data-type="indexterm" data-primary="source processors" id="idm46281564528136" target="_blank" rel="noopener noreferrer"></a>Records are produced to Kafka in the sink processor, and reread from Kafka by the source processors.</p>
              <p>Now that we understand how to represent a stream processing program as a processor topology, let’s take a look at how data actually flows through the interconnected processors in a Kafka Streams application.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Depth-First Processing">
            <div class="sect2" id="idm46281564526488">
              <h2>Depth-First Processing</h2>
              <p>Kafka Streams uses a depth-first strategy when processing data.<a data-type="indexterm" data-primary="processor topologies" data-secondary="depth-first processing" id="idm46281564524920" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="processor topologies" data-tertiary="depth-first processing" id="idm46281564523944" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="depth-first processing" id="idm46281564522728" target="_blank" rel="noopener noreferrer"></a> When a new record is received, it is routed through each stream processor in the topology before another record is processed. The flow of data through Kafka Streams looks something like what’s shown in <a data-type="xref" href="#C2_DFIRST_1" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-6</a>.</p>
              <figure>
                <div id="C2_DFIRST_1" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0206.png" alt="Kafka Streams abstraction levels" style="width: 19.4rem">
                  <h6><span class="label">Figure 2-6. </span>In depth-first processing, a single record moves through the entire topology before another record is processed</h6>
                </div>
              </figure>
              <p class="pagebreak-before">This depth-first strategy makes the dataflow much easier to reason about, but also means that slow stream processing operations can block other records from being processed in the same thread. <a data-type="xref" href="#C2_DFIRST_2" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-7</a> demonstrates something you will never see happen in Kafka Streams: multiple records going through a topology at once.</p>
              <figure>
                <div id="C2_DFIRST_2" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0207.png" alt="Kafka Streams abstraction levels" style="width: 27rem">
                  <h6><span class="label">Figure 2-7. </span>Multiple records will never go through the topology at the same time</h6>
                </div>
              </figure>
              <div data-type="note" epub:type="note">
                <h6>Note</h6>
                <p>When multiple sub-topologies are in play, the single-event rule does not apply to the entire topology, but to each sub-topology.</p>
              </div>
              <p>Now that we know how to design processor topologies and how data flows through them, let’s take a look at the advantages of this data-centric approach to building stream processing applications.</p>
            </div>
          </section>
          <section data-type="sect2" class="less_space" data-pdf-bookmark="Benefits of Dataflow Programming">
            <div class="sect2" id="idm46281564513048">
              <h2>Benefits of Dataflow Programming</h2>
              <p>There are several advantages of using Kafka Streams and the dataflow programming model for building stream processing applications.<a data-type="indexterm" data-primary="processor topologies" data-secondary="benefits of dataflow processing" id="idm46281564511256" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="dataflow programming (DFP)" data-secondary="benefits of" id="idm46281564510216" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="processor topologies" data-tertiary="benefits of dataflow programming" id="idm46281564509256" target="_blank" rel="noopener noreferrer"></a> First, representing the program as a directed graph makes it easy to reason about. You don’t need to follow a bunch of conditionals and control logic to figure out how data is flowing through your application. Simply find the source and sink processors to determine where data enters and exits your program, and look at the stream processors in between to discover how the data is being processed, transformed, and enriched along the way.<a data-type="indexterm" data-primary="DFP" data-see="dataflow programming" id="idm46281564507432" target="_blank" rel="noopener noreferrer"></a></p>
              <p>Furthermore, expressing our stream processing program as a directed graph allows us to standardize the way we frame real-time data processing problems and, subsequently, the way we build our streaming solutions.<a data-type="indexterm" data-primary="directed acyclic graph (DAG), Kafka Streams processing" id="idm46281564505880" target="_blank" rel="noopener noreferrer"></a> A Kafka Streams application that I write will have some level of familiarity to anyone who has worked with Kafka Streams before in their own projects—not only due to the reusable abstractions available in the library itself, but also thanks to a common problem-solving approach: defining the flow of data using operators (nodes) and streams (edges). Again, this makes Kafka Streams applications easy to reason about and maintain.</p>
              <p>Directed graphs are also an intuitive way of visualizing the flow of data for non-technical stakeholders. There is often a disconnect about how a program works between engineering teams and nonengineering teams. Sometimes, this leads to nontechnical teams treating the software as a closed box. This can have dangerous side effects, especially in the age of data privacy laws and GDPR compliance, which requires close coordination between engineers, legal teams, and other stakeholders. Thus, being able to simply communicate how data is being processed in your application allows people who are focused on another aspect of a business problem to understand or even contribute to the design of your application.</p>
              <p>Finally, the processor topology, which contains the source, sink, and stream processors, acts as a <em>template</em> that can be instantiated and parallelized very easily across multiple threads and application instances. Therefore, defining the dataflow in this way allows us to realize performance and scalability benefits since we can easily replicate our stream processing program when data volume demands it.</p>
              <p>Now, to understand how this process of replicating topologies works, we first need to understand the relationship between tasks, stream threads, and partitions.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Tasks and Stream Threads">
            <div class="sect2" id="C2_STREAM_THREADS">
              <h2>Tasks and Stream Threads</h2>
              <p>When we define a topology in Kafka Streams, we are not actually executing the program.<a data-type="indexterm" data-primary="processor topologies" data-secondary="tasks and stream threads" id="idm46281564499224" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="processor topologies" data-tertiary="tasks and stream threads" id="idm46281564498280" target="_blank" rel="noopener noreferrer"></a> Instead, we are building a template for how data should flow through our application.<a data-type="indexterm" data-primary="tasks" data-secondary="and stream threads" data-secondary-sortas="stream" id="idm46281564496920" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stream threads" id="idm46281564495704" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="threads (stream)" id="idm46281564495032" target="_blank" rel="noopener noreferrer"></a> This template (our topology) can be instantiated multiple times in a single application instance, and parallelized across many <em>tasks</em> and <em>stream threads</em> (which we’ll refer to as simply threads going forward.<sup><a data-type="noteref" id="idm46281564493256-marker" href="ch02.html#idm46281564493256" target="_blank" rel="noopener noreferrer">17</a></sup>) There is a close relationship between the number of tasks/threads and the amount of work your stream processing application can handle, so understanding the content in this section is especially important for achieving good performance with Kafka Streams.</p>
              <p>Let’s start by looking at tasks:</p>
              <blockquote>
                <p>A task is the smallest unit of work that can be performed in parallel in a Kafka Streams application…</p>
                <p>Slightly simplified, the maximum parallelism at which your application may run is bounded by the maximum number of stream tasks, which itself is determined by the maximum number of partitions of the input topic(s) the application is reading from.</p>
                <p data-type="attribution">Andy Bryant</p>
              </blockquote>
              <p>Translating this quote into a formula, we can calculate the number of tasks that can be created for a given Kafka Streams sub-topology<sup><a data-type="noteref" id="idm46281564488888-marker" href="ch02.html#idm46281564488888" target="_blank" rel="noopener noreferrer">18</a></sup> with the following math:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">max(source_topic_1_partitions, ... source_topic_n_partitions)</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>For example, if your topology reads from one source topic that contains <code>16</code> partitions, then Kafka Streams will create <code>16</code> tasks, each of which will instantiate its own copy of the underlying processor topology. Once Kafka Streams has created all of the tasks, it will assign the source partitions to be read from to each task.</p>
              <p>As you can see, tasks are just logical units that are used to instantiate and run a processor topology. <em>Threads</em>, on the other hand, are what actually execute the task. In Kafka Streams, the stream threads are designed to be isolated and thread-safe.<sup><a data-type="noteref" id="idm46281564484520-marker" href="ch02.html#idm46281564484520" target="_blank" rel="noopener noreferrer">19</a></sup> <span class="keep-together">Furthermore</span>, unlike tasks, there isn’t any formula that Kafka Streams applies to figure out how many threads your application should run. Instead, you are responsible <span class="keep-together">for specifying</span> the thread count using a configuration property named <span class="keep-together"><code>num.stream.threads</code></span>. The upper bound for the number of threads you can utilize corresponds to the task count, and there are different strategies for deciding on the number of stream threads you should run with.<sup><a data-type="noteref" id="idm46281564481048-marker" href="ch02.html#idm46281564481048" target="_blank" rel="noopener noreferrer">20</a></sup></p>
              <p>Now, let’s improve our understanding of these concepts by visualizing how tasks and threads are created using two separate configs, each specifying a different number of threads. In each example, our Kafka Streams application is reading from a source topic that contains four partitions (denoted by <code>p1</code> - <code>p4</code> in <a data-type="xref" href="#C2_ARCH_1" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-8</a>).</p>
              <p>First, let’s configure our application to run with two threads (<code>num.stream.threads = 2</code>). Since our source topic has four partitions, four tasks will be created and distributed across each thread. We end up with the task/thread layout depicted in <a data-type="xref" href="#C2_ARCH_1" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-8</a>.</p>
              <figure>
                <div id="C2_ARCH_1" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0208.png" style="width: 27rem">
                  <h6><span class="label">Figure 2-8. </span>Four Kafka Streams tasks running in two threads</h6>
                </div>
              </figure>
              <p>Running more than one task per thread is perfectly fine, but sometimes it is often desirable to run with a higher thread count to take full advantage of the available CPU resources. Increasing the number of threads doesn’t change the number of tasks, but it does change the distribution of tasks among threads. For example, if we reconfigure the same Kafka Streams application to run with four threads instead of two (<code>num.stream.threads = 4</code>), we end up with the task/thread layout depicted in <a data-type="xref" href="#C2_ARCH_2" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-9</a>.</p>
              <figure>
                <div id="C2_ARCH_2" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0209.png" alt="Kafka Streams abstraction levels" style="width: 28rem">
                  <h6><span class="label">Figure 2-9. </span>Four Kafka Streams tasks running in four threads</h6>
                </div>
              </figure>
              <p>Now that we’ve learned about Kafka Streams’ architecture, let’s take a look at the APIs that Kafka Streams exposes for creating stream processing applications.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="processor topologies" data-startref="ix_KafStrprc" id="idm46281564469016" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processor topologies" data-startref="ix_proctop" id="idm46281564467768" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="High-Level DSL Versus Low-Level Processor API">
        <div class="sect1" id="idm46281564591720">
          <h1>High-Level DSL Versus Low-Level Processor API</h1>
          <blockquote>
            <p>Different solutions present themselves at different layers of abstraction.</p>
            <p data-type="attribution">James Clear<sup><a data-type="noteref" id="idm46281564464216-marker" href="ch02.html#idm46281564464216" target="_blank" rel="noopener noreferrer">21</a></sup></p>
          </blockquote>
          <p>A common notion <a data-type="indexterm" data-primary="Kafka Streams" data-secondary="high-level DSL versus low-level Processor API" id="idm46281564462232" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="versus high-level DSL in Kafka Streams" data-secondary-sortas="high-level" id="idm46281564461160" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="high-level DSL versus low-level Processor API in Kafka Streams" id="idm46281564459976" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="abstractions" id="idm46281564459048" target="_blank" rel="noopener noreferrer"></a>in the software engineering field is that abstraction usually comes at a cost: the more you abstract the details away, the more the software feels like “magic,” and the more control you give up. As you get started with Kafka Streams, you may wonder what kind of control you will be giving up by choosing to implement a stream processing application using a high-level library instead of designing your solution using the lower-level Consumer/Producer APIs directly.<a data-type="indexterm" data-primary="domain-specific language" data-see="DSL" id="idm46281564457768" target="_blank" rel="noopener noreferrer"></a></p>
          <p>Luckily for us, Kafka Streams allows developers to choose the abstraction level that works best for them, depending on the project and also the experience and preference of the developer.</p>
          <p>The two APIs you can choose from are:</p>
          <ul>
            <li>
              <p>The high-level DSL</p></li>
            <li>
              <p>The low-level Processor API</p></li>
          </ul>
          <p>The relative abstraction level for both the high-level DSL and low-level Processor API is shown in <a data-type="xref" href="#C2_ABS_LEVELS" target="_blank" rel="noopener noreferrer">Figure&nbsp;2-10</a>.</p>
          <figure>
            <div id="C2_ABS_LEVELS" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0210.png" alt="Kafka Streams abstraction levels" style="width: 19rem">
              <h6><span class="label">Figure 2-10. </span>Abstraction levels of Kafka Streams APIs</h6>
            </div>
          </figure>
          <p>The high-level DSL is built on top of the Processor API, but the interface each exposes is slightly different.<a data-type="indexterm" data-primary="abstractions" data-secondary="abstraction levels of Kafka Streams APIs" id="idm46281564449608" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="APIs" data-secondary="abstraction levels of Kafka Streams APIs" id="idm46281564448536" target="_blank" rel="noopener noreferrer"></a> If you would like to build your stream processing application using a functional style of programming, and would also like to leverage some higher-level abstractions for working with your data (streams and tables), then the DSL is for you.</p>
          <p>On the other hand, if you need lower-level access to your data (e.g., access to record metadata), the ability to schedule periodic functions, more granular access to your application state, or more fine-grained control over the timing of certain operations, then the Processor API is a better choice. In the following tutorial, you will see examples of both the DSL and Processor API. In subsequent chapters, we will explore both the DSL and Processor API in further detail.</p>
          <p>Now, the best way to see the difference between these two abstraction levels is with a code example. Let’s move on to our first Kafka Streams tutorial: Hello Streams.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Introducing Our Tutorial: Hello, Streams">
        <div class="sect1" id="C2_TUTORIAL_HELLO_STREAMS">
          <h1>Introducing Our Tutorial: Hello, Streams</h1>
          <p>In this section, we will get our first hands-on <a data-type="indexterm" data-primary="Hello Streams tutorial" id="ix_HStut" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Hello Streams tutorial" id="ix_KafStrHStut" target="_blank" rel="noopener noreferrer"></a>experience with Kafka Streams. This is a variation of the “Hello, world” tutorial that has become the standard when learning new programming languages and libraries. There are two implementations of this tutorial: the first uses the high-level DSL, while the second uses the low-level <span class="keep-together">Processor</span> API. Both programs are functionally equivalent, and will print a simple <span class="keep-together">greeting</span> whenever they receive a message from the <code>users</code> topic in Kafka (e.g., upon receiving the message <code>Mitch</code>, each application will print <code>Hello, Mitch</code>).</p>
          <p>Before we get started, let’s take a look at how to set up the project.</p>
          <section data-type="sect2" data-pdf-bookmark="Project Setup">
            <div class="sect2" id="idm46281564437928">
              <h2>Project Setup</h2>
              <p>All of the tutorials in this book will require a running Kafka cluster, and the source code<a data-type="indexterm" data-primary="Hello Streams tutorial" data-secondary="project setup" id="idm46281564436248" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Hello Streams tutorial" data-tertiary="project setup" id="idm46281564435272" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Docker" data-secondary="running development cluster for Hello Streams project" id="idm46281564434056" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="clusters" data-secondary="running development cluster for Hello Streams project" id="idm46281564433080" target="_blank" rel="noopener noreferrer"></a> for each chapter will include a <em>docker-compose.yml</em> file that will allow you to run a development cluster using Docker. Since Kafka Streams applications are meant to run outside of a Kafka cluster (e.g., on different machines than the brokers), it’s best to view the Kafka cluster as a separate infrastructure piece that is required but distinct from your Kafka Streams application.</p>
              <p>To start running the Kafka cluster, clone the repository and change to the directory containing this chapter’s tutorial. The following commands will do the trick:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">$ git clone git@github.com:mitch-seymour/mastering-kafka-streams-and-ksqldb.git
$ cd mastering-kafka-streams-and-ksqldb/chapter-02/hello-streams</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Then, start the Kafka cluster by running:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">docker-compose up</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>The broker will be<a data-type="indexterm" data-primary="brokers" data-secondary="for Hello Streams tutorial" data-secondary-sortas="Hello" id="idm46281564428200" target="_blank" rel="noopener noreferrer"></a> listening on port 29092.<sup><a data-type="noteref" id="idm46281564426728-marker" href="ch02.html#idm46281564426728" target="_blank" rel="noopener noreferrer">22</a></sup> Furthermore, the preceding command will start a container that will precreate the <code>users</code> topic needed for this tutorial. Now, with our Kafka cluster running, we can start building our Kafka Streams application.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Creating a New Project">
            <div class="sect2" id="idm46281564424952">
              <h2>Creating a New Project</h2>
              <p>In this <a data-type="indexterm" data-primary="Hello Streams tutorial" data-secondary="creating a new project" id="idm46281564423416" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Hello Streams tutorial" data-tertiary="creating a new project" id="idm46281564422408" target="_blank" rel="noopener noreferrer"></a>book, we will use a build tool called Gradle<sup><a data-type="noteref" id="idm46281564421064-marker" href="ch02.html#idm46281564421064" target="_blank" rel="noopener noreferrer">23</a></sup> to compile and run our Kafka Streams applications.<a data-type="indexterm" data-primary="Gradle" id="idm46281564419096" target="_blank" rel="noopener noreferrer"></a> Other build tools (e.g., Maven) are also supported, but we have chosen Gradle due to the improved readability of its build files.</p>
              <p>In addition to being able to compile and run your code, Gradle can also be used to quickly bootstrap new Kafka Streams applications that you build outside of this book. This can be accomplished by creating a directory for your project to live in and then by running the <code>gradle init</code> command from within that directory.<a data-type="indexterm" data-primary="gradle init command" id="idm46281564417096" target="_blank" rel="noopener noreferrer"></a> An example of this workflow is as follows:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting" data-code-language="sh"><code class="nv">$ </code>mkdir my-project <code class="o">&amp;&amp;</code> <code class="nb">cd </code>my-project

<code class="nv">$ </code>gradle init <code class="se">\</code>
 --type java-application <code class="se">\</code>
 --dsl groovy <code class="se">\</code>
 --test-framework junit-jupiter <code class="se">\</code>
 --project-name my-project <code class="se">\</code>
 --package com.example</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>The source code for this book already contains the initialized project structure for each tutorial, so it’s not necessary to run <code>gradle init</code> unless you are starting a new project for yourself. We simply mention it here with the assumption that you will be writing your own Kafka Streams applications at some point, and want a quick way to bootstrap your next project.</p>
              <p>Here is the basic project structure for a Kafka Streams application:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">.
├── build.gradle
└── src
    ├── main
    │   ├── java
    │   └── resources
    └── test
        └── java
<p>Now that we’ve learned how to bootstrap new Kafka Streams projects and have had an initial look at the project structure, let’s take a look at how to add Kafka Streams to our project.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Adding the Kafka Streams Dependency"><div class="sect2" id="idm46281564378936">
<h2>Adding the Kafka Streams Dependency</h2>

<p>To start working with Kafka Streams, we simply need to add the Kafka Streams library as a dependency <a data-type="indexterm" data-primary="Gradle" data-secondary="adding Kafka Streams dependency to build file" id="idm46281564359720" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Hello Streams tutorial" data-secondary="adding Kafka Streams dependency to Gradle build file" id="idm46281564358648" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Hello Streams tutorial" data-tertiary="adding Kafka Streams dependency to Gradle build file" id="idm46281564357672" target="_blank" rel="noopener noreferrer"></a>in our build file. (In Gradle projects, our build file is called <em>build.gradle</em>.) An example<a data-type="indexterm" data-primary="build.gradle file" id="idm46281564355912" target="_blank" rel="noopener noreferrer"></a> build file is shown here:</p>

<div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer"><pre data-type="programlisting">plugins {
    id 'java'
    id 'application'
}

repositories {
    jcenter()
}

dependencies {
    implementation 'org.apache.kafka:kafka-streams:2.7.0'
}

task runDSL(type: JavaExec) {
    main = 'com.example.DslExample'
    classpath sourceSets.main.runtimeClasspath
}

task runProcessorAPI(type: JavaExec) {
    main = 'com.example.ProcessorApiExample'
    classpath sourceSets.main.runtimeClasspath
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Now, to build our project (which will actually pull the dependency from the remote repository into our project), we can run the following command:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">./gradlew build</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>That’s it! Kafka Streams is installed and ready to use. Now, let’s continue with the tutorial.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="DSL">
            <div class="sect2" id="idm46281564337896">
              <h2>DSL</h2>
              <p>The DSL example is exceptionally simple.<a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="example for Kafka Streams Hello Streams tutorial" id="idm46281564335848" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="StreamsBuilder class" id="idm46281564334872" target="_blank" rel="noopener noreferrer"></a> We first need to use a Kafka Streams class called <code>StreamsBuilder</code> to build our processor topology:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">StreamsBuilder builder = new StreamsBuilder();</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Next, as we learned in <a data-type="xref" href="#C2_SECTION_PROCESSOR" target="_blank" rel="noopener noreferrer">“Processor Topologies”</a>, we need to add a source processor in order to read data from a Kafka topic (in this case, our topic will be called <code>users</code>). There are a few different methods we could use here depending on how we decide to model our data (we will discuss different approaches in <a data-type="xref" href="#C2_SECTION_MODEL" target="_blank" rel="noopener noreferrer">“Streams and Tables”</a>), but for now, let’s model our data as a stream.<a data-type="indexterm" data-primary="source processors" data-secondary="adding to Hello Streams tutorial" id="idm46281564330088" target="_blank" rel="noopener noreferrer"></a> The following line adds the source processor:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KStream&lt;Void, String&gt; stream = builder.stream("users");<a class="co" id="co_getting_started_with_kafka_streams_CO3-1" href="#callout_getting_started_with_kafka_streams_CO3-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO3-1" href="#co_getting_started_with_kafka_streams_CO3-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We’ll discuss this more in the next chapter, but the generics in <code>KStream&lt;Void, String&gt;</code> refer to the key and value types. In this case, the key is empty (<code>Void</code>) and the value is a <code>String</code> type.</p>
                </dd>
              </dl>
              <p>Now, it’s time to add a stream processor.<a data-type="indexterm" data-primary="stream processors" data-secondary="adding to Hello Streams tutorial" id="idm46281564321464" target="_blank" rel="noopener noreferrer"></a> Since we’re just printing a simple greeting for each message, we can use the <code>foreach</code> operator with a simple lambda<a data-type="indexterm" data-primary="foreach operator" id="idm46281564319960" target="_blank" rel="noopener noreferrer"></a> like so:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">stream.foreach(
    (key, value) -&gt; {
        System.out.println("(DSL) Hello, " + value);
    });</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Finally, it’s time to build our topology and start running our stream processing<a data-type="indexterm" data-primary="processor topologies" data-secondary="building for Hello Streams tutorial" id="idm46281564317912" target="_blank" rel="noopener noreferrer"></a> <span class="keep-together">application</span>:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KafkaStreams streams = new KafkaStreams(builder.build(), config);
streams.start();</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>The full code, including some boilerplate needed to run the program, is shown in <a data-type="xref" href="#C2_EX3" target="_blank" rel="noopener noreferrer">Example&nbsp;2-1</a>.</p>
              <div id="C2_EX3" data-type="example">
                <h5><span class="label">Example 2-1. </span>Hello, world—DSL example</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting" data-code-language="java"><code class="kd">class </code><code class="nc">DslExample </code><code class="o">{</code>

<code>  </code><code class="kd">public </code><code class="kd">static </code><code class="kt">void </code><code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[</code><code class="o">] </code><code class="n">args</code><code class="o">) </code><code class="o">{</code>
<code>    </code><code class="n">StreamsBuilder </code><code class="n">builder </code><code class="o">= </code><code class="k">new </code><code class="n">StreamsBuilder</code><code class="o">(</code><code class="o">)</code><code class="o">; </code><a class="co" id="co_getting_started_with_kafka_streams_CO4-1" href="#callout_getting_started_with_kafka_streams_CO4-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

<code>    </code><code class="n">KStream</code><code class="o">&lt;</code><code class="n">Void</code><code class="o">, </code><code class="n">String</code><code class="o">&gt; </code><code class="n">stream </code><code class="o">= </code><code class="n">builder</code><code class="o">.</code><code class="na">stream</code><code class="o">(</code><code class="s">"users"</code><code class="o">)</code><code class="o">; </code> <code class="c1">// The builder is used to construct the topology</code>

<code>    </code><code class="n">stream</code><code class="o">.</code><code class="na">foreach</code><code class="o">( </code><a class="co" id="co_getting_started_with_kafka_streams_CO4-3" href="#callout_getting_started_with_kafka_streams_CO4-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
<code>        </code><code class="o">(</code><code class="n">key</code><code class="o">, </code><code class="n">value</code><code class="o">) </code><code class="o">-&gt;</code> <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="s">"(DSL) Hello, " </code><code class="o">+ </code><code class="n">value</code><code class="o">)</code><code class="o">)</code><code class="o">;</code>

<code>    </code><code class="c1">// omitted for brevity
    </code><code class="n">Properties </code><code class="n">config </code><code class="o">= </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="o">; </code><a class="co" id="co_getting_started_with_kafka_streams_CO4-4" href="#callout_getting_started_with_kafka_streams_CO4-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>

<code>    </code><code class="n">KafkaStreams </code><code class="n">streams </code><code class="o">= </code><code class="k">new </code><code class="n">KafkaStreams</code><code class="o">(</code><code class="n">builder</code><code class="o">.</code><code class="na">build</code><code class="o">(</code><code class="o">)</code><code class="o">, </code><code class="n">config</code><code class="o">)</code><code class="o">; </code><a class="co" id="co_getting_started_with_kafka_streams_CO4-5" href="#callout_getting_started_with_kafka_streams_CO4-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
<code>    </code><code class="n">streams</code><code class="o">.</code><code class="na">start</code><code class="o">(</code><code class="o">)</code><code class="o">;</code>

<code>    </code><code class="c1">// close Kafka Streams when the JVM shuts down (e.g., SIGTERM)
    </code><code class="n">Runtime</code><code class="o">.</code><code class="na">getRuntime</code><code class="o">(</code><code class="o">)</code><code class="o">.</code><code class="na">addShutdownHook</code><code class="o">(</code><code class="k">new </code><code class="n">Thread</code><code class="o">(</code><code class="nl">streams:</code><code class="o">:</code><code class="n">close</code><code class="o">)</code><code class="o">)</code><code class="o">; </code><a class="co" id="co_getting_started_with_kafka_streams_CO4-6" href="#callout_getting_started_with_kafka_streams_CO4-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
<code>  </code><code class="o">}</code>
<code class="o">}</code></pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO4-1" href="#co_getting_started_with_kafka_streams_CO4-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The builder is used to construct the topology.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO4-3" href="#co_getting_started_with_kafka_streams_CO4-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Use the DSL’s <code>foreach</code> operator to print a simple message. The DSL includes many operators that we will be exploring in upcoming chapters.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO4-4" href="#co_getting_started_with_kafka_streams_CO4-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We have omitted the Kafka Streams configuration for brevity, but will discuss this in upcoming chapters. Among other things, this configuration allows us to specify which Kafka cluster our application should read from and what consumer group this application belongs to.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO4-5" href="#co_getting_started_with_kafka_streams_CO4-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Build the topology and start streaming.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO4-6" href="#co_getting_started_with_kafka_streams_CO4-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Close Kafka Streams when the JVM shuts down.</p>
                </dd>
              </dl>
              <p>To run the application, simply execute the following command:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">./gradlew runDSL --info</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Now your Kafka Streams application is running and listening for incoming data. As you may recall from <a data-type="xref" href="ch01.html#C1_HELLO_KAFKA" target="_blank" rel="noopener noreferrer">“Hello, Kafka”</a>, we can produce some data to our Kafka cluster using the <code>kafka-console-producer</code> console script.<a data-type="indexterm" data-primary="producers" data-secondary="kafka-console-producer script" id="idm46281564088296" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="docker-compose exec kafka bash command" id="idm46281564087320" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="kafka-console-producer script" id="idm46281564086680" target="_blank" rel="noopener noreferrer"></a> To do this, run the following commands:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting" data-code-language="sh"><code>docker-compose </code><code class="nb">exec </code><code>kafka </code><code>bash </code><a class="co" id="co_getting_started_with_kafka_streams_CO5-1" href="#callout_getting_started_with_kafka_streams_CO5-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a><code>

</code><code>kafka-console-producer </code><code class="se">\ </code><a class="co" id="co_getting_started_with_kafka_streams_CO5-2" href="#callout_getting_started_with_kafka_streams_CO5-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a><code>
    </code><code>--bootstrap-server </code><code>localhost:9092 </code><code class="se">\
    </code><code>--topic </code><code>users</code></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO5-1" href="#co_getting_started_with_kafka_streams_CO5-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The console scripts are available in the <code>kafka</code> container, which is running the broker in our development cluster. You can also download these scripts as part of the official Kafka distribution.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO5-2" href="#co_getting_started_with_kafka_streams_CO5-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Start a local producer that will write data to the <code>users</code> topic.</p>
                </dd>
              </dl>
              <p>Once you are in the producer prompt, create one or more records by typing the name of the user, followed by the Enter key. When you are finished, press Control-C on your keyboard to exit the prompt:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">&gt;angie
&gt;guy
&gt;kate
&gt;mark</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Your Kafka Streams application should emit the following greetings:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">(DSL) Hello, angie
(DSL) Hello, guy
(DSL) Hello, kate
(DSL) Hello, mark</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>We have now verified that our application is working as expected. We will explore some more interesting use cases over the next several chapters, but this process of defining a topology and running our application is a foundation we can build upon. Next, let’s look at how to create the same Kafka Streams topology with the lower-level Processor API.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Processor API">
            <div class="sect2" id="idm46281564336952">
              <h2>Processor API</h2>
              <p>The Processor API lacks<a data-type="indexterm" data-primary="Hello Streams tutorial" data-secondary="Processor API for" id="idm46281564036376" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Hello Streams tutorial" data-tertiary="Processor API" id="idm46281564035368" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="creating for Hello Streams tutorial" id="idm46281564034152" target="_blank" rel="noopener noreferrer"></a> some of the abstractions available in the high-level DSL, and its syntax is more of a direct reminder that we’re building processor topologies, with methods like <code>Topology.addSource</code>, <code>Topology.addProcessor</code>, and <code>Topology.addSink</code> (the latter of which is not used in this example).<a data-type="indexterm" data-primary="Topology.addSource method" id="idm46281564031656" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Topology instance" id="idm46281564030984" target="_blank" rel="noopener noreferrer"></a> The first step in using the processor topology is to instantiate a new <code>Topology</code> instance, like so:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">Topology topology = new Topology();</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Next, we will create a source processor to read data from the <code>users</code> topic, and a stream processor to print a simple greeting. The stream processor references a class called <code>SayHelloProcessor</code> that we’ll implement shortly:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">topology.addSource("UserSource", "users"); <a class="co" id="co_getting_started_with_kafka_streams_CO6-1" href="#callout_getting_started_with_kafka_streams_CO6-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
topology.addProcessor("SayHello", SayHelloProcessor::new, "UserSource"); <a class="co" id="co_getting_started_with_kafka_streams_CO6-2" href="#callout_getting_started_with_kafka_streams_CO6-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO6-1" href="#co_getting_started_with_kafka_streams_CO6-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The first argument for the <code>addSource</code> method is an arbitrary name for this stream processor.<a data-type="indexterm" data-primary="Topology.addProcessor method" id="idm46281564020072" target="_blank" rel="noopener noreferrer"></a> In this case, we simply call this processor <code>UserSource</code>. We will refer to this name in the next line when we want to connect a child processor, which in turn defines how data should flow through our topology. The second argument is the topic name that this source processor should read from (in this case, <code>users</code>).</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO6-2" href="#co_getting_started_with_kafka_streams_CO6-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>This line creates a new downstream processor called <code>SayHello</code> whose processing logic is defined in the <code>SayHelloProcessor</code> class (we will create this in the next section). In the Processor API, we can connect one processor to another by specifying the name of the parent processor. In this case, we specify the <code>UserSource</code> processor as the parent of the <code>SayHello</code> processor, which means data will flow from the <code>UserSource</code> to <code>SayHello</code>.</p>
                </dd>
              </dl>
              <p>As we saw before, in the DSL tutorial, we now need to build the topology and call <code>streams.start()</code> to run it:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KafkaStreams streams = new KafkaStreams(topology, config);
streams.start();</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p class="pagebreak-before">Before running the code, we need to implement the <code>SayHelloProcessor</code> class. Whenever you build a custom stream processor using the Processor API, you need to implement the <code>Processor</code> interface.<a data-type="indexterm" data-primary="Processor interface" data-secondary="implementing for Hello Streams tutorial" id="idm46281564009000" target="_blank" rel="noopener noreferrer"></a> The interface specifies methods for initializing the stream processor (<code>init</code>), applying the stream processing logic to a single record (<code>process</code>), and a cleanup function (<code>close</code>). The initialization and cleanup function aren’t needed in this example.</p>
              <p>The following is a simple implementation of <code>SayHelloProcessor</code> that we will use <span class="keep-together">for this</span> example. We will explore more complex examples, and all of the interface <span class="keep-together">methods</span> in the Processor interface (<code>init</code>, <code>process</code>, and <code>close</code>), in more detail in <span class="keep-together"><a data-type="xref" href="ch07.html#ch7" target="_blank" rel="noopener noreferrer">Chapter&nbsp;7</a>.</span></p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting" class="code-quiet-shrink">public class SayHelloProcessor implements Processor&lt;Void, String, Void, Void&gt; { <a class="co" id="co_getting_started_with_kafka_streams_CO7-1" href="#callout_getting_started_with_kafka_streams_CO7-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  @Override
  public void init(ProcessorContext&lt;Void, Void&gt; context) {} <a class="co" id="co_getting_started_with_kafka_streams_CO7-2" href="#callout_getting_started_with_kafka_streams_CO7-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

  @Override
  public void process(Record&lt;Void, String&gt; record) { <a class="co" id="co_getting_started_with_kafka_streams_CO7-3" href="#callout_getting_started_with_kafka_streams_CO7-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    System.out.println("(Processor API) Hello, " + record.value());
  }

  @Override
  public void close() {} <a class="co" id="co_getting_started_with_kafka_streams_CO7-4" href="#callout_getting_started_with_kafka_streams_CO7-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO7-1" href="#co_getting_started_with_kafka_streams_CO7-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The first two generics in the <code>Processor</code> interface (in this example, <span class="keep-together"><code>Processor&lt;Void, String, ..., ...&gt;</code></span>) refer to the <em>input</em> key and value types. Since our keys are null and our values are usernames (i.e., text strings), <code>Void</code> <span class="keep-together">and <code>String</code></span> are the appropriate choices. The last two generics <span class="keep-together">(<code>Processor&lt;..., ..., Void, Void&gt;</code>)</span> refer to the <em>output</em> key and value types. In this example, our <code>SayHelloProcessor</code> simply prints a greeting. Since we aren’t forwarding any output keys or values downstream, <code>Void</code> is the appropriate type for the final two generics.<sup><a data-type="noteref" id="idm46281563984088-marker" href="ch02.html#idm46281563984088" target="_blank" rel="noopener noreferrer">24</a></sup></p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO7-2" href="#co_getting_started_with_kafka_streams_CO7-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>No special initialization is needed in this example, so the method body is empty. The generics in the <code>ProcessorContext</code> interface (<code>ProcessorContext&lt;Void, Void&gt;</code>) refer to the output key and value types (again, as we’re not forwarding any messages downstream in this example, both are <code>Void</code>).</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO7-3" href="#co_getting_started_with_kafka_streams_CO7-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The processing logic lives in the aptly named <code>process</code> method in the <code>Processor</code> interface. Here, we print a simple greeting. Note that the generics in the <code>Record</code> interface refer to the key and value type of the <em>input</em> records.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_getting_started_with_kafka_streams_CO7-4" href="#co_getting_started_with_kafka_streams_CO7-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>No special cleanup needed in this example.</p>
                </dd>
              </dl>
              <p>We can now run the code using the same command we used in the DSL example:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">./gradlew runProcessorAPI --info</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>You should see the following output to indicate your Kafka Streams application is working as expected:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">(Processor API) Hello, angie
(Processor API) Hello, guy
(Processor API) Hello, kate
(Processor API) Hello, mark</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Now, despite the Processor API’s power, which we will see in <a data-type="xref" href="ch07.html#ch7" target="_blank" rel="noopener noreferrer">Chapter&nbsp;7</a>, using the DSL is often preferable because, among other benefits, it includes two very powerful abstractions: streams and tables. We will get our first look at these abstractions in the next section.<a data-type="indexterm" data-primary="Kafka Streams" data-secondary="Hello Streams tutorial" data-startref="ix_KafStrHStut" id="idm46281563966776" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Hello Streams tutorial" data-startref="ix_HStut" id="idm46281563965560" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Streams and Tables">
        <div class="sect1" id="C2_SECTION_MODEL">
          <h1>Streams and Tables</h1>
          <p>If you look closely at <a data-type="xref" href="#C2_EX3" target="_blank" rel="noopener noreferrer">Example&nbsp;2-1</a>, you will notice that we used a DSL operator called <code>stream</code> to read a Kafka topic into a <em>stream</em>. <a data-type="indexterm" data-primary="stream operator" id="idm46281563960440" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="streams and tables in Kafka Streams applications" id="ix_StrmTbl" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="streams and tables" id="ix_KafStrstrtbl" target="_blank" rel="noopener noreferrer"></a>The relevant line of code is:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KStream&lt;Void, String&gt; stream = builder.stream("users");</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>However, kafka streams also supports an additional way to view our data: as a <em>table</em>. in this section, we’ll take a look at both options and learn when to use <em>streams</em> and when to use <em>tables</em>.<a data-type="indexterm" data-primary="tables" data-secondary="streams and tables in Kafka Streams applications" id="ix_tblstrm" target="_blank" rel="noopener noreferrer"></a></p>
          <p>As discussed in <a data-type="xref" href="#C2_SECTION_PROCESSOR" target="_blank" rel="noopener noreferrer">“Processor Topologies”</a>, designing a processor topology involves specifying a set of source and sink processors, which correspond to the topics your application will read from and write to. However, instead of working with Kafka topics <em>directly</em>, the Kafka Streams DSL allows you to work with different <em>representations</em> of a topic, each of which are suitable for different use cases.<a data-type="indexterm" data-primary="topics" data-secondary="representations in Kafka Streams DSL" id="idm46281563950792" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="representations of topics in Kafka Streams DSL" id="idm46281563949848" target="_blank" rel="noopener noreferrer"></a> There are two ways to model the data in your Kafka topics: as a <em>stream</em> (also called a <em>record stream</em>) or a <em>table</em> (also known as a <em>changelog stream</em>).<a data-type="indexterm" data-primary="record stream" id="idm46281563947048" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="changelog stream" id="idm46281563946312" target="_blank" rel="noopener noreferrer"></a> The easiest way to compare these two data models is through an example.</p>
          <p>Say we have a topic containing ssh logs, where each record is keyed by a user ID as shown in <a data-type="xref" href="#table0201" target="_blank" rel="noopener noreferrer">Table&nbsp;2-2</a>.</p>
          <table id="table0201" class="pagebreak-before">
            <caption>
              <span class="label">Table 2-2. </span>Keyed records in a single topic-partition
            </caption>
            <thead>
              <tr>
                <th>Key</th>
                <th>Value</th>
                <th>Offset</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div>
                    <p>mitch</p>
                  </div></td>
                <td>
                  <div>
                    <p><code>{ "action": "login" }</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>0</p>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>mitch</p>
                  </div></td>
                <td>
                  <div>
                    <p><code>{ "action": "logout" }</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>1</p>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>elyse</p>
                  </div></td>
                <td>
                  <div>
                    <p><code>{ "action": "login" }</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>2</p>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>isabelle</p>
                  </div></td>
                <td>
                  <div>
                    <p><code>{ "action": "login" }</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>3</p>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>Before consuming this data, we need to decide which abstraction to use: a stream or a table. When making this decision, we need to consider whether or not we want to track only the latest state/representation of a given key, or the entire history of messages.<a data-type="indexterm" data-primary="streams" data-secondary="streams and tables in Kafka Streams applications" data-tertiary="stream view of ssh logs" id="idm46281563926968" target="_blank" rel="noopener noreferrer"></a> Let’s compare the two options side by side:</p>
          <dl>
            <dt>
              Streams
            </dt>
            <dd>
              <div class="openblock">
                <p>These can be thought of as inserts in database parlance. Each distinct record remains in this view of the log. The stream representation of our topic can be seen in <a data-type="xref" href="#stream" target="_blank" rel="noopener noreferrer">Table&nbsp;2-3</a>.</p>
                <table id="stream">
                  <caption>
                    <span class="label">Table 2-3. </span>Stream view of ssh logs
                  </caption>
                  <thead>
                    <tr>
                      <th>Key</th>
                      <th>Value</th>
                      <th>Offset</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>
                        <div>
                          <p>mitch</p>
                        </div></td>
                      <td>
                        <div>
                          <p><code>{ "action": "login" }</code></p>
                        </div></td>
                      <td>
                        <div>
                          <p>0</p>
                        </div></td>
                    </tr>
                    <tr>
                      <td>
                        <div>
                          <p>mitch</p>
                        </div></td>
                      <td>
                        <div>
                          <p><code>{ "action": "logout" }</code></p>
                        </div></td>
                      <td>
                        <div>
                          <p>1</p>
                        </div></td>
                    </tr>
                    <tr>
                      <td>
                        <div>
                          <p>elyse</p>
                        </div></td>
                      <td>
                        <div>
                          <p><code>{ "action": "login" }</code></p>
                        </div></td>
                      <td>
                        <div>
                          <p>2</p>
                        </div></td>
                    </tr>
                    <tr>
                      <td>
                        <div>
                          <p>isabelle</p>
                        </div></td>
                      <td>
                        <div>
                          <p><code>{ "action": "login" }</code></p>
                        </div></td>
                      <td>
                        <div>
                          <p>3</p>
                        </div></td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </dd>
            <dt>
              Tables
            </dt>
            <dd>
              <div class="openblock">
                <p>Tables can be thought of as updates to a database.<a data-type="indexterm" data-primary="tables" data-secondary="streams and tables in Kafka Streams applications" data-tertiary="table view of ssh logs" id="idm46281563903496" target="_blank" rel="noopener noreferrer"></a> In this view of the logs, only the current state (either the latest record for a given key or some kind of aggregation) for each key is retained. Tables are usually built from <em>compacted topics</em> (i.e., topics that are configured with a <code>cleanup.policy</code> of <code>compact</code>, which tells Kafka that you only want to keep the latest representation of each key). The table representation of our topic can be seen in <a data-type="xref" href="#table" target="_blank" rel="noopener noreferrer">Table&nbsp;2-4</a>.</p>
                <table id="table">
                  <caption>
                    <span class="label">Table 2-4. </span>Table view of ssh logs
                  </caption>
                  <thead>
                    <tr>
                      <th>Key</th>
                      <th>Value</th>
                      <th>Offset</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>
                        <div>
                          <p>mitch</p>
                        </div></td>
                      <td>
                        <div>
                          <p><code>{ "action": "logout" }</code></p>
                        </div></td>
                      <td>
                        <div>
                          <p>1</p>
                        </div></td>
                    </tr>
                    <tr>
                      <td>
                        <div>
                          <p>elyse</p>
                        </div></td>
                      <td>
                        <div>
                          <p><code>{ "action": "login" }</code></p>
                        </div></td>
                      <td>
                        <div>
                          <p>2</p>
                        </div></td>
                    </tr>
                    <tr>
                      <td>
                        <div>
                          <p>isabelle</p>
                        </div></td>
                      <td>
                        <div>
                          <p><code>{ "action": "login" }</code></p>
                        </div></td>
                      <td>
                        <div>
                          <p>3</p>
                        </div></td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </dd>
          </dl>
          <p>Tables, by nature, are <em>stateful</em>, and are often used for performing aggregations <a data-type="indexterm" data-primary="tables" data-secondary="streams and tables in Kafka Streams applications" data-tertiary="tables performing aggregations in Kafka Streams" id="idm46281563884648" target="_blank" rel="noopener noreferrer"></a>in Kafka Streams.<sup><a data-type="noteref" id="idm46281563883144-marker" href="ch02.html#idm46281563883144" target="_blank" rel="noopener noreferrer">25</a></sup> In <a data-type="xref" href="#table" target="_blank" rel="noopener noreferrer">Table&nbsp;2-4</a>, we didn’t really perform a mathematical aggregation, we just kept the latest ssh event for each user ID. However, tables also support mathematical aggregations. For example, instead of tracking the latest record for each key, we could have just as easily calculated a rolling <code>count</code>. In this case, we would have ended up with a slightly different table, where the values contain the result of our <code>count</code> aggregation. You can see a count-aggregated table in <a data-type="xref" href="#aggregated-table" target="_blank" rel="noopener noreferrer">Table&nbsp;2-5</a>.</p>
          <table id="aggregated-table">
            <caption>
              <span class="label">Table 2-5. </span>Aggregated table view of ssh logs
            </caption>
            <thead>
              <tr>
                <th>Key</th>
                <th>Value</th>
                <th>Offset</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div>
                    <p>mitch</p>
                  </div></td>
                <td>
                  <div>
                    <p><code>2</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>1</p>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>elyse</p>
                  </div></td>
                <td>
                  <div>
                    <p><code>1</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>2</p>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>isabelle</p>
                  </div></td>
                <td>
                  <div>
                    <p><code>1</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>3</p>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>Careful readers may have noticed a discrepancy between the design of Kafka’s storage layer (a distributed, append-only log) and a table. Records that are written to Kafka are immutable, so how is it possible to model data as updates, using a <em>table</em> representation of a Kafka topic?<a data-type="indexterm" data-primary="immutability" data-secondary="records written to Kafka" id="idm46281563863832" target="_blank" rel="noopener noreferrer"></a></p>
          <p>The answer is simple: the table is materialized on the Kafka Streams side using a key-value store which, by default, is implemented using RocksDB.<sup><a data-type="noteref" id="idm46281563862024-marker" href="ch02.html#idm46281563862024" target="_blank" rel="noopener noreferrer">26</a></sup> By consuming an ordered stream of events and keeping only the latest record for each key in the client-side key-value store (more commonly called a <em>state store</em> in Kafka Streams terminology), we end up with a table or map-like representation of the data. In other words, the table isn’t something we <em>consume</em> from Kafka, but something we <em>build</em> on the <span class="keep-together">client</span> side.</p>
          <p>You can actually write a few lines of Java code to implement this basic idea. In the following code snippet, the <code>List</code> represents a stream since it contains an ordered collection of records,<sup><a data-type="noteref" id="idm46281563855512-marker" href="ch02.html#idm46281563855512" target="_blank" rel="noopener noreferrer">27</a></sup> and the table is constructed by iterating through the list (<code>stream.forEach</code>) and only retaining the latest record for a given key using a <code>Map</code>. The following Java code demonstrates this basic idea:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">import java.util.Map.Entry;

var stream = List.of(
    Map.entry("a", 1),
    Map.entry("b", 1),
    Map.entry("a", 2));

var table = new HashMap&lt;&gt;();

stream.forEach((record) -&gt; table.put(record.getKey(), record.getValue()));</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>If you were to print the stream and table after running this code, you would see the following output:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">stream ==&gt; [a=1, b=1, a=2]

table ==&gt; {a=2, b=1}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Of course, the Kafka Streams implementation of this is more sophisticated, and can leverage fault-tolerant data structures as opposed to an in-memory <code>Map</code>. But this ability to construct a table representation of an unbounded stream is only one side of a more complex relationship between streams and tables, which we will explore next.</p>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Stream/Table Duality">
            <div class="sect2" id="C2_STREAMS_VS_TABLES">
              <h2>Stream/Table Duality</h2>
              <p>The <em>duality</em> of tables and streams comes from the fact that tables can be represented as streams, and streams can be used to reconstruct tables.<a data-type="indexterm" data-primary="streams" data-secondary="streams and tables in Kafka Streams applications" data-tertiary="stream/table duality" id="idm46281563847496" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="tables" data-secondary="streams and tables in Kafka Streams applications" data-tertiary="stream/table duality" id="idm46281563846216" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="streams and tables" data-tertiary="stream/table duality" id="idm46281563844968" target="_blank" rel="noopener noreferrer"></a> We saw the latter transformation of a stream into a table in the previous section, when discussing the discrepancy between Kafka’s append-only, immutable log and the notion of a mutable table structure that accepts updates to its data.</p>
              <p>This ability to reconstruct tables from streams isn’t unique to Kafka Streams, and is in fact pretty common in various types of storage. For example, MySQL’s replication process relies on the same notion of taking a stream of events (i.e., row changes) to reconstruct a source table on a downstream replica. Similarly, Redis has the notion of an append-only file (AOF) that captures every command that is written to the in-memory key-value store. If a Redis server goes offline, then the stream of commands in the AOF can be replayed to reconstruct the dataset.</p>
              <p>What about the other side of the coin (representing a table as a stream)? When viewing a table, you are viewing a single point-in-time representation of a stream. As we saw earlier, tables can be updated when a new record arrives. By changing our view of the table to a stream, we can simply process the update as an insert, and append the new record to the end of the log instead of updating the key. Again, the intuition behind this can be seen using a few lines of Java code:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">var stream = table.entrySet().stream().collect(Collectors.toList());

stream.add(Map.entry("a", 3));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>This time, if you print the contents of the stream, you’ll see we’re no longer using update semantics, but instead insert semantics:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">stream ==&gt; [a=2, b=1, a=3]</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>So far, we’ve been working with the standard libraries in Java to build intuition around streams and tables. However, when working with streams and tables in Kafka Streams, you’ll use a set of more specialized abstractions. We’ll take a look at these abstractions next.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="KStream,  KTable, GlobalKTable">
            <div class="sect2" id="idm46281563838600">
              <h2>KStream, KTable, GlobalKTable</h2>
              <p>One of the benefits of using the high-level DSL over the lower-level Processor API in Kafka Streams is that the former includes a set of abstractions that make working with streams and tables extremely easy.<a data-type="indexterm" data-primary="tables" data-secondary="streams and tables in Kafka Streams applications" data-tertiary="KStream, KTable, and GlobalKTable" id="idm46281563836840" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="streams and tables in Kafka Streams applications" data-tertiary="KStream, KTable, and GlobalKTable" id="idm46281563835560" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="streams and tables" data-tertiary="KStream, KTable, and GlobalKTable" id="idm46281563834296" target="_blank" rel="noopener noreferrer"></a></p>
              <p class="pagebreak-before">The following list includes a high-level overview of each:</p>
              <dl>
                <dt>
                  <code>KStream</code>
                </dt>
                <dd>
                  <p>A <code>KStream</code> is an abstraction of a partitioned <em>record stream</em>, in which data is represented using insert semantics (i.e., each event is considered to be <em>independent</em> of other events).<a data-type="indexterm" data-primary="KStream class" id="idm46281563829160" target="_blank" rel="noopener noreferrer"></a></p>
                </dd>
                <dt>
                  <code>KTable</code>
                </dt>
                <dd>
                  <p>A <code>KTable</code> is an abstraction of a partitioned table (i.e., <em>changelog stream</em>), in which data is represented using update semantics (the latest representation of a given key<a data-type="indexterm" data-primary="KTable class" id="idm46281563826040" target="_blank" rel="noopener noreferrer"></a> is tracked by the application). Since <code>KTable</code>s are partitioned, each Kafka Streams task contains only a subset of the full table.<sup><a data-type="noteref" id="idm46281563824824-marker" href="ch02.html#idm46281563824824" target="_blank" rel="noopener noreferrer">28</a></sup></p>
                </dd>
                <dt>
                  <code>GlobalKTable</code>
                </dt>
                <dd>
                  <p>This <a data-type="indexterm" data-primary="GlobalKTable class" id="idm46281563822632" target="_blank" rel="noopener noreferrer"></a>is similar to a <code>KTable</code>, except each <code>GlobalKTable</code> contains a complete (i.e., unpartitioned) copy of the underlying data. We’ll learn when to use <code>KTable</code>s and when to use <code>GlobalKTable</code>s in <a data-type="xref" href="ch04.html#ch4" target="_blank" rel="noopener noreferrer">Chapter&nbsp;4</a>.</p>
                </dd>
              </dl>
              <p>Kafka Streams applications can make use of multiple stream/table abstractions, or just one. It’s entirely dependent on your use case, and as we work through the next few chapters, you will learn when to use each one. This completes our initial discussion of streams and tables, so let’s move on to the next chapter and explore Kafka Streams in more depth.<a data-type="indexterm" data-primary="tables" data-secondary="streams and tables in Kafka Streams applications" data-startref="ix_tblstrm" id="idm46281563818264" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="streams and tables in Kafka Streams applications" data-startref="ix_StrmTbl" id="idm46281563816920" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Streams" data-secondary="streams and tables" data-startref="ix_KafStrstrtbl" id="idm46281563815672" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Summary">
        <div class="sect1" id="idm46281563963704">
          <h1>Summary</h1>
          <p>Congratulations, you made it through the end of your first date with Kafka Streams. Here’s what you learned:</p>
          <ul>
            <li>
              <p>Kafka Streams lives in the stream processing layer of the Kafka ecosystem. This is where sophisticated data processing, transformation, and enrichment happen.</p></li>
            <li>
              <p>Kafka Streams was built to simplify the development of stream processing applications with a simple, functional API and a set of stream processing primitives that can be reused across projects. When more control is needed, a lower-level Processor API can also be used to define your topology.</p></li>
            <li>
              <p>Kafka Streams has a friendlier learning curve and a simpler deployment model than cluster-based solutions like Apache Flink and Apache Spark Streaming. It also supports event-at-a-time processing, which is considered true streaming.</p></li>
            <li>
              <p>Kafka Streams is great for solving problems that require or benefit from real-time decision making and data processing. Furthermore, it is reliable, maintainable, scalable, and elastic.</p></li>
            <li>
              <p>Installing and running Kafka Streams is simple, and the code examples in this chapter can be found at <a href="https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb</em></a>.</p></li>
          </ul>
          <p>In the next chapter, we’ll learn about stateless processing in Kafka Streams. We will also get some hands-on experience with several new DSL operators, which will help us build more advanced and powerful<a data-type="indexterm" data-primary="Kafka Streams" data-startref="ix_KafStr" id="idm46281563805816" target="_blank" rel="noopener noreferrer"></a> stream processing applications.</p>
        </div>
      </section>
      <div data-type="footnotes">
        <p data-type="footnote" id="idm46281564786680"><sup><a href="ch02.html#idm46281564786680-marker" target="_blank" rel="noopener noreferrer">1</a></sup> We are referring to the official ecosystem here, which includes all of the components that are maintained under the Apache Kafka project.</p>
        <p data-type="footnote" id="idm46281564773768"><sup><a href="ch02.html#idm46281564773768-marker" target="_blank" rel="noopener noreferrer">2</a></sup> Jay Kreps, one of the original authors of Apache Kafka, discussed this in detail in an O’Reilly blog post <a href="https://oreil.ly/vzRH-" target="_blank" rel="noopener noreferrer">back in 2014</a>.</p>
        <p data-type="footnote" id="idm46281564770600"><sup><a href="ch02.html#idm46281564770600-marker" target="_blank" rel="noopener noreferrer">3</a></sup> This includes aggregated streams/tables, which we’ll discuss later in this chapter.</p>
        <p data-type="footnote" id="idm46281564769208"><sup><a href="ch02.html#idm46281564769208-marker" target="_blank" rel="noopener noreferrer">4</a></sup> We have an entire chapter dedicated to time, but also see Matthias J. Sax’s great presentation on the subject from <a href="https://oreil.ly/wr123" target="_blank" rel="noopener noreferrer">Kafka Summit 2019</a>.</p>
        <p data-type="footnote" id="idm46281564761352"><sup><a href="ch02.html#idm46281564761352-marker" target="_blank" rel="noopener noreferrer">5</a></sup> Guozhang Wang, who has played a key role in the development of Kafka Streams, deserves much of the recognition for submitting the original KIP for what would later become Kafka Streams. See <a href="https://oreil.ly/l2wbc" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://oreil.ly/l2wbc</em></a>.</p>
        <p data-type="footnote" id="idm46281564752104"><sup><a href="ch02.html#idm46281564752104-marker" target="_blank" rel="noopener noreferrer">6</a></sup> Kafka Connect veered a little into event processing territory by adding support for something called <em>single message transforms</em>, but this is extremely limited compared to what Kafka Streams can do.</p>
        <p data-type="footnote" id="idm46281564732280"><sup><a href="ch02.html#idm46281564732280-marker" target="_blank" rel="noopener noreferrer">7</a></sup> Kafka Streams will work with other JVM-based languages as well, including Scala and Kotlin. However, we exclusively use Java in this book.</p>
        <p data-type="footnote" id="idm46281564713912"><sup><a href="ch02.html#idm46281564713912-marker" target="_blank" rel="noopener noreferrer">8</a></sup> Multiple consumer groups can consume from a single topic, and each consumer group processes messages independently of other consumer groups.</p>
        <p data-type="footnote" id="idm46281564710536"><sup><a href="ch02.html#idm46281564710536-marker" target="_blank" rel="noopener noreferrer">9</a></sup> While partitions can be added to an existing topic, the recommended pattern is to create a new source topic with the desired number of partitions, and to migrate all of the existing workloads to the new topic.</p>
        <p data-type="footnote" id="idm46281564698648"><sup><a href="ch02.html#idm46281564698648-marker" target="_blank" rel="noopener noreferrer">10</a></sup> Including some features that are specific to <em>stateful applications</em>, which we will discuss in <a data-type="xref" href="ch04.html#ch4" target="_blank" rel="noopener noreferrer">Chapter&nbsp;4</a>.</p>
        <p data-type="footnote" id="idm46281564681272"><sup><a href="ch02.html#idm46281564681272-marker" target="_blank" rel="noopener noreferrer">11</a></sup> The ever-growing nature of the stream processing space makes it difficult to compare every solution to Kafka Streams, so we have decided to focus on the most popular and mature stream processing solutions available at the time of this writing.</p>
        <p data-type="footnote" id="idm46281564652376"><sup><a href="ch02.html#idm46281564652376-marker" target="_blank" rel="noopener noreferrer">12</a></sup> Although there is an open, albeit dated, proposal to support <a href="https://oreil.ly/v3DbO" target="_blank" rel="noopener noreferrer">batch processing in Kafka Streams</a>.</p>
        <p data-type="footnote" id="idm46281564639976"><sup><a href="ch02.html#idm46281564639976-marker" target="_blank" rel="noopener noreferrer">13</a></sup> Including a Kafka-backed relational database called KarelDB, a graph analytics library built on Kafka Streams, and more. See <a href="https://yokota.blog" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://yokota.blog</em></a>.</p>
        <p data-type="footnote" id="idm46281564626536"><sup><a href="ch02.html#idm46281564626536-marker" target="_blank" rel="noopener noreferrer">14</a></sup> At the time of this writing, Apache Flink had recently released a beta version of queryable state, though the API itself was less mature and came with the following warning in the official Flink documentation: “The client APIs for queryable state are currently in an evolving state and there are no guarantees made about stability of the provided interfaces. It is likely that there will be breaking API changes on the client side in the upcoming Flink versions.” Therefore, while the Apache Flink team is working to close this gap, Kafka Streams still has the more mature and production-ready API for querying state.</p>
        <p data-type="footnote" id="idm46281564543416"><sup><a href="ch02.html#idm46281564543416-marker" target="_blank" rel="noopener noreferrer">15</a></sup> The exception to this is when topics are joined. In this case, a single topology will read from each source topic involved in the join without further dividing the step into sub-topologies. This is required for the join to work. See <a data-type="xref" href="ch04.html#CP_COPARTITIONING" target="_blank" rel="noopener noreferrer">“Co-Partitioning”</a> for more information.</p>
        <p data-type="footnote" id="idm46281564536776"><sup><a href="ch02.html#idm46281564536776-marker" target="_blank" rel="noopener noreferrer">16</a></sup> In this example, we write to two intermediate topics (<code>valid-mentions</code> and <code>invalid-mentions</code>) and then immediately consume data from each. Using intermediate topics like this is usually only required for certain operations (for example, repartitioning data). We do it here for discussion purposes only.</p>
        <p data-type="footnote" id="idm46281564493256"><sup><a href="ch02.html#idm46281564493256-marker" target="_blank" rel="noopener noreferrer">17</a></sup> A Java application may execute many different types of threads. Our discussion will simply focus on the stream threads that are created and managed by the Kafka Streams library for running a processor topology.</p>
        <p data-type="footnote" id="idm46281564488888"><sup><a href="ch02.html#idm46281564488888-marker" target="_blank" rel="noopener noreferrer">18</a></sup> Remember, a Kafka Streams topology can be composed of multiple sub-topologies, so to get the number of tasks for the entire program, you should sum the task count across all sub-topologies.</p>
        <p data-type="footnote" id="idm46281564484520"><sup><a href="ch02.html#idm46281564484520-marker" target="_blank" rel="noopener noreferrer">19</a></sup> This doesn’t mean a poorly implemented stream processor is immune from concurrency issues. However, by default, the stream threads do not share any state.</p>
        <p data-type="footnote" id="idm46281564481048"><sup><a href="ch02.html#idm46281564481048-marker" target="_blank" rel="noopener noreferrer">20</a></sup> For example, the number of cores that your application has access to could inform the number of threads you decide to run with. If your application instance is running on a 4-core machine and your topology supports 16 tasks, you may want to configure the thread count to 4, which will give you a thread for each core. On the other hand, if your 16-task application was running on a 48-core machine, you may want to run with 16 threads (you wouldn’t run with 48 since the upper bound is the task count, or in this case: 16).</p>
        <p data-type="footnote" id="idm46281564464216"><sup><a href="ch02.html#idm46281564464216-marker" target="_blank" rel="noopener noreferrer">21</a></sup> From <a href="https://oreil.ly/Ry4nI" target="_blank" rel="noopener noreferrer"><em>First Principles: Elon Musk on the Power of Thinking for Yourself</em></a>.</p>
        <p data-type="footnote" id="idm46281564426728"><sup><a href="ch02.html#idm46281564426728-marker" target="_blank" rel="noopener noreferrer">22</a></sup> If you want to verify and have telnet installed, you can run <code>echo 'exit' | telnet localhost 29092</code>. If the port is open, you should see “Connected to localhost” in the output.</p>
        <p data-type="footnote" id="idm46281564421064"><sup><a href="ch02.html#idm46281564421064-marker" target="_blank" rel="noopener noreferrer">23</a></sup> Instructions for installing Gradle can be found at <a href="https://gradle.org" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://gradle.org</em></a>. We used version 6.6.1 for the tutorials in this book.</p>
        <p data-type="footnote" id="idm46281563984088"><sup><a href="ch02.html#idm46281563984088-marker" target="_blank" rel="noopener noreferrer">24</a></sup> This version of the <code>Processor</code> interface was introduced In Kafka Streams version 2.7 and deprecates an earlier version of the interface that was available in Kafka Streams 2.6 and earlier. In the earlier version of the <span class="keep-together"><code>Processor</code></span> interface, only input types are specified. This presented some issues with type-safety checks, so the newer form of the <code>Processor</code> interface is recommended.</p>
        <p data-type="footnote" id="idm46281563883144"><sup><a href="ch02.html#idm46281563883144-marker" target="_blank" rel="noopener noreferrer">25</a></sup> In fact, tables are sometimes referred to as <em>aggregated streams</em>. See <a href="https://oreil.ly/dgSCn" target="_blank" rel="noopener noreferrer">“Of Streams and Tables in Kafka and Stream Processing, Part 1” by Michael Noll</a>, which explores this topic further.</p>
        <p data-type="footnote" id="idm46281563862024"><sup><a href="ch02.html#idm46281563862024-marker" target="_blank" rel="noopener noreferrer">26</a></sup> RocksDB is a fast, embedded key-value store that was originally developed at Facebook. We will talk more about RocksDB and key-value stores in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#ch4" target="_blank" rel="noopener noreferrer">4</a>–<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#ch6" target="_blank" rel="noopener noreferrer">6</a>.</p>
        <p data-type="footnote" id="idm46281563855512"><sup><a href="ch02.html#idm46281563855512-marker" target="_blank" rel="noopener noreferrer">27</a></sup> To go even deeper with the analogy, the index position for each item in the list would represent the offset of the record in the underlying Kafka topic.</p>
        <p data-type="footnote" id="idm46281563824824"><sup><a href="ch02.html#idm46281563824824-marker" target="_blank" rel="noopener noreferrer">28</a></sup> Assuming your source topic contains more than one partition.</p>
      </div>
    </div>
    <div class="chapter" id="ch3">
      <h1><span class="label">Chapter 3. </span>Stateless Processing</h1>
      <p>The simplest form of stream processing requires no memory of previously seen events.<a data-type="indexterm" data-primary="stateless processing" id="ix_stlprc" target="_blank" rel="noopener noreferrer"></a> Each event is consumed, processed,<sup><a data-type="noteref" id="idm46281563801256-marker" href="ch03.html#idm46281563801256" target="_blank" rel="noopener noreferrer">1</a></sup> and subsequently forgotten. This paradigm is called <em>stateless processing</em>, and Kafka Streams includes a rich set of operators for working with data in a stateless way.</p>
      <p>In this chapter, we will explore the <em>stateless operators</em> that are included in Kafka Streams, and in doing so, we’ll see how some of the most common stream processing tasks can be tackled with ease.<a data-type="indexterm" data-primary="operators" data-secondary="stateless" id="idm46281563797144" target="_blank" rel="noopener noreferrer"></a> The topics we will explore include:</p>
      <ul>
        <li>
          <p>Filtering records</p></li>
        <li>
          <p>Adding and removing fields</p></li>
        <li>
          <p>Rekeying records</p></li>
        <li>
          <p>Branching streams</p></li>
        <li>
          <p>Merging streams</p></li>
        <li>
          <p>Transforming records into one or more outputs</p></li>
        <li>
          <p>Enriching records, one at a time</p></li>
      </ul>
      <p>We’ll take a tutorial-based approach for introducing these concepts. Specifically, we’ll be streaming data about cryptocurrencies from Twitter and applying some stateless operators to convert the raw data into something more meaningful: investment signals. By the end of this chapter, you will understand how to use stateless operators in Kafka Streams to enrich and transform raw data, which will prepare you for the more advanced concepts that we will explore in later chapters.</p>
      <p>Before we jump into the tutorial, let’s get a better frame of reference for what stateless processing is by comparing it to the other form of stream processing: <em>stateful</em> <span class="keep-together"><em>processing</em>.</span></p>
      <section data-type="sect1" data-pdf-bookmark="Stateless Versus Stateful Processing">
        <div class="sect1" id="idm46281563786264">
          <h1>Stateless Versus Stateful Processing</h1>
          <p>One of the most important things you should consider when building a Kafka Streams application is whether or not your application requires stateful processing.<a data-type="indexterm" data-primary="stateful processing" data-secondary="stateless processing versus" id="idm46281563784552" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stream processing" data-secondary="stateless versus stateful" id="idm46281563783512" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="versus stateful processing" data-secondary-sortas="stateful" id="idm46281563782552" target="_blank" rel="noopener noreferrer"></a> The following describes the distinction between stateless and stateful stream <span class="keep-together">processing:</span></p>
          <ul>
            <li>
              <p>In <em>stateless applications</em>, each event handled by your Kafka Streams application is processed independently of other events, and only <em>stream</em> views are needed by your application (see <a data-type="xref" href="ch02.html#C2_SECTION_MODEL" target="_blank" rel="noopener noreferrer">“Streams and Tables”</a>). In other words, your application treats each event as a self-contained insert and requires no memory of previously seen events.</p></li>
            <li>
              <p><em>Stateful applications</em>, on the other hand, need to remember information about previously seen events <em>in one or more steps</em> of your processor topology, usually for the purpose of aggregating, windowing, or joining event streams. These applications are more complex under the hood since they need to track additional data, or <em>state</em>.<a data-type="indexterm" data-primary="state" id="idm46281563775464" target="_blank" rel="noopener noreferrer"></a></p></li>
          </ul>
          <p>In the high-level DSL, the type of stream processing application you ultimately build boils down to the individual <em>operators</em> that are used in your topology.<sup><a data-type="noteref" id="idm46281563773128-marker" href="ch03.html#idm46281563773128" target="_blank" rel="noopener noreferrer">2</a></sup> Operators are <a data-type="indexterm" data-primary="operators" data-secondary="stateless and stateful" id="idm46281563771464" target="_blank" rel="noopener noreferrer"></a>stream processing functions (e.g., <code>filter</code>, <code>map</code>, <code>flatMap</code>, <code>join</code>, etc.) that are applied to events as they flow through your topology. Some operators, like <code>filter</code>, are considered <em>stateless</em> because they only need to look at the current record to perform an action (in this case, <code>filter</code> looks at each record individually to determine whether or not the record should be forwarded to downstream processors). Other operators, like <code>count</code>, are <em>stateful</em> since they require knowledge of previous events (<code>count</code> needs to know how many events it has seen so far in order to track the number of messages).<a data-type="indexterm" data-primary="stateful operators" id="idm46281563765464" target="_blank" rel="noopener noreferrer"></a></p>
          <p>If your Kafka Streams application requires <em>only</em> stateless operators (and therefore does not need to maintain any memory of previously seen events), then your application is considered <em>stateless</em>. However, if you introduce one or more stateful operators (which we will learn about in the next chapter), regardless of whether or not your application also uses stateless operators, then your application is considered <em>stateful</em>. The added complexity of stateful applications warrants additional considerations with regards to maintenance, scalability, and fault tolerance, so we will cover this form of stream processing separately in the next chapter.</p>
          <p>If all of this sounds a little abstract, don’t worry. We’ll demonstrate these concepts by building a stateless Kafka Streams application in the following sections, and getting some first-hand experience with stateless operators. So without further ado, let’s introduce this chapter’s tutorial.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Introducing Our Tutorial: Processing a Twitter Stream">
        <div class="sect1" id="idm46281563762040">
          <h1>Introducing Our Tutorial: Processing a Twitter Stream</h1>
          <p>In this<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" id="idm46281563760504" target="_blank" rel="noopener noreferrer"></a> tutorial, we will explore the use case of algorithmic trading.<a data-type="indexterm" data-primary="Twitter stream, processing" data-see="processing a Twitter stream tutorial" id="idm46281563759624" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" id="idm46281563758648" target="_blank" rel="noopener noreferrer"></a> Sometimes called <em>high-frequency trading</em> (HFT), this lucrative practice involves building software to evaluate and purchase securities automatically, by processing and responding to many types of market signals with minimal latency.<a data-type="indexterm" data-primary="high-frequency trading (HFT)" id="idm46281563756936" target="_blank" rel="noopener noreferrer"></a></p>
          <p>To assist<a data-type="indexterm" data-primary="data models" data-secondary="for cryptocurrency sentiment tutorial" data-secondary-sortas="cryptocurrency" id="idm46281563755832" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="sentiment analysis" id="idm46281563754536" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="cryptocurrencies, gauging market sentiment around" data-seealso="processing a Twitter stream tutorial" id="idm46281563753864" target="_blank" rel="noopener noreferrer"></a> our fictional trading software, we will build a stream processing application that will help us gauge market sentiment around different types of cryptocurrencies (Bitcoin, Ethereum, Ripple, etc.), and use these sentiment scores as investment/divestment signals in a custom trading algorithm.<sup><a data-type="noteref" id="idm46281563752376-marker" href="ch03.html#idm46281563752376" target="_blank" rel="noopener noreferrer">3</a></sup> Since millions of people use Twitter to share their thoughts on cryptocurrencies and other topics, we will use Twitter as the data source for our application.</p>
          <p>Before we get started, let’s look at the steps required to build our stream processing application. We will then use these requirements to design a processor topology, which will be a helpful guide as we build our stateless Kafka Streams application. The key concepts in each step are italicized:</p>
          <ol>
            <li>
              <p>Tweets that mention certain digital currencies (#bitcoin, #ethereum) should be consumed from a source topic called <code>tweets</code>:</p>
              <ul>
                <li>
                  <p>Since each record is JSON-encoded, we need to figure out how to properly <em>deserialize</em> these records into higher-level data classes.<a data-type="indexterm" data-primary="deserialization" data-seealso="serialization/deserialization" id="idm46281563747288" target="_blank" rel="noopener noreferrer"></a></p></li>
                <li>
                  <p>Unneeded fields should be removed during the deserialization process to simplify our code. Selecting only a subset of fields to work with is referred to as <em>projection</em>, and is one of the most common tasks in stream processing.</p></li>
              </ul></li>
            <li>
              <p>Retweets should be excluded from processing. This will involve some form of data <em>filtering</em>.<a data-type="indexterm" data-primary="filtering data" id="idm46281563743384" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="projection" id="idm46281563742648" target="_blank" rel="noopener noreferrer"></a></p></li>
            <li>
              <p>Tweets that aren’t written in English should be <em>branched</em> into a separate stream for translating.</p></li>
            <li>
              <p>Non-English tweets need to be translated to English. This involves <em>mapping</em> one input value (the non-English tweet) to a new output value (an English-translated tweet).<a data-type="indexterm" data-primary="mapping" id="idm46281563739208" target="_blank" rel="noopener noreferrer"></a></p></li>
            <li>
              <p>The newly translated tweets should be <em>merged</em> with the English tweets stream to create one unified stream.<a data-type="indexterm" data-primary="merging streams" id="idm46281563737256" target="_blank" rel="noopener noreferrer"></a></p></li>
            <li>
              <p>Each tweet should be enriched with a sentiment score, which indicates whether Twitter users are conveying positive or negative emotion when discussing certain digital currencies. Since a single tweet could mention multiple cryptocurrencies, we will demonstrate how to convert each input (tweet) into a variable number of outputs using a <code>flatMap</code> operator.<a data-type="indexterm" data-primary="flatMap operator" id="idm46281563734952" target="_blank" rel="noopener noreferrer"></a></p></li>
            <li>
              <p>The enriched tweets should be serialized using Avro, and written to an output topic called <code>crypto-sentiment</code>. Our fictional trading algorithm will read from this topic and make investment decisions based on the signals it sees.</p></li>
          </ol>
          <p>Now that the requirements have been captured, we can design our processor topology. <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a> shows what we’ll be building in this chapter and how data will flow through our Kafka Streams application.<a data-type="indexterm" data-primary="processor topologies" data-secondary="for tweet enrichment application" data-secondary-sortas="tweet" id="idm46281563731224" target="_blank" rel="noopener noreferrer"></a></p>
          <figure>
            <div id="C3_TOPOLOGY" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0301.png" style="width: 33rem">
              <h6><span class="label">Figure 3-1. </span>The topology that we will be implementing for our tweet enrichment <span class="keep-together">application</span></h6>
            </div>
          </figure>
          <p>With our topology design in hand, we can now start implementing our Kafka Streams application by working our way through each of the processing steps (labeled 1–7) in <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>. We will start by setting up our project, and then move on to the first step in our topology: streaming tweets from the source topic.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Project Setup">
        <div class="sect1" id="C3_PROJECT_SETUP">
          <h1>Project Setup</h1>
          <p>The code for this chapter is located at <a href="https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git</em></a>.<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="project setup" id="idm46281563722888" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="project setup" id="idm46281563721976" target="_blank" rel="noopener noreferrer"></a></p>
          <p>If you would like to reference the code as we work our way through each topology step, clone the repository and change to the directory containing this chapter’s tutorial. The following command will do the trick:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ git clone git@github.com:mitch-seymour/mastering-kafka-streams-and-ksqldb.git
$ cd mastering-kafka-streams-and-ksqldb/chapter-03/crypto-sentiment</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>You can build the project anytime by running the following command:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ ./gradlew build --info</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p>We have omitted the implementation details of tweet translation and sentiment analysis (steps 4 and 6 in <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>) since they aren’t necessary to demonstrate the stateless operators in Kafka Streams. However, the source code in GitHub does include a full working example, so please consult the project’s <em>README.md</em> file if you are interested in these implementation details.</p>
          </div>
          <p>Now that our project is set up, let’s start creating our Kafka Streams application.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Adding a KStream Source Processor">
        <div class="sect1" id="idm46281563714952">
          <h1>Adding a KStream Source Processor</h1>
          <p>All Kafka Streams applications have one thing in common: they consume data from one or more source topics.<a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="adding KStream source processor" id="idm46281563712584" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="source processors" data-secondary="adding to Twitter stream processing tutorial" id="idm46281563711304" target="_blank" rel="noopener noreferrer"></a> In this tutorial, we only have one source topic: <code>tweets</code>. This topic is populated with tweets from the <a href="https://oreil.ly/yvEoX" target="_blank" rel="noopener noreferrer">Twitter source connector</a>, which streams tweets from Twitter’s streaming API and writes JSON-encoded tweet records to Kafka. An example tweet value<sup><a data-type="noteref" id="idm46281563708920-marker" href="ch03.html#idm46281563708920" target="_blank" rel="noopener noreferrer">4</a></sup> is shown in <a data-type="xref" href="#C3_TWEET_JSON" target="_blank" rel="noopener noreferrer">Example&nbsp;3-1</a>.</p>
          <div id="C3_TWEET_JSON" data-type="example" class="pagebreak-before">
            <h5><span class="label">Example 3-1. </span>Example record value in the <code>tweets</code> source topic</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">{
    "CreatedAt": 1602545767000,
    "Id": 1206079394583924736,
    "Text": "Anyone else buying the Bitcoin dip?",
    "Source": "",
    "User": {
        "Id": "123",
        "Name": "Mitch",
        "Description": "",
        "ScreenName": "timeflown",
        "URL": "https://twitter.com/timeflown",
        "FollowersCount": "1128",
        "FriendsCount": "1128"
    }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <p>Now that we know what the data looks like, the first step we need to tackle is getting the data from our source topic into our Kafka Streams application. In the previous chapter, we learned that we can use the <code>KStream</code> abstraction to represent a stateless record stream. <a data-type="indexterm" data-primary="KStream class" data-secondary="source processor for processing Twitter stream" id="idm46281563703240" target="_blank" rel="noopener noreferrer"></a>As you can see in the following code block, adding a <code>KStream</code> source processor in Kafka Streams is simple and requires just a couple of lines of code:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">StreamsBuilder builder = new StreamsBuilder(); <a class="co" id="co_stateless_processing_CO1-1" href="#callout_stateless_processing_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

KStream&lt;byte[], byte[]&gt; stream = builder.stream("tweets"); <a class="co" id="co_stateless_processing_CO1-2" href="#callout_stateless_processing_CO1-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateless_processing_CO1-1" href="#co_stateless_processing_CO1-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>When using the high-level DSL, processor topologies are built using a <span class="keep-together"><code>StreamsBuilder</code></span> instance.<a data-type="indexterm" data-primary="StreamsBuilder class" id="idm46281563693368" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO1-2" href="#co_stateless_processing_CO1-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p><code>KStream</code> instances are created by passing the topic name to the <span class="keep-together"><code>StreamsBuilder.stream</code></span> method. The <code>stream</code> method can optionally accept additional parameters, which we will explore in later sections.</p>
            </dd>
          </dl>
          <p>One thing you may notice is that the <code>KStream</code> we just created is parameterized with <code>byte[]</code> types:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KStream&lt;byte[], byte[]&gt;</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>We briefly touched on this in the previous chapter, but the <code>KStream</code> interface leverages two generics: one for specifying the type of keys (<code>K</code>) in our Kafka topic and the other for specifying the type of values (<code>V</code>). If we were to peel back the floorboards in the Kafka Streams library, we would see an interface that looks like this:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">public interface KStream&lt;K, V&gt; {
  // omitted for brevity
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Therefore, our <code>KStream</code> instance, which is parameterized as <code>KStream&lt;byte[], byte[]&gt;</code>, indicates that the record keys and values coming out of the <code>tweets</code> topic are being encoded as byte arrays. However, we just mentioned that the tweet records are actually encoded as JSON objects by the source connector (see <a data-type="xref" href="#C3_TWEET_JSON" target="_blank" rel="noopener noreferrer">Example&nbsp;3-1</a>), so what gives?</p>
          <p>Kafka Streams, <em>by default</em>, represents data flowing through our application as byte arrays.<a data-type="indexterm" data-primary="byte arrays" data-secondary="storage and transmission of data in Kafka" id="idm46281563680072" target="_blank" rel="noopener noreferrer"></a> This is due to the fact that Kafka itself stores and transmits data as raw byte sequences, so representing the data as a byte array will always work (and is therefore a sensible default). Storing and transmitting raw bytes makes Kafka flexible because it doesn’t impose any particular data format on its clients, and also fast, since it requires less memory and CPU cycles on the brokers to transfer a raw byte stream over the network.<sup><a data-type="noteref" id="idm46281563678424-marker" href="ch03.html#idm46281563678424" target="_blank" rel="noopener noreferrer">5</a></sup> However, this means that Kafka clients, including Kafka Streams applications, are responsible for serializing and deserializing these byte streams<a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams into higher-level objects" id="idm46281563677288" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="JSON" data-secondary="working with in Kafka Streams application" id="idm46281563676312" target="_blank" rel="noopener noreferrer"></a> in order to work with higher-level objects and formats, including strings (delimited or non-delimited), JSON, Avro, Protobuf, etc.<sup><a data-type="noteref" id="idm46281563675064-marker" href="ch03.html#idm46281563675064" target="_blank" rel="noopener noreferrer">6</a></sup></p>
          <p>Before we address the issue of deserializing our tweet records into higher-level objects, let’s add the additional boilerplate code needed to run our Kafka Streams application.<a data-type="indexterm" data-primary="deserialization" data-secondary="deserializing tweet records into higher-level objects" id="idm46281563673400" target="_blank" rel="noopener noreferrer"></a> For testability purposes, it’s often beneficial to separate the logic for building a Kafka Streams topology from the code that actually runs the application.<a data-type="indexterm" data-primary="Java" data-secondary="class defining Kafka Streams topology" id="idm46281563672088" target="_blank" rel="noopener noreferrer"></a> So the boilerplate code will include two classes. First, we’ll define a class for building our Kafka Streams topology, as shown in <a data-type="xref" href="#C3_BOILER_PLATE" target="_blank" rel="noopener noreferrer">Example&nbsp;3-2</a>.</p>
          <div id="C3_BOILER_PLATE" data-type="example">
            <h5><span class="label">Example 3-2. </span>A Java class that defines our Kafka Streams topology</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">class CryptoTopology {

  public static Topology build() {
    StreamsBuilder builder = new StreamsBuilder();

    KStream&lt;byte[], byte[]&gt; stream = builder.stream("tweets");
    stream.print(Printed.&lt;byte[], byte[]&gt;toSysOut().withLabel("tweets-stream")); <a class="co" id="co_stateless_processing_CO2-1" href="#callout_stateless_processing_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

    return builder.build();
  }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateless_processing_CO2-1" href="#co_stateless_processing_CO2-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>print</code> operator allows us to easily view data as it flows through our application. It is generally recommended for development use only.<a data-type="indexterm" data-primary="print operator" id="idm46281563662376" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
          </dl>
          <p>The<a data-type="indexterm" data-primary="Java" data-secondary="class to run Kafka Streams application" id="idm46281563661160" target="_blank" rel="noopener noreferrer"></a> second class, which we’ll call <code>App</code>, will simply instantiate and run the topology, as shown in <a data-type="xref" href="#C3_BOILER_PLATE_APP_CLASS" target="_blank" rel="noopener noreferrer">Example&nbsp;3-3</a>.</p>
          <div id="C3_BOILER_PLATE_APP_CLASS" data-type="example">
            <h5><span class="label">Example 3-3. </span>A separate Java class used to run our Kafka Streams application</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">class App {
  public static void main(String[] args) {
    Topology topology = CryptoTopology.build();

    Properties config = new Properties(); <a class="co" id="co_stateless_processing_CO3-1" href="#callout_stateless_processing_CO3-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    config.put(StreamsConfig.APPLICATION_ID_CONFIG, "dev");
    config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:29092");

    KafkaStreams streams = new KafkaStreams(topology, config); <a class="co" id="co_stateless_processing_CO3-2" href="#callout_stateless_processing_CO3-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

    Runtime.getRuntime().addShutdownHook(new Thread(streams::close)); <a class="co" id="co_stateless_processing_CO3-3" href="#callout_stateless_processing_CO3-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>

    System.out.println("Starting Twitter streams");
    streams.start(); <a class="co" id="co_stateless_processing_CO3-4" href="#callout_stateless_processing_CO3-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
  }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateless_processing_CO3-1" href="#co_stateless_processing_CO3-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Kafka Streams requires us to set some basic configuration, including an application ID (which corresponds to a consumer group) and the Kafka bootstrap servers. We set these configs using a <code>Properties</code> object.<a data-type="indexterm" data-primary="Properties object" id="idm46281563645320" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO3-2" href="#co_stateless_processing_CO3-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Instantiate a <code>KafkaStreams</code> object with the processor topology and streams <span class="keep-together">config.</span></p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO3-3" href="#co_stateless_processing_CO3-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Add a shutdown hook to gracefully stop the Kafka Streams application when a global shutdown signal is received.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO3-4" href="#co_stateless_processing_CO3-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Start the Kafka Streams application. Note that <code>streams.start()</code> does not block, and the topology is executed via background processing threads. This is the reason why a shutdown hook is required.</p>
            </dd>
          </dl>
          <p>Our application is now ready to run. If we were to start our Kafka Streams application and then produce some data to our <code>tweets</code> topic, we would see the raw byte arrays (the cryptic values that appear after the comma in each output row) being printed to the screen:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">[tweets-stream]: null, [B@c52d992
[tweets-stream]: null, [B@a4ec036
[tweets-stream]: null, [B@3812c614</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>As you might expect, the low-level nature of byte arrays makes them a little difficult to work with. In fact, additional stream processing steps will be much easier to <span class="keep-together">implement</span> if we find a different method of representing the data in our source topic. This is where the concepts of data serialization and deserialization come into play.<a data-type="indexterm" data-primary="byte arrays" data-seealso="serialization/deserialization" id="idm46281563632040" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Serialization/Deserialization">
        <div class="sect1" id="C3_SERDES_SECTION">
          <h1>Serialization/Deserialization</h1>
          <p>Kafka is a bytes-in, bytes-out stream processing platform. This means that clients, like Kafka Streams, are responsible for converting the byte streams they consume into higher-level objects.<a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="serialization/deserialization of byte streams" id="ix_stlprcserdes" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serialization/deserialization" id="ix_proctutserdes" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams to/from higher-level objects" id="idm46281563626296" target="_blank" rel="noopener noreferrer"></a> This process is called <em>deserialization</em>. Similarly, clients must also convert any data they want to write back to Kafka back into byte arrays. This process is called <em>serialization</em>. These processes are depicted in <a data-type="xref" href="#C3_SERDES" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-2</a>.</p>
          <figure>
            <div id="C3_SERDES" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0302.png" style="width: 30rem">
              <h6><span class="label">Figure 3-2. </span>An architectural view of where the deserialization and serialization processes occur in a Kafka Streams application</h6>
            </div>
          </figure>
          <p>In Kafka Streams, serializer and deserializer classes are often combined into a single <a data-type="indexterm" data-primary="Serdes classes" id="idm46281563620744" target="_blank" rel="noopener noreferrer"></a>class called a <em>Serdes</em>, and the library ships with several implementations, shown in <a data-type="xref" href="#C3_DEFAULT_SERDES" target="_blank" rel="noopener noreferrer">Table&nbsp;3-1</a>.<sup><a data-type="noteref" id="idm46281563618664-marker" href="ch03.html#idm46281563618664" target="_blank" rel="noopener noreferrer">7</a></sup> For example, the String Serdes (accessible via the <code>Serdes.String()</code> method) includes both the String serializer <em>and</em> deserializer class.</p>
          <table id="C3_DEFAULT_SERDES">
            <caption>
              <span class="label">Table 3-1. </span>Default Serdes implementations that are available in Kafka Streams
            </caption>
            <thead>
              <tr>
                <th>Data type</th>
                <th>Serdes class</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <p>byte[]</p></td>
                <td>
                  <p><code>Serdes.ByteArray()</code>, <code>Serdes.Bytes()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>ByteBuffer</p></td>
                <td>
                  <p><code>Serdes.ByteBuffer()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Double</p></td>
                <td>
                  <p><code>Serdes.Double()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Integer</p></td>
                <td>
                  <p><code>Serdes.Integer()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Long</p></td>
                <td>
                  <p><code>Serdes.Long()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>String</p></td>
                <td>
                  <p><code>Serdes.String()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>UUID</p></td>
                <td>
                  <p><code>Serdes.UUID()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Void</p></td>
                <td>
                  <p><code>Serdes.Void()</code></p></td>
              </tr>
            </tbody>
          </table>
          <p>Whenever you need to deserialize/serialize data in Kafka Streams, you should first check to see whether or not one of the built-in Serdes classes fits your needs. However, as you may have noticed, Kafka Streams doesn’t ship with Serdes classes for some common formats, including JSON,<sup><a data-type="noteref" id="idm46281563597864-marker" href="ch03.html#idm46281563597864" target="_blank" rel="noopener noreferrer">8</a></sup> Avro, and Protobuf. However, we can implement our own Serdes when the need arises, and since tweets are represented <span class="keep-together">as JSON</span> objects, we will learn how to build our own custom Serdes to handle this <span class="keep-together">format</span> next.</p>
          <section data-type="sect2" data-pdf-bookmark="Building a Custom Serdes">
            <div class="sect2" id="idm46281563595048">
              <h2>Building a Custom Serdes</h2>
              <p>As mentioned earlier, the tweets in our source topic are encoded as JSON objects, but only the raw bytes are stored in Kafka.<a data-type="indexterm" data-primary="Serdes classes" data-secondary="building a custom Serdes" id="idm46281563593336" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams to/from higher-level objects" data-tertiary="building a custom Serdes" id="idm46281563592296" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serialization/deserialization" data-tertiary="building a custom Serdes" id="idm46281563591016" target="_blank" rel="noopener noreferrer"></a> So, the first thing we will do is write some code for deserializing tweets as higher-level JSON objects, which will ensure the data is easy to work with in our stream processing application. We could just use the built-in String Serdes, <code>Serdes.String()</code>, instead of implementing our own, but that would make working with the Twitter data difficult, since we couldn’t easily access each field in the tweet object.<sup><a data-type="noteref" id="idm46281563588808-marker" href="ch03.html#idm46281563588808" target="_blank" rel="noopener noreferrer">9</a></sup></p>
              <p>If your data is in a common format, like JSON or Avro, then you won’t need to reinvent the wheel by implementing your own low-level JSON serialization/deserialization logic.<a data-type="indexterm" data-primary="JSON" data-secondary="serialization/deserialization using Gson library" id="idm46281563587560" target="_blank" rel="noopener noreferrer"></a> There are many Java libraries for serializing and deserializing JSON, but the one we will be using in this tutorial is <a href="https://oreil.ly/C2_5K" target="_blank" rel="noopener noreferrer">Gson</a>. Gson was developed by Google and has an intuitive API for converting JSON bytes to Java objects.<a data-type="indexterm" data-primary="Gson library" id="idm46281563585512" target="_blank" rel="noopener noreferrer"></a> The following code block demonstrates the basic method for deserializing byte arrays with Gson, which will come in handy whenever we need to read JSON records from Kafka:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">Gson gson = new Gson();
byte[] bytes = ...; <a class="co" id="co_stateless_processing_CO4-1" href="#callout_stateless_processing_CO4-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
Type type = ...; <a class="co" id="co_stateless_processing_CO4-2" href="#callout_stateless_processing_CO4-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
gson.fromJson(new String(bytes), type); <a class="co" id="co_stateless_processing_CO4-3" href="#callout_stateless_processing_CO4-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO4-1" href="#co_stateless_processing_CO4-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The raw bytes that we need to deserialize.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO4-2" href="#co_stateless_processing_CO4-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p><code>type</code> is a Java class that will be used to represent the deserialized record.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO4-3" href="#co_stateless_processing_CO4-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>fromJson</code> method actually converts the raw bytes into a Java class.</p>
                </dd>
              </dl>
              <p>Furthermore, Gson also supports the inverse of this process, which is to say it allows us to convert (i.e., serialize) Java objects into raw byte arrays. The following code block shows how serialization works in Gson:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">Gson gson = new Gson();
gson.toJson(instance).getBytes(StandardCharsets.UTF_8);</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Since the Gson library takes care of the more complex task of serializing/deserializing JSON at a low level, we just need to leverage Gson’s capabilities when implementing a custom Serdes. The first step is to define a data class, which is what the raw byte arrays will be deserialized into.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Defining Data Classes">
            <div class="sect2" id="idm46281563567336">
              <h2>Defining Data Classes</h2>
              <p>One feature of Gson (and some other JSON serialization libraries, like Jackson) is that it allows us to convert JSON byte arrays into Java objects.<a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams to/from higher-level objects" data-tertiary="defining data classes" id="idm46281563565608" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serialization/deserialization" data-tertiary="defining data classes" id="idm46281563564360" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="data classes" data-secondary="defining class for deserializing tweet records" id="idm46281563563160" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Java" data-secondary="converting JSON into Java objects with Gson" id="idm46281563562184" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="plain old Java objects (POJOs)" data-secondary="defining class for deserializing tweet records" id="idm46281563561208" target="_blank" rel="noopener noreferrer"></a> In order to do this, we simply need to define a data class, or POJO (Plain Old Java Object), that contains the fields that we want to deserialize from the source object.</p>
              <p>As you can see in <a data-type="xref" href="#C3_DATA_CLASS" target="_blank" rel="noopener noreferrer">Example&nbsp;3-4</a>, our data class for representing raw tweet records in our Kafka Streams application is pretty simple. We simply define a class property for each field we want to capture from the raw tweet (e.g., <code>createdAt</code>), and a corresponding getter/setter method for accessing each property (e.g., <code>getCreatedAt</code>).</p>
              <div id="C3_DATA_CLASS" data-type="example">
                <h5><span class="label">Example 3-4. </span>A data class to use for deserializing tweet records</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">public class Tweet {
  private Long createdAt;
  private Long id;
  private String lang;
  private Boolean retweet;
  private String text;

  // getters and setters omitted for brevity
}</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <p>If we don’t want to capture certain fields in the source record, then we can just omit the field from the data class, and Gson will drop it automatically. Whenever you create your own deserializer, you should consider dropping any fields in the source record that aren’t needed by your application or downstream applications during the deserialization process. This process of reducing the available fields to a smaller subset is called <em>projection</em> and is similar to using a <code>SELECT</code> statement in the SQL world to only select the columns of interest.<a data-type="indexterm" data-primary="projection" id="idm46281563553752" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="SELECT statements" id="idm46281563553048" target="_blank" rel="noopener noreferrer"></a></p>
              <p>Now that we have created our data class, we can implement a Kafka Streams deserializer for converting the raw byte arrays from the <code>tweets</code> topic to higher-level <code>Tweet</code> Java objects (which will be much easier to work with).</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Implementing a Custom Deserializer">
            <div class="sect2" id="idm46281563550792">
              <h2>Implementing a Custom Deserializer</h2>
              <p>The code required for implementing a custom deserializer is pretty minimal, <span class="keep-together">especially</span> when you leverage a library that hides most of the complexity of deserializing byte arrays (as we’re doing with Gson).<a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams to/from higher-level objects" data-tertiary="implementing custom deserializer" id="idm46281563548200" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serialization/deserialization" data-tertiary="implementing custom deserializer" id="idm46281563546888" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Deserializer interface" id="idm46281563545624" target="_blank" rel="noopener noreferrer"></a> We simply need to implement the <span class="keep-together"><code>Deserializer</code></span> interface in the Kafka client library, and invoke the deserialization logic whenever <span class="keep-together"><code>deserialize</code></span> is invoked. The following code block shows how to implement the <code>Tweet</code> deserializer:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">public class TweetDeserializer implements Deserializer&lt;Tweet&gt; {
  private Gson gson =
      new GsonBuilder()
          .setFieldNamingPolicy(FieldNamingPolicy.UPPER_CAMEL_CASE) <a class="co" id="co_stateless_processing_CO5-1" href="#callout_stateless_processing_CO5-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
          .create();

  @Override
  public Tweet deserialize(String topic, byte[] bytes) { <a class="co" id="co_stateless_processing_CO5-2" href="#callout_stateless_processing_CO5-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    if (bytes == null) return null; <a class="co" id="co_stateless_processing_CO5-3" href="#callout_stateless_processing_CO5-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    return gson.fromJson(
      new String(bytes, StandardCharsets.UTF_8), Tweet.class); <a class="co" id="co_stateless_processing_CO5-4" href="#callout_stateless_processing_CO5-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
  }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO5-1" href="#co_stateless_processing_CO5-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Gson supports several different formats for JSON field names. Since the Twitter Kafka connector uses upper camel case for field names, we set the appropriate field naming policy to ensure the JSON objects are deserialized correctly. This is pretty implementation-specific, but when you write your own deserializer, feel free to leverage third-party libraries and custom configurations like we’re doing here to help with the heavy lifting of byte array deserialization.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO5-2" href="#co_stateless_processing_CO5-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We override the <code>deserialize</code> method with our own logic for deserializing records in the <code>tweets</code> topic. This method returns an instance of our data class (<code>Tweet</code>).</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO5-3" href="#co_stateless_processing_CO5-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Don’t try to deserialize the byte array if it’s null.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO5-4" href="#co_stateless_processing_CO5-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Use the Gson library to deserialize the byte array into a <code>Tweet</code> object.</p>
                </dd>
              </dl>
              <p>Now, we are ready to implement our serializer.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Implementing a Custom Serializer">
            <div class="sect2" id="idm46281563521080">
              <h2>Implementing a Custom Serializer</h2>
              <p>The code <a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams to/from higher-level objects" data-tertiary="implementing custom serializer" id="idm46281563519480" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serialization/deserialization" data-tertiary="implementing custom serializer" id="idm46281563518120" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Serializer interface" id="idm46281563516856" target="_blank" rel="noopener noreferrer"></a>for implementing a custom serializer is also very straightforward. This time, we need to implement the <code>serialize</code> method of the <code>Serializer</code> interface that is included in the Kafka client library. Again, we will leverage Gson’s serialization capabilities to do most of the heavy lifting. The following code block shows the <code>Tweet</code> serializer we will be using in this chapter:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">class TweetSerializer implements Serializer&lt;Tweet&gt; {
  private Gson gson = new Gson();

  @Override
  public byte[] serialize(String topic, Tweet tweet) {
    if (tweet == null) return null; <a class="co" id="co_stateless_processing_CO6-1" href="#callout_stateless_processing_CO6-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    return gson.toJson(tweet).getBytes(StandardCharsets.UTF_8); <a class="co" id="co_stateless_processing_CO6-2" href="#callout_stateless_processing_CO6-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO6-1" href="#co_stateless_processing_CO6-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Don’t try to deserialize a null <code>Tweet</code> object.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO6-2" href="#co_stateless_processing_CO6-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>If the <code>Tweet</code> object is not null, use Gson to convert the object into a byte array.</p>
                </dd>
              </dl>
              <p>With both a <code>Tweet</code> deserializer <em>and</em> serializer in place, we can now combine these two classes into a Serdes.</p>
            </div>
          </section>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Building the Tweet Serdes">
            <div class="sect2" id="C3_TWEET_SERDES">
              <h2>Building the Tweet Serdes</h2>
              <p>So far, the deserializer and serializer implementations have been very light on code.<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serialization/deserialization" data-tertiary="building a Tweet Serdes" id="idm46281563499912" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams to/from higher-level objects" data-tertiary="building a Tweet Serdes" id="idm46281563498584" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Serdes classes" data-secondary="building a Tweet Serdes" id="idm46281563497320" target="_blank" rel="noopener noreferrer"></a> Similarly, our custom Serdes class, whose sole purpose is to combine the deserializer and serializer in a convenient wrapper class for Kafka Streams to use, is also pretty minimal. The following code block shows how easy it is to implement a custom Serdes class:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">public class TweetSerdes implements Serde&lt;Tweet&gt; {

  @Override
  public Serializer&lt;Tweet&gt; serializer() {
    return new TweetSerializer();
  }

  @Override
  public Deserializer&lt;Tweet&gt; deserializer() {
    return new TweetDeserializer();
  }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Now that our custom Serdes is in place, we can make a slight modification to the code in <a data-type="xref" href="#C3_BOILER_PLATE" target="_blank" rel="noopener noreferrer">Example&nbsp;3-2</a>. Let’s change:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KStream&lt;byte[], byte[]&gt; stream = builder.stream("tweets");
stream.print(Printed.&lt;byte[], byte[]&gt;toSysOut().withLabel("tweets-stream"));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>to:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KStream&lt;byte[], Tweet&gt; stream = <a class="co" id="co_stateless_processing_CO7-1" href="#callout_stateless_processing_CO7-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  builder.stream(
    "tweets",
    Consumed.with(Serdes.ByteArray(), new TweetSerdes())); <a class="co" id="co_stateless_processing_CO7-2" href="#callout_stateless_processing_CO7-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

stream.print(Printed.&lt;byte[], Tweet&gt;toSysOut().withLabel("tweets-stream"));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO7-1" href="#co_stateless_processing_CO7-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Notice that our value type has changed from <code>byte[]</code> to <code>Tweet</code>. Working with <code>Tweet</code> instances will make our life much easier, as you will see in upcoming <span class="keep-together">sections.</span></p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO7-2" href="#co_stateless_processing_CO7-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Explicitly set the key and value Serdes to use for this <code>KStream</code> using Kafka Stream’s <code>Consumed</code> helper.<a data-type="indexterm" data-primary="Consumed helper class" id="idm46281563479512" target="_blank" rel="noopener noreferrer"></a> By setting the value Serdes to <code>new TweetSerdes()</code>, our stream will now be populated with <code>Tweet</code> objects instead of byte arrays. The key Serdes remains unchanged.</p>
                </dd>
              </dl>
              <p>Note, the keys are still being serialized as byte arrays. This is because our topology doesn’t require us to do anything with the record keys, so there is no point in <span class="keep-together">deserializing</span> these.<sup><a data-type="noteref" id="idm46281563476520-marker" href="ch03.html#idm46281563476520" target="_blank" rel="noopener noreferrer">10</a></sup> Now, if we were to run our Kafka Streams application and produce some data to the source topic, you would see that we are now working with <code>Tweet</code> objects, which include the helpful getter methods for accessing the original JSON fields that we will leverage in later stream processing steps:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">[tweets-stream]: null, Tweet@182040a6
[tweets-stream]: null, Tweet@46efe0cb
[tweets-stream]: null, Tweet@176ef3db</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Now that we have created a <code>KStream</code> that leverages our <code>Tweet</code> data class, we can start implementing the rest of the topology. The next step is to filter out any retweets from our Twitter stream.<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serialization/deserialization of byte streams" data-startref="ix_proctutserdes" id="idm46281563472744" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="serialization/deserialization of byte streams" data-startref="ix_stlprcserdes" id="idm46281563471336" target="_blank" rel="noopener noreferrer"></a></p>
              <div data-type="tip" id="C3_EXCEPTION_HANDLER_TIP">
                <h1>Handling Deserialization Errors in Kafka Streams</h1>
                <p>You should always be explicit with how your application handles deserialization errors, otherwise you may get an unwelcome surprise if a malformed record ends up in your source topic. <a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="byte streams to/from higher-level objects" data-tertiary="being explicit with deserialization errors" id="idm46281563468072" target="_blank" rel="noopener noreferrer"></a>Kafka Streams has a special config, <code>DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG</code>, that can be used to specify a deserialization exception handler. You can implement your own exception handler, or use one of the built-in defaults, including <code>LogAndFailExceptionHandler</code> (which logs an error and then sends a shutdown signal to Kafka Streams) or <code>LogAndContinueExceptionHandler</code> (which logs an error and continues processing).</p>
              </div>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Filtering Data">
        <div class="sect1" id="idm46281563630968">
          <h1>Filtering Data</h1>
          <p>One of the most common stateless tasks in a stream processing application is filtering data.<a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="filtering data" id="ix_stlprcfil" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="filtering data" id="ix_proctutfil" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="filtering data" id="ix_fltr" target="_blank" rel="noopener noreferrer"></a> Filtering involves selecting only a subset of records to be processed, and ignoring the rest. <a data-type="xref" href="#C3_FILTERIMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-3</a> shows the basic idea of filtering.</p>
          <figure>
            <div id="C3_FILTERIMG" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0303.png" style="width: 29rem">
              <h6><span class="label">Figure 3-3. </span>Filtering data allows us to process only a subset of data in our event stream</h6>
            </div>
          </figure>
          <p>In Kafka Streams, the two primary operators<sup><a data-type="noteref" id="idm46281563456088-marker" href="ch03.html#idm46281563456088" target="_blank" rel="noopener noreferrer">11</a></sup> that are used for filtering are:</p>
          <ul>
            <li>
              <p><code>filter</code></p></li>
            <li>
              <p><code>filterNot</code></p></li>
          </ul>
          <p>We’ll start by <a data-type="indexterm" data-primary="filter operator" id="idm46281563450488" target="_blank" rel="noopener noreferrer"></a>exploring the <code>filter</code> operator. <code>filter</code> simply requires us to pass in a Boolean expression, called a <code>Predicate</code>, in order to determine if a message should be kept.<a data-type="indexterm" data-primary="predicates" data-secondary="using to filter data" id="idm46281563448280" target="_blank" rel="noopener noreferrer"></a> If the predicate returns <code>true</code>, the event will be forwarded to downstream processors. If it returns <code>false</code>, then the record will be excluded from further processing. Our use case requires us to filter out retweets, so our predicate will leverage the <code>Tweet.isRetweet()</code> method.</p>
          <p>Since <code>Predicate</code> is a functional interface,<sup><a data-type="noteref" id="idm46281563445032-marker" href="ch03.html#idm46281563445032" target="_blank" rel="noopener noreferrer">12</a></sup> we can use a lambda with the <code>filter</code> operator. <a data-type="xref" href="#C3_FILTER" target="_blank" rel="noopener noreferrer">Example&nbsp;3-5</a> shows how to filter out retweets using this method.</p>
          <div id="C3_FILTER" data-type="example">
            <h5><span class="label">Example 3-5. </span>An example of how to use the DSL’s <code>filter</code> operator</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">KStream&lt;byte[], Tweet&gt; filtered =
  stream.filter(
      (key, tweet) -&gt; {
        return !tweet.isRetweet();
      });</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <p>The <code>filterNot</code> operator is very similar to <code>filter</code>, except<a data-type="indexterm" data-primary="filterNot operator" id="idm46281563438024" target="_blank" rel="noopener noreferrer"></a> the Boolean logic is inverted (i.e., returning <code>true</code> will result in the record being dropped, which is the opposite of <code>filter</code>). As you can see in <a data-type="xref" href="#C3_FILTER" target="_blank" rel="noopener noreferrer">Example&nbsp;3-5</a>, our filtering condition is negated: <code>!tweet.isRetweet()</code>. We could have just as easily negated at the operator level instead, using the code in <a data-type="xref" href="#C3_FILTER_NOT" target="_blank" rel="noopener noreferrer">Example&nbsp;3-6</a>.</p>
          <div id="C3_FILTER_NOT" data-type="example">
            <h5><span class="label">Example 3-6. </span>An example of how to use the DSL’s <code>filterNot</code> operator</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">KStream&lt;byte[], Tweet&gt; filtered =
  stream.filterNot(
      (key, tweet) -&gt; {
        return tweet.isRetweet();
      });</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <p>These two approaches are functionally equivalent. However, when the filtering logic contains a negation, I prefer to use <code>filterNot</code> to improve readability. For this <span class="keep-together">tutorial</span>, we will use the <code>filterNot</code> implementation shown in <a data-type="xref" href="#C3_FILTER_NOT" target="_blank" rel="noopener noreferrer">Example&nbsp;3-6</a> to exclude retweets from further processing.</p>
          <div data-type="tip">
            <h6>Tip</h6>
            <p>If your application requires a filtering condition, you should filter as early as possible. There’s no point in transforming or enriching data that will simply be thrown away in a subsequent step, especially if the logic for processing the unneeded event is computationally expensive.</p>
          </div>
          <p>We have now implemented steps 1 and 2 in our processor topology (<a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>). We are ready to move on to the third step: separating our filtered stream into substreams based on the source language of the tweet.<a data-type="indexterm" data-primary="filtering data" data-startref="ix_fltr" id="idm46281563425752" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="filtering data" data-startref="ix_stlprcfil" id="idm46281563424808" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="filtering data" data-startref="ix_proctutfil" id="idm46281563423256" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" class="less_space" data-pdf-bookmark="Branching Data">
        <div class="sect1" id="idm46281563464344">
          <h1>Branching Data</h1>
          <p>In the previous section, we learned how to use Boolean conditions called predicates to filter streams.<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="branching data" id="idm46281563419944" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="branching data" id="idm46281563418952" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="branching data" id="idm46281563417720" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="predicates" data-secondary="using to branch streams" id="idm46281563417048" target="_blank" rel="noopener noreferrer"></a> Kafka Streams also allows us to use predicates to separate (or <em>branch</em>) streams. Branching is typically required when events need to be routed to different stream processing steps or output topics based on some attribute of the event itself.</p>
          <p>Revisiting our requirements list, we see that tweets can appear in multiple languages in our source topic. This is a great use case for branching, since a subset of records in our stream (non-English tweets) require an extra processing step: they need to be translated to English. The diagram in <a data-type="xref" href="#C3_BRANCH" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-4</a> depicts the branching behavior we need to implement in our application.</p>
          <figure>
            <div id="C3_BRANCH" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0304.png" style="width: 27rem">
              <h6><span class="label">Figure 3-4. </span>Branching operations split a single stream into multiple output streams</h6>
            </div>
          </figure>
          <p>Let’s create two lambda functions: one for capturing English tweets, and another for capturing everything else. Thanks to our earlier deserialization efforts, we can leverage another getter method in our data class to implement our branching logic: <code>Tweet.getLang()</code>. The following code block shows the predicates we will use for branching our stream:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">Predicate&lt;byte[], Tweet&gt; englishTweets =
  (key, tweet) -&gt; tweet.getLang().equals("en"); <a class="co" id="co_stateless_processing_CO8-1" href="#callout_stateless_processing_CO8-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

Predicate&lt;byte[], Tweet&gt; nonEnglishTweets =
  (key, tweet) -&gt; !tweet.getLang().equals("en"); <a class="co" id="co_stateless_processing_CO8-2" href="#callout_stateless_processing_CO8-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateless_processing_CO8-1" href="#co_stateless_processing_CO8-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>This predicate matches all English tweets.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO8-2" href="#co_stateless_processing_CO8-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>This predicate is the inverse of the first, and captures all non-English tweets.</p>
            </dd>
          </dl>
          <p>Now that we have defined our branching conditions, we can leverage Kafka Streams’ <code>branch</code> operator, which accepts one or more predicates and returns a list of output streams that correspond to each predicate.<a data-type="indexterm" data-primary="branch operator" id="idm46281563399816" target="_blank" rel="noopener noreferrer"></a> Note that each predicate is evaluated in order, and a record can only be added to a single branch. If a record doesn’t match any predicate, then it will be dropped:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KStream&lt;byte[], Tweet&gt;[] branches =
  filtered.branch(englishTweets, nonEnglishTweets); <a class="co" id="co_stateless_processing_CO9-1" href="#callout_stateless_processing_CO9-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

KStream&lt;byte[], Tweet&gt; englishStream = branches[0]; <a class="co" id="co_stateless_processing_CO9-2" href="#callout_stateless_processing_CO9-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

KStream&lt;byte[], Tweet&gt; nonEnglishStream = branches[1]; <a class="co" id="co_stateless_processing_CO9-3" href="#callout_stateless_processing_CO9-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateless_processing_CO9-1" href="#co_stateless_processing_CO9-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>We create two branches of our stream by evaluating the source language of a tweet.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO9-2" href="#co_stateless_processing_CO9-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Since we passed the <code>englishTweets</code> predicate first, the first <code>KStream</code> in our list (at the 0 index) contains the English output stream.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO9-3" href="#co_stateless_processing_CO9-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Since the <code>nonEnglishTweets</code> predicate was used as the final branching condition, the tweets that need translating will be the last <code>KStream</code> in our list (at index <span class="keep-together">position</span> 1).</p>
            </dd>
          </dl>
          <div data-type="warning" epub:type="warning">
            <h6>Warning</h6>
            <p>The method of branching streams may be changing in an upcoming Kafka Streams release.<a data-type="indexterm" data-primary="KIP (Kafka Improvement Proposal) for branching streams" id="idm46281563380984" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="branching data" data-secondary="future changes in Kafka Streams" id="idm46281563380184" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Kafka Improvement Proposal (KIP) for branching streams" id="idm46281563379176" target="_blank" rel="noopener noreferrer"></a> If you are using a Kafka Streams version greater than 2.7.0, you may want to check <a href="https://oreil.ly/h_GvU" target="_blank" rel="noopener noreferrer">“KIP-418: A method-chaining way to branch KStream”</a> for potential API changes. Some backward compatibility is expected, and the current implementation does work on the latest version of Kafka Streams at the time of writing (version 2.7.0).</p>
          </div>
          <p>Now that we have created two substreams (<code>englishStream</code> and <code>nonEnglishStream</code>), we can apply different processing logic to each. This takes care of the third step of our processor topology (see <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>), so we’re now ready to move on to the next step: translating non-English tweets into English.</p>
        </div>
      </section>
      <section data-type="sect1" class="" data-pdf-bookmark="Translating Tweets">
        <div class="sect1" id="idm46281563421432">
          <h1>Translating Tweets</h1>
          <p>At this point, we have two record streams: tweets that are written in English (<code>englishStream</code>), and tweets that are written in another language (<code>nonEnglishStream</code>).<a data-type="indexterm" data-primary="translating tweets" id="ix_transtwt" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="translating tweets" id="ix_stlprctrns" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="translating tweets" id="ix_proctuttrns" target="_blank" rel="noopener noreferrer"></a> We would like to perform sentiment analysis on each tweet, but the API we will be using to perform this sentiment analysis only supports a few languages (English being one of them). Therefore, we need to translate each tweet in the <code>nonEnglishStream</code> to English.</p>
          <p>We’re talking about this as a business problem, though. Let’s look at the requirement from a stream processing perspective. What we really need is a way of transforming one input record into exactly one new output record (whose key or value may or may not be the same type as the input record).<a data-type="indexterm" data-primary="map operator" id="idm46281563366520" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="mapValues operator" id="idm46281563365816" target="_blank" rel="noopener noreferrer"></a> Luckily for us, Kafka Streams has two <span class="keep-together">operators</span> that fit the bill:</p>
          <ul>
            <li>
              <p><code>map</code></p></li>
            <li>
              <p><code>mapValues</code></p></li>
          </ul>
          <p>A visualization of a mapping operation is shown in <a data-type="xref" href="#C3_MAP" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-5</a>.</p>
          <figure>
            <div id="C3_MAP" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0305.png" style="width: 27rem">
              <h6><span class="label">Figure 3-5. </span>Mapping operations allow us to perform a 1:1 transformation of records</h6>
            </div>
          </figure>
          <p>The <code>map</code> and <code>mapValues</code> operators are very similar (they both have a 1:1 mapping between input and output records), and both could work for this use case. The only difference is that <code>map</code> requires us to specify a new record value <em>and</em> record key, while <code>mapValues</code> requires us to just set a new value.</p>
          <p>Let’s first take a look at how we might implement this with <code>map</code>. Assume that we not only want to translate the tweet text, but also rekey each record by the Twitter username associated with a given tweet. We could implement the tweet translation step as shown in the following code block:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KStream&lt;byte[], Tweet&gt; translatedStream =
  nonEnglishStream.map(
      (key, tweet) -&gt; { <a class="co" id="co_stateless_processing_CO10-1" href="#callout_stateless_processing_CO10-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
        byte[] newKey = tweet.getUsername().getBytes();
        Tweet translatedTweet = languageClient.translate(tweet, "en");
        return KeyValue.pair(newKey, translatedTweet); <a class="co" id="co_stateless_processing_CO10-2" href="#callout_stateless_processing_CO10-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
      });</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateless_processing_CO10-1" href="#co_stateless_processing_CO10-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Our mapping function is invoked with the current record key and record value (called <code>tweet</code>).</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO10-2" href="#co_stateless_processing_CO10-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Our mapping function is required to return a new record key <em>and</em> value, represented using Kafka Streams’ <code>KeyValue</code> class. Here, the new key is set to the Twitter username, and the new value is set to the translated tweet. The actual logic for translating text is out of scope for this tutorial, but you can check the source code for implementation details.</p>
            </dd>
          </dl>
          <p>However, the requirements we outlined don’t require us to rekey any records. As we’ll see in the next chapter, rekeying records is often used when stateful operations will be performed in a latter stream processing step.<sup><a data-type="noteref" id="idm46281563341976-marker" href="ch03.html#idm46281563341976" target="_blank" rel="noopener noreferrer">13</a></sup> So while the preceding code works, we can simplify our implementation by using the <code>mapValues</code> operator instead, which is only concerned with transforming record values.</p>
          <p>An example of how to use the <code>mapValues</code> operator is shown in the following code block:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre id="C3_MAP_VALUES" data-type="programlisting">KStream&lt;byte[], Tweet&gt; translatedStream =
  nonEnglishStream.mapValues(
      (tweet) -&gt; { <a class="co" id="co_stateless_processing_CO11-1" href="#callout_stateless_processing_CO11-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
        return languageClient.translate(tweet, "en"); <a class="co" id="co_stateless_processing_CO11-2" href="#callout_stateless_processing_CO11-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
      });</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist pagebreak-before">
            <dt>
              <a class="co" id="callout_stateless_processing_CO11-1" href="#co_stateless_processing_CO11-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Our <code>mapValues</code> function is invoked with <em>only</em> the record value.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO11-2" href="#co_stateless_processing_CO11-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>We are only required to return the new record value when using the <code>mapValues</code> operator. In this case, the value is a translated <code>Tweet</code>.</p>
            </dd>
          </dl>
          <p>We will stick with the <code>mapValues</code> implementation since we don’t need to rekey any records.</p>
          <div data-type="tip">
            <h6>Tip</h6>
            <p>It is recommended to use <code>mapValues</code> instead of <code>map</code> whenever possible, as it allows Kafka Streams to potentially execute the program more efficiently.</p>
          </div>
          <p>After translating all non-English tweets, we now have two <code>KStream</code>s that contain English tweets:</p>
          <ul>
            <li>
              <p><code>englishStream</code>, the original stream of English tweets</p></li>
            <li>
              <p><code>translatedStream</code>, the newly translated tweets, which are now in English</p></li>
          </ul>
          <p>This wraps up step 4 in our processor topology (see <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>). However, the goal of our application is to perform sentiment analysis on <em>all</em> English tweets. We <em>could</em> duplicate this logic across both of these streams: <code>englishStream</code> and <code>translatedStream</code>. However, instead of introducing unnecessary code duplication, it would be better to <em>merge</em> these two streams. We’ll explore how to merge streams in the next section.<a data-type="indexterm" data-primary="translating tweets" data-startref="ix_transtwt" id="idm46281563317112" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="tranlsating tweets" data-startref="ix_stlprctrns" id="idm46281563316136" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="translating tweets" data-startref="ix_proctuttrns" id="idm46281563314632" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Merging Streams">
        <div class="sect1" id="C3_MERGE_Section">
          <h1>Merging Streams</h1>
          <p>Kafka Streams provides an easy method for combining multiple streams into a single stream.<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="merging streams" id="idm46281563311896" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="merging streams" id="idm46281563310952" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="merging streams" id="idm46281563309720" target="_blank" rel="noopener noreferrer"></a> Merging streams can be thought of as the opposite of a branching operation, and is typically used when the same processing logic can be applied to more than one stream in your application.</p>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p>The equivalent of a <code>merge</code> operator in <a data-type="indexterm" data-primary="UNION query (SQL)" id="idm46281563307288" target="_blank" rel="noopener noreferrer"></a>the SQL world is a union query. For example:</p>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">SELECT column_name(s) FROM table1
UNION
SELECT column_name(s) FROM table2;</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <p>Now that we have translated all tweets in the non-English stream, we have two separate streams that we need to perform sentiment analysis on: <code>englishStream</code> and <code>translatedStream</code>. Furthermore, all tweets that we enrich with sentiment scores will need to be written to a single output topic: <code>crypto-sentiment</code>. This is an ideal use case for merging since we can reuse the same stream/sink processors for both of these separate streams of data.</p>
          <p>The diagram in <a data-type="xref" href="#C3_MERGE" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-6</a> depicts what we are trying to accomplish.</p>
          <figure>
            <div id="C3_MERGE" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0306.png" style="width: 21rem">
              <h6><span class="label">Figure 3-6. </span>Merging operations combine multiple streams into a single stream</h6>
            </div>
          </figure>
          <p>The code for merging streams is very simple.<a data-type="indexterm" data-primary="merge operator" id="idm46281563299688" target="_blank" rel="noopener noreferrer"></a> We only need to pass the streams we want to combine to the <code>merge</code> operator, as shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KStream&lt;byte[], Tweet&gt; merged = englishStream.merge(translatedStream);</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Now that our streams are combined, we are ready to move to the next step.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Enriching Tweets">
        <div class="sect1" id="idm46281563313272">
          <h1>Enriching Tweets</h1>
          <p>We are nearing the goal of being able to enrich each tweet with a sentiment score.<a data-type="indexterm" data-primary="data enrichment" data-secondary="enriching tweets" id="ix_daenrch" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="enriching tweets and performing sentiment analysis" id="ix_stlprcenrch" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="enriching tweets" id="ix_proctutenrc" target="_blank" rel="noopener noreferrer"></a> However, as you may have noticed, our current data class, <code>Tweet</code>, represents the structure of the raw tweets in our <em>source topic</em>. We need a new data class for representing the enriched records that we will be writing to our <em>output topic</em> (<code>crypto-sentiment</code>). This time, instead of serializing our data using JSON, we will be using a data serialization format called <a href="https://oreil.ly/eFV8s" target="_blank" rel="noopener noreferrer">Avro</a>. Let’s take a deeper look at Avro and learn how to create <span class="keep-together">an Avro</span> data class for representing the enriched records in our Kafka Streams <span class="keep-together">application</span>.</p>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Avro Data Class">
            <div class="sect2" id="idm46281563287720">
              <h2>Avro Data Class</h2>
              <p>Avro is a popular format in the Kafka community, largely due to its compact byte representation (which is advantageous for high throughput applications), native<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="enriching tweets" data-tertiary="Avro data class" id="idm46281563285320" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="data classes" data-secondary="Avro" id="idm46281563284104" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="record schemas" id="idm46281563283160" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="schemas" data-secondary="Avro" id="idm46281563282488" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Avro" data-secondary="use in Kafka community" id="idm46281563281544" target="_blank" rel="noopener noreferrer"></a> support for record schemas,<sup><a data-type="noteref" id="idm46281563280472-marker" href="ch03.html#idm46281563280472" target="_blank" rel="noopener noreferrer">14</a></sup> and a schema management tool called Schema Registry, which works well with Kafka Streams and has had strong support for Avro since its inception.<sup><a data-type="noteref" id="idm46281563279464-marker" href="ch03.html#idm46281563279464" target="_blank" rel="noopener noreferrer">15</a></sup> There are other advantages as well. For example, some Kafka Connectors can use Avro schemas to automatically infer the table structure of downstream data stores, so encoding our output records in this format can help with data integration downstream.</p>
              <p>When working with Avro, you can use either <em>generic records</em> or <em>specific records</em>.<a data-type="indexterm" data-primary="records" data-secondary="generic or specific, using with Avro" id="idm46281563276072" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="generic records" id="idm46281563275096" target="_blank" rel="noopener noreferrer"></a> Generic records are suitable when the record schema isn’t known at runtime. They allow you to access field names using generic getters and setters. For example: <code>GenericRecord.get(String key)</code> and <code>GenericRecord.put(String key, Object value)</code>.</p>
              <p>Specific records, on the other hand, are <a data-type="indexterm" data-primary="specific records" id="idm46281563272904" target="_blank" rel="noopener noreferrer"></a>Java classes that are generated from Avro schema files. They provide a much nicer interface for accessing record data. For example, if you generate a specific record class named <code>EntitySentiment</code>, then you can access fields using dedicated getters/setters for each field name. For example: <code>entitySentiment.getSentimentScore()</code>.<sup><a data-type="noteref" id="idm46281563270968-marker" href="ch03.html#idm46281563270968" target="_blank" rel="noopener noreferrer">16</a></sup></p>
              <p>Since our application defines the format of its output records (and therefore, the schema is known at build time), we’ll use Avro to generate a specific record (which we’ll refer to as a data class from here on out).<a data-type="indexterm" data-primary="Avro" data-secondary="data class for enriched tweets" id="idm46281563268792" target="_blank" rel="noopener noreferrer"></a> A good place to add a schema definition for Avro data is in the <em>src/main/avro</em> directory of your Kafka Streams project. <a data-type="xref" href="#C3_AVRO_SCHEMA" target="_blank" rel="noopener noreferrer">Example&nbsp;3-7</a> shows the Avro schema definition we will be using for our sentiment-enriched output records. Save this schema in a file named <em>entity_sentiment.avsc</em> in the <em>src/main/avro/</em> directory.</p>
              <div id="C3_AVRO_SCHEMA" data-type="example">
                <h5><span class="label">Example 3-7. </span>An Avro schema for our enriched tweets</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">{
  "namespace": "com.magicalpipelines.model", <a class="co" id="co_stateless_processing_CO12-1" href="#callout_stateless_processing_CO12-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  "name": "EntitySentiment", <a class="co" id="co_stateless_processing_CO12-2" href="#callout_stateless_processing_CO12-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  "type": "record",
  "fields": [
    {
      "name": "created_at",
      "type": "long"
    },
    {
      "name": "id",
      "type:" "long"
    },
    {
      "name": "entity",
      "type": "string"
    },
    {
      "name": "text",
      "type": "string"
    },
    {
      "name": "sentiment_score",
      "type": "double"
    },
    {
      "name": "sentiment_magnitude",
      "type": "double"
    },
    {
      "name": "salience",
      "type": "double"
    }
  ]
}</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO12-1" href="#co_stateless_processing_CO12-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The desired package name for your data class.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO12-2" href="#co_stateless_processing_CO12-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The name of the Java class that will contain the Avro-based data model. This class will be used in subsequent stream processing steps.</p>
                </dd>
              </dl>
              <p>Now that we have defined our schema, we need to generate a data class from this definition. In order to do that, we need to add some dependencies to our project.<a data-type="indexterm" data-primary="Java" data-secondary="Gradle plug-in generating Java classes from Avro schema" id="idm46281563252936" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Gradle" data-secondary="plug-in generating Java classes from Avro schema definitions" id="idm46281563251864" target="_blank" rel="noopener noreferrer"></a> This can be accomplished by adding the following lines in the project’s <em>build.gradle</em> file:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">plugins {
  id 'com.commercehub.gradle.plugin.avro' version '0.9.1' <a class="co" id="co_stateless_processing_CO13-1" href="#callout_stateless_processing_CO13-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
}

dependencies {
  implementation 'org.apache.avro:avro:1.8.2'  <a class="co" id="co_stateless_processing_CO13-2" href="#callout_stateless_processing_CO13-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO13-1" href="#co_stateless_processing_CO13-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>This Gradle plug-in is used to autogenerate Java classes from Avro schema definitions, like the one we created in <a data-type="xref" href="#C3_AVRO_SCHEMA" target="_blank" rel="noopener noreferrer">Example&nbsp;3-7</a>.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO13-2" href="#co_stateless_processing_CO13-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>This dependency contains the core classes for working with Avro.</p>
                </dd>
              </dl>
              <p>Now, when we build our project (see <a data-type="xref" href="#C3_PROJECT_SETUP" target="_blank" rel="noopener noreferrer">“Project Setup”</a>), a new data class named <code>EntitySentiment</code> will be automatically generated for us.<sup><a data-type="noteref" id="idm46281563237640-marker" href="ch03.html#idm46281563237640" target="_blank" rel="noopener noreferrer">17</a></sup> This generated data class contains a new set of fields for storing tweet sentiment (<code>sentiment_score</code>, <span class="keep-together"><code>sentiment_magnitude</code>,</span> and <code>salience</code>) and corresponding getter/setter methods. With our data class in hand, we can now proceed with adding sentiment scores to our tweets. This will introduce us to a new set of DSL operators that are extremely useful for transforming data.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Sentiment Analysis">
            <div class="sect2" id="idm46281563287064">
              <h2>Sentiment Analysis</h2>
              <p>We have already seen how to transform records during the tweet translation step by using <code>map</code> and <code>mapValues</code>.<a data-type="indexterm" data-primary="sentiment analysis" data-secondary="performing for tweet stream on cryptocurrencies" id="idm46281563231608" target="_blank" rel="noopener noreferrer"></a> However, both <code>map</code> and <code>mapValues</code> produce exactly one output record for each input record they receive. In some cases, we may want to produce zero, one, or even multiple output records for a single input record.</p>
              <p>Consider the example of a tweet that mentions multiple cryptocurrencies:</p>
              <blockquote>
                <p>#bitcoin is looking super strong. #ethereum has me worried though</p>
              </blockquote>
              <p>This fictional tweet explicitly mentions two cryptocurrencies, or <em>entities</em> (Bitcoin and Ethereum) and includes two separate sentiments (positive sentiment for Bitcoin, and negative for Ethereum).<a data-type="indexterm" data-primary="entities" id="idm46281563227416" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Natural Language Processing (NLP) libraries" id="idm46281563226712" target="_blank" rel="noopener noreferrer"></a> Modern Natural Language Processing (NLP) libraries and services are often smart enough to calculate the sentiment for each entity within the text, so a single input string, like the one just shown, could lead to multiple output records. For example:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">{"entity": "bitcoin", "sentiment_score": 0.80}
{"entity": "ethereum", "sentiment_score": -0.20}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p class="adjust-bottom">Once again, let’s consider this requirement outside of the business problem itself. Here, we have a very common stream processing task: we need to convert a single input<a data-type="indexterm" data-primary="flatMap operator" id="idm46281563223960" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="flatMapValues operator" id="idm46281563223256" target="_blank" rel="noopener noreferrer"></a> record into a variable number of output records. Luckily for us, Kafka Streams includes two operators that can help with this use case:</p>
              <ul class="less-space-list">
                <li>
                  <p><code>flatMap</code></p></li>
                <li>
                  <p><code>flatMapValues</code></p></li>
              </ul>
              <p class="adjust-bottom">Both <code>flatMap</code> and <code>flatMapValues</code> are capable of producing zero, one, or multiple output records every time they are invoked. <a data-type="xref" href="#C3_FLATMAP" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-7</a> visualizes this 1:N mapping of input to output records.</p>
              <figure>
                <div id="C3_FLATMAP" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0307.png" style="width: 21rem">
                  <h6><span class="label">Figure 3-7. </span><code>flatMap</code> operations allow us to transform one input record into zero or more output records</h6>
                </div>
              </figure>
              <p>Similar to the <code>map</code> operation we saw earlier, <code>flatMap</code> requires us to set a new record key and value. On the other hand, <code>flatMapValues</code> only requires us to specify a new value.<a data-type="indexterm" data-primary="EntitySentiment class (Avro-based)" id="idm46281563212888" target="_blank" rel="noopener noreferrer"></a> Since we don’t need to process the record key in this example, we will use <span class="keep-together"><code>flatMapValues</code></span> to perform entity-level sentiment analysis for our tweets, as shown in <span class="keep-together">the following</span> code block (notice that we are using our new Avro-based data class, <span class="keep-together"><code>EntitySentiment</code>):</span></p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KStream&lt;byte[], EntitySentiment&gt; enriched =
  merged.flatMapValues(
      (tweet) -&gt; {
        List&lt;EntitySentiment&gt; results =
          languageClient.getEntitySentiment(tweet); <a class="co" id="co_stateless_processing_CO14-1" href="#callout_stateless_processing_CO14-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

        results.removeIf(
            entitySentiment -&gt; !currencies.contains(
              entitySentiment.getEntity())); <a class="co" id="co_stateless_processing_CO14-2" href="#callout_stateless_processing_CO14-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

        return results; <a class="co" id="co_stateless_processing_CO14-3" href="#callout_stateless_processing_CO14-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
      });</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO14-1" href="#co_stateless_processing_CO14-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Get a list of sentiment scores for each entity in the tweet.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO14-2" href="#co_stateless_processing_CO14-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Remove all entities that don’t match one of the cryptocurrencies we are tracking. The final list size after all of these removals is variable (a key characteristic of <code>flatMap</code> and <code>flatMapValues</code> return values), and could have zero or more items.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO14-3" href="#co_stateless_processing_CO14-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Since the <code>flatMap</code> and <code>flatMapValues</code> operators can return any number of records, we will return a list that contains every record that should be added to the output stream. Kafka Streams will handle the “flattening” of the collection we return (i.e., it will break out each element in the list into a distinct record in the stream).</p>
                </dd>
              </dl>
              <div data-type="tip">
                <h6>Tip</h6>
                <p>Similar to our recommendation about using <code>mapValues</code>, it is also recommended to use <code>flatMapValues</code> instead of <code>flatMap</code> whenever possible, as it allows Kafka Streams to potentially execute the program more efficiently.</p>
              </div>
              <p>We are now ready to tackle the final step: writing the enriched data (the sentiment scores) to a new output topic. In order to do this, we need to build an Avro Serdes that can be used for serializing the Avro-encoded <code>EntitySentiment</code> records that we created.<a data-type="indexterm" data-primary="data enrichment" data-secondary="enriching tweets" data-startref="ix_daenrch" id="idm46281563189304" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="enriching tweets and performing sentiment analysis" data-startref="ix_stlprcenrch" id="idm46281563188056" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="enriching tweets" data-startref="ix_proctutenrc" id="idm46281563186408" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" class="less_space" data-pdf-bookmark="Serializing Avro Data">
        <div class="sect1" id="idm46281563185048">
          <h1>Serializing Avro Data</h1>
          <p>As mentioned earlier, Kafka is a bytes-in, bytes-out stream processing platform.<a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="serializing Avro data" id="ix_serdesAvro" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serializing Avro data" id="ix_proctutserAv" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="serializing Avro data" id="ix_stlprcserAv" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Avro" data-secondary="serializing Avro data into byte arrays" id="ix_Avroser" target="_blank" rel="noopener noreferrer"></a> Therefore, in order to write the <code>EntitySentiment</code> records to our output topic, we need to serialize these Avro records into byte arrays.</p>
          <p>When we serialize data using Avro, we have two choices:</p>
          <ul>
            <li>
              <p>Include the Avro schema in each record.</p></li>
            <li>
              <p>Use an even more compact format, by saving the Avro schema in Confluent Schema Registry, and only including a much smaller schema ID in each record instead of the entire schema.<a data-type="indexterm" data-primary="schemas" data-secondary="Avro" data-tertiary="in serialization of Avro data" id="idm46281563174760" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Confluent Schema Registry" id="idm46281563173448" target="_blank" rel="noopener noreferrer"></a></p></li>
          </ul>
          <p>As shown in <a data-type="xref" href="#C3_AVRO" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-8</a>, the benefit of the first approach is you don’t have to set up and run a separate service alongside your Kafka Streams application. Since Confluent Schema Registry is a REST service for creating and retrieving Avro, Protobuf, and JSON schemas, it requires a separate deployment and therefore introduces a maintenance cost and an additional point of failure. However, with the first approach, you do end up with larger message sizes since the schema is included.</p>
          <p>However, if you are trying to eke every ounce of performance out of your Kafka Streams application, the smaller payload sizes that Confluent Schema Registry enables may be necessary. Furthermore, if you anticipate ongoing evolution of your record schemas and data model, the schema compatibility checks that are included in Schema Registry help ensure that future schema changes are made in safe, non-breaking ways.<sup><a data-type="noteref" id="idm46281563169960-marker" href="ch03.html#idm46281563169960" target="_blank" rel="noopener noreferrer">18</a></sup></p>
          <figure>
            <div id="C3_AVRO" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0308.png" style="width: 30rem">
              <h6><span class="label">Figure 3-8. </span>Advantages and disadvantages of including an Avro schema in each record</h6>
            </div>
          </figure>
          <p>Since Avro Serdes classes are<a data-type="indexterm" data-primary="Serdes classes" data-secondary="Avro" id="idm46281563166216" target="_blank" rel="noopener noreferrer"></a> available for both approaches, the amount of code you need to introduce to your application for serializing Avro data is minimal. The <span class="keep-together">following</span> sections show how to configure both a registryless and Schema Registry-aware Avro Serdes.</p>
          <section data-type="sect2" data-pdf-bookmark="Registryless Avro Serdes">
            <div class="sect2" id="idm46281563164056">
              <h2>Registryless Avro Serdes</h2>
              <p>While a registryless Avro Serdes can be implemented<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serializing Avro data" data-tertiary="registryless Avro Serdes" id="idm46281563162248" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="serializing Avro data" data-tertiary="registryless Avro Serdes" id="idm46281563160968" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="registryless Avro Serdes" id="idm46281563159720" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Serdes classes" data-secondary="Avro" data-tertiary="registryless" id="idm46281563159032" target="_blank" rel="noopener noreferrer"></a> very simply by yourself, I have open sourced a version under the <code>com.mitchseymour:kafka-registryless-avro-serdes</code> package.<sup><a data-type="noteref" id="idm46281563157304-marker" href="ch03.html#idm46281563157304" target="_blank" rel="noopener noreferrer">19</a></sup> You can use this package by updating your <em>build.gradle</em> file with the following code:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">dependencies {
  implementation 'com.mitchseymour:kafka-registryless-avro-serdes:1.0.0'
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Whenever you need to use this Serdes in your Kafka Streams application, just provide an Avro-generated class to the <code>AvroSerdes.get</code> method, as shown here:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">AvroSerdes.get(EntitySentiment.class)</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>The resulting Serdes can be used anywhere you would normally use one of Kafka’s built-in Serdes.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Schema Registry–Aware Avro Serdes">
            <div class="sect2" id="idm46281563152040">
              <h2>Schema Registry–Aware Avro Serdes</h2>
              <p>Confluent has also published a package for distributing its Schema Registry–aware Avro Serdes.<a data-type="indexterm" data-primary="Serdes classes" data-secondary="Schema Registry Avro Serdes" id="idm46281563150232" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="schemas" data-secondary="Schema Registry–aware Avro Serdes" id="idm46281563149192" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serializing Avro data" data-tertiary="Schema Registry–aware Avro Serdes" id="idm46281563148232" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Avro" data-secondary="serializing Avro data into byte arrays" data-tertiary="Schema Registry–aware Avro Serdes" id="idm46281563146984" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="serializing Avro data" data-tertiary="Schema Registry–aware Avro Serdes" id="idm46281563145736" target="_blank" rel="noopener noreferrer"></a> If you want to leverage the Confluent Schema Registry, update your <em>build.gradle</em> file as follows:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">repositories {
  mavenCentral()

  maven {
      url "https://packages.confluent.io/maven/" <a class="co" id="co_stateless_processing_CO15-1" href="#callout_stateless_processing_CO15-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  }
}

dependencies {
  implementation ('io.confluent:kafka-streams-avro-serde:6.0.1') { <a class="co" id="co_stateless_processing_CO15-2" href="#callout_stateless_processing_CO15-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    exclude group: 'org.apache.kafka', module: 'kafka-clients' <a class="co" id="co_stateless_processing_CO15-3" href="#callout_stateless_processing_CO15-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
  }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO15-1" href="#co_stateless_processing_CO15-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Add the Confluent Maven repository since this is where the artifact for the Schema Registry–aware Avro Serdes lives.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO15-2" href="#co_stateless_processing_CO15-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Add the required dependency for using a Schema Registry–aware Avro Serdes.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateless_processing_CO15-3" href="#co_stateless_processing_CO15-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Exclude an incompatible transitive dependency included in the <code>kafka-streams-avro-serde</code> at the time of writing.<sup><a data-type="noteref" id="idm46281563128776-marker" href="ch03.html#idm46281563128776" target="_blank" rel="noopener noreferrer">20</a></sup></p>
                </dd>
              </dl>
              <p>The Schema Registry–aware Avro Serdes requires some additional configuration, so you can improve the readability of your code by creating a factory class for instantiating Serdes instances for each of the data classes in your project. For example, the <span class="keep-together">following</span> code block shows how to create a registry-aware Avro Serdes for the <span class="keep-together"><code>TweetSentiment</code></span> class:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">public class AvroSerdes {
  public static Serde&lt;EntitySentiment&gt; EntitySentiment(
    String url, boolean isKey) {

      Map&lt;String, String&gt; serdeConfig =
        Collections.singletonMap("schema.registry.url", url); <a class="co" id="co_stateless_processing_CO16-1" href="#callout_stateless_processing_CO16-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
      Serde&lt;EntitySentiment&gt; serde = new SpecificAvroSerde&lt;&gt;();
      serde.configure(serdeConfig, isKey);
      return serde;
  }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO16-1" href="#co_stateless_processing_CO16-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The register-aware Avro Serdes requires us to configure the Schema Registry endpoint.</p>
                </dd>
              </dl>
              <p>Now, you can instantiate the Serdes wherever you need it in your Kafka Streams application with the following code:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">AvroSerdes.EntitySentiment("http://localhost:8081", false) <a class="co" id="co_stateless_processing_CO17-1" href="#callout_stateless_processing_CO17-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateless_processing_CO17-1" href="#co_stateless_processing_CO17-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Update the Schema Registry endpoint to the appropriate value.</p>
                </dd>
              </dl>
              <div data-type="tip">
                <h6>Tip</h6>
                <p>You can interact with Schema Registry directly for <a href="https://oreil.ly/GXBgi" target="_blank" rel="noopener noreferrer">registering, deleting, modifying, or listing schemas</a>. However, when using a registry-aware Avro Serdes from Kafka Streams, your schema will automatically be registered for you. Furthermore, to improve performance, the registry-aware Avro Serdes minimizes the number of schema lookups by caching schema IDs and schemas locally.</p>
              </div>
              <p>Now that we have our Avro Serdes in place, we can create our sink processor.<a data-type="indexterm" data-primary="Avro" data-secondary="serializing Avro data into byte arrays" data-startref="ix_Avroser" id="idm46281563110600" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="serializing Avro data" data-startref="ix_serdesAvro" id="idm46281563109336" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="serializing Avro data" data-startref="ix_stlprcserAv" id="idm46281563108104" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="serializing Avro data" data-startref="ix_proctutserAv" id="idm46281563106600" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Adding a Sink Processor">
        <div class="sect1" id="idm46281563184584">
          <h1>Adding a Sink Processor</h1>
          <p>The last step is to write the enriched data to our output topic: <code>crypto-sentiment</code>. <a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="adding a sink processor" id="idm46281563103592" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="sink processors" data-secondary="adding to Twitter stream processing" id="idm46281563102568" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="sink processor" id="idm46281563101608" target="_blank" rel="noopener noreferrer"></a>There are a few operators for doing this:</p>
          <ul class="less-space-list">
            <li>
              <p><code>to</code></p></li>
            <li>
              <p><code>through</code></p></li>
            <li>
              <p><code>repartition</code></p></li>
          </ul>
          <p>If you want to return a new <code>KStream</code> instance for appending additional operators/stream processing logic, then you should use the <code>repartition</code> or <code>through</code> operator (the latter was deprecated right before this book was published, but is still widely used and backward compatibility is expected).<a data-type="indexterm" data-primary="through operator" id="idm46281563094072" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="repartition operator" id="idm46281563093368" target="_blank" rel="noopener noreferrer"></a> Internally, these operators call <code>builder.stream</code> again, so using them will result in additional sub-topologies (see <a data-type="xref" href="ch02.html#C2_SUBTOPOLOGIES" target="_blank" rel="noopener noreferrer">“Sub-Topologies”</a>) being created by Kafka Streams. <a data-type="indexterm" data-primary="to operator" id="idm46281563091304" target="_blank" rel="noopener noreferrer"></a>However, if you have reached a terminal step in your stream, as we have, then you should use the <code>to</code> operator, which returns <code>void</code> since no other stream processors need to be added to the underlying <code>KStream</code>.</p>
          <p>In this example, we will use the <code>to</code> operator since we have reached the last step of our processor topology, and we will also use the Schema Registry–aware Avro Serdes since we want better support for schema evolution and also smaller message sizes. The following code shows how to add the sink processor:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">enriched.to(
  "crypto-sentiment",
  Produced.with(
    Serdes.ByteArray(),
    AvroSerdes.EntitySentiment("http://localhost:8081", false)));</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>We have now implemented each of the steps in our processor topology (see <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>). The final step is to run our code and verify that it works as expected.</p>
        </div>
      </section>
      <section data-type="sect1" class="less_space" data-pdf-bookmark="Running the Code">
        <div class="sect1" id="idm46281563086040">
          <h1>Running the Code</h1>
          <p>In order to run the application, you’ll need to stand up a Kafka cluster and Schema Registry instance.<a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="running the code" id="idm46281563084168" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="running the code" id="idm46281563083176" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="schemas" data-secondary="running Schema Registy instance" id="idm46281563081944" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="clusters" data-secondary="running a Kafka cluster" id="idm46281563080984" target="_blank" rel="noopener noreferrer"></a> The source code for this tutorial includes a Docker Compose environment to help you accomplish this.<sup><a data-type="noteref" id="idm46281563079800-marker" href="ch03.html#idm46281563079800" target="_blank" rel="noopener noreferrer">21</a></sup> Once the Kafka cluster and Schema Registry service are running, you can start your Kafka Streams application with the following command:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting" data-code-language="sh">./gradlew run --info</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Now we’re ready to test. The next section shows how to verify that our application works as expected.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Empirical Verification">
        <div class="sect1" id="idm46281563075960">
          <h1>Empirical Verification</h1>
          <p>As mentioned in <a data-type="xref" href="ch02.html#ch2" target="_blank" rel="noopener noreferrer">Chapter&nbsp;2</a>, one of the easiest ways to verify that your application is working as expected is through empirical verification.<a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="empirically verifying the application" id="ix_stlprcEV" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="testing" data-secondary="empirical verification of applications" id="ix_tstEV" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="empirical verification" id="ix_proctutEV" target="_blank" rel="noopener noreferrer"></a> This involves generating data in a local Kafka cluster and subsequently observing the data that gets written to the output topic. The easiest way to do this is to save some example <code>tweet</code> records<a data-type="indexterm" data-primary="kafka-console-producer script" id="idm46281563067640" target="_blank" rel="noopener noreferrer"></a> in a text file, and use the <code>kafka-console-producer</code> to write these records to our source topic: <code>tweets</code>.</p>
          <p>The source code includes a file called <em>test.json</em>, with two records that we will use for testing. Note: the actual example records are flattened in <em>test.json</em>, but we have shown the pretty version of each record in <a data-type="xref" href="#C3_TEST" target="_blank" rel="noopener noreferrer">Example&nbsp;3-8</a> to improve readability.</p>
          <div id="C3_TEST" data-type="example">
            <h5><span class="label">Example 3-8. </span>Two example tweets used for testing our Kafka Streams application</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">{
  "CreatedAt": 1577933872630,
  "Id": 10005,
  "Text": "Bitcoin has a lot of promise. I'm not too sure about #ethereum",
  "Lang": "en",
  "Retweet": false, <a class="co" id="co_stateless_processing_CO18-1" href="#callout_stateless_processing_CO18-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  "Source": "",
  "User": {
    "Id": "14377871",
    "Name": "MagicalPipelines",
    "Description": "Learn something magical today.",
    "ScreenName": "MagicalPipelines",
    "URL": "http://www.magicalpipelines.com",
    "FollowersCount": "248247",
    "FriendsCount": "16417"
  }
}
{
  "CreatedAt": 1577933871912,
  "Id": 10006,
  "Text": "RT Bitcoin has a lot of promise. I'm not too sure about #ethereum",
  "Lang": "en",
  "Retweet": true, <a class="co" id="co_stateless_processing_CO18-2" href="#callout_stateless_processing_CO18-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  "Source": "",
  "User": {
    "Id": "14377870",
    "Name": "Mitch",
    "Description": "",
    "ScreenName": "Mitch",
    "URL": "http://mitchseymour.com",
    "FollowersCount": "120",
    "FriendsCount": "120"
  }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateless_processing_CO18-1" href="#co_stateless_processing_CO18-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>This first tweet (ID 10005) is <em>not</em> a retweet. We expect this tweet to be enriched with sentiment scores.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateless_processing_CO18-2" href="#co_stateless_processing_CO18-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The second tweet (ID 10006) <em>is</em> a retweet, and we expect this record to be ignored.</p>
            </dd>
          </dl>
          <p>Now, let’s produce these example records to our local Kafka cluster using the following command:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">kafka-console-producer \
  --bootstrap-server kafka:9092 \
  --topic tweets &lt; test.json</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>In another tab, let’s use <code>kafka-console-consumer</code> to consume the enriched records.<a data-type="indexterm" data-primary="kafka-console-consumer script" id="idm46281563032232" target="_blank" rel="noopener noreferrer"></a> Run the following command:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic crypto-sentiment \
  --from-beginning</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>You should see some cryptic-looking output with lots of strange symbols:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">����[��|Bitcoin has a lot of promise.
I'm not too sure about #ethereumbitcoin`ff�?`ff�? -��?
����[��|Bitcoin has a lot of promise.
I'm not too sure about #ethereumethereum���ɿ���ɿ���?</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p class="pagebreak-before">This is because Avro is a binary format. If you’re using Schema Registry, as we are, then you can use a special console script developed by Confluent that improves the readability of Avro data.<a data-type="indexterm" data-primary="Avro" data-secondary="improving readability of data with kafka-avro-console-consumer script" id="idm46281563028296" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="kafka-avro-console-consumer script" id="idm46281563027352" target="_blank" rel="noopener noreferrer"></a> Simply change <code>kafka-console-consumer</code> to <code>kafka-avro-console-consumer</code> as shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">kafka-avro-console-consumer \
 --bootstrap-server kafka:9092 \
 --topic crypto-sentiment \
 --from-beginning</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Finally, you should see output similar to the following:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">{
  "created_at": 1577933872630,
  "id": 10005,
  "text": "Bitcoin has a lot of promise. I'm not too sure about #ethereum",
  "entity": "bitcoin",
  "sentiment_score": 0.699999988079071,
  "sentiment_magnitude": 0.699999988079071,
  "salience": 0.47968605160713196
}
{
  "created_at": 1577933872630,
  "id": 10005,
  "text": "Bitcoin has a lot of promise. I'm not too sure about #ethereum",
  "entity": "ethereum",
  "sentiment_score": -0.20000000298023224,
  "sentiment_magnitude": -0.20000000298023224,
  "salience": 0.030233483761548996
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Notice that tweet ID <code>10006</code> does not appear in the output. This was a retweet, and therefore was filtered in the second topology step in <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>. Also notice that tweet ID <code>10005</code> resulted in two output records. This is expected since this tweet mentioned two separate cryptocurrencies (each with a separate sentiment), and verifies that our <code>flatMapValues</code> operator worked as expected (the sixth topology step in <a data-type="xref" href="#C3_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-1</a>).<a data-type="indexterm" data-primary="flatMapValues operator" data-secondary="verification of results" id="idm46281563019288" target="_blank" rel="noopener noreferrer"></a></p>
          <p>If you would like to verify the language translation step, feel free to update the example records in <em>test.json</em> with a foreign language tweet.<sup><a data-type="noteref" id="idm46281563017528-marker" href="ch03.html#idm46281563017528" target="_blank" rel="noopener noreferrer">22</a></sup> We will leave this as an <span class="keep-together">exercise</span> for the reader.<a data-type="indexterm" data-primary="stateless processing" data-secondary="processing a Twitter stream tutorial" data-tertiary="empirically verifying the application" data-startref="ix_stlprcEV" id="idm46281563015336" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="testing" data-secondary="empirical verification of applications" data-startref="ix_tstEV" id="idm46281563013704" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processing a Twitter stream tutorial" data-secondary="empirical verification" data-startref="ix_proctutEV" id="idm46281563012472" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Summary">
        <div class="sect1" id="idm46281563075464">
          <h1>Summary</h1>
          <p>You have now learned a new set of capabilities for building stateless stream processing applications with Kafka Streams, including:</p>
          <ul>
            <li>
              <p>Filtering data with <code>filter</code> and <code>filterNot</code></p></li>
            <li>
              <p>Creating substreams using the <code>branch</code> operator</p></li>
            <li>
              <p>Combining streams with the <code>merge</code> operator</p></li>
            <li>
              <p>Performing 1:1 record transformations using <code>map</code> and <code>mapValues</code></p></li>
            <li>
              <p>Performing 1:N record transformations using <code>flatMap</code> and <code>flatMapValues</code></p></li>
            <li>
              <p>Writing records to output topics using <code>to</code>, <code>through</code>, and <code>repartition</code></p></li>
            <li>
              <p>Serializing, deserializing, and reserializing data using custom serializers, deserializers, and Serdes implementations</p></li>
          </ul>
          <p>In the next chapter, we will <a data-type="indexterm" data-primary="stateless processing" data-startref="ix_stlprc" id="idm46281562997880" target="_blank" rel="noopener noreferrer"></a>explore more complex stream processing operations by introducing stateful tasks, including joining, windowing, and aggregating streams.</p>
        </div>
      </section>
      <div data-type="footnotes">
        <p data-type="footnote" id="idm46281563801256"><sup><a href="ch03.html#idm46281563801256-marker" target="_blank" rel="noopener noreferrer">1</a></sup> Processed is a loaded word. Here, we use the word in its broadest sense, and refer to the process of <em>enriching</em>, <em>transforming</em>, <em>reacting</em> to, and optionally <em>writing</em> the processed data to an output topic.</p>
        <p data-type="footnote" id="idm46281563773128"><sup><a href="ch03.html#idm46281563773128-marker" target="_blank" rel="noopener noreferrer">2</a></sup> We’ll focus on the DSL in this chapter, but we will also cover stateless and stateful processing via the Processor API in <a data-type="xref" href="ch07.html#ch7" target="_blank" rel="noopener noreferrer">Chapter&nbsp;7</a>.</p>
        <p data-type="footnote" id="idm46281563752376"><sup><a href="ch03.html#idm46281563752376-marker" target="_blank" rel="noopener noreferrer">3</a></sup> We won’t develop the full trading algorithm in this tutorial since, ideally, our trading algorithm would include many types of signals, not just market sentiment (unless we wanted to lose a bunch of money).</p>
        <p data-type="footnote" id="idm46281563708920"><sup><a href="ch03.html#idm46281563708920-marker" target="_blank" rel="noopener noreferrer">4</a></sup> We won’t worry about the record keys in this chapter since our application doesn’t perform any key-level operations.</p>
        <p data-type="footnote" id="idm46281563678424"><sup><a href="ch03.html#idm46281563678424-marker" target="_blank" rel="noopener noreferrer">5</a></sup> Storing and transmitting data as a byte array allows Kafka to leverage something called zero-copy, which means the data doesn’t need to cross the user-kernel space boundary for serialization/deserialization purposes, since this is handled on the client side. This is a major performance benefit.</p>
        <p data-type="footnote" id="idm46281563675064"><sup><a href="ch03.html#idm46281563675064-marker" target="_blank" rel="noopener noreferrer">6</a></sup> So when we say that the Twitter connector encodes tweets as JSON, we don’t mean the tweet records in Kafka are stored as raw JSON. We simply mean the bytes representing these tweets in the underlying Kafka topic should, <em>when deserialized</em>, be formatted as JSON.</p>
        <p data-type="footnote" id="idm46281563618664"><sup><a href="ch03.html#idm46281563618664-marker" target="_blank" rel="noopener noreferrer">7</a></sup> More Serdes classes are likely to be introduced in future releases. Please refer to the official documentation for a <a href="https://oreil.ly/GIsuV" target="_blank" rel="noopener noreferrer">complete list of the available Serdes classes</a>.</p>
        <p data-type="footnote" id="idm46281563597864"><sup><a href="ch03.html#idm46281563597864-marker" target="_blank" rel="noopener noreferrer">8</a></sup> An example JSON Serdes is included in the Kafka source code, but was not exposed via the <code>Serdes</code> factory class like the official Serdes at the time of this writing.</p>
        <p data-type="footnote" id="idm46281563588808"><sup><a href="ch03.html#idm46281563588808-marker" target="_blank" rel="noopener noreferrer">9</a></sup> This would require a nasty use of regex, and even then, still wouldn’t work very well.</p>
        <p data-type="footnote" id="idm46281563476520"><sup><a href="ch03.html#idm46281563476520-marker" target="_blank" rel="noopener noreferrer">10</a></sup> Unnecessary serialization/deserialization can negatively impact performance in some applications.</p>
        <p data-type="footnote" id="idm46281563456088"><sup><a href="ch03.html#idm46281563456088-marker" target="_blank" rel="noopener noreferrer">11</a></sup> As you’ll see later, <code>flatMap</code> and <code>flatMapValues</code> can also be used for filtering. But for clarity, it’s advisable to use <code>filter</code> or <code>filterNot</code> unless the filtering step sometimes needs to produce more than one record.</p>
        <p data-type="footnote" id="idm46281563445032"><sup><a href="ch03.html#idm46281563445032-marker" target="_blank" rel="noopener noreferrer">12</a></sup> In Java, functional interfaces have a single abstract method. The <code>Predicate</code> class only contains one abstract method, called <code>test</code>.</p>
        <p data-type="footnote" id="idm46281563341976"><sup><a href="ch03.html#idm46281563341976-marker" target="_blank" rel="noopener noreferrer">13</a></sup> Rekeying helps ensure related data is colocated to the same streams task, which is important when aggregating and joining data. We will discuss this in detail in the next chapter</p>
        <p data-type="footnote" id="idm46281563280472"><sup><a href="ch03.html#idm46281563280472-marker" target="_blank" rel="noopener noreferrer">14</a></sup> Record schemas define the field names and types for a given record. They allow us to provide strong contracts about the data format between different applications and services.</p>
        <p data-type="footnote" id="idm46281563279464"><sup><a href="ch03.html#idm46281563279464-marker" target="_blank" rel="noopener noreferrer">15</a></sup> Both Protobuf and JSON schemas are supported since Confluent Platform 5.5. See <a href="https://oreil.ly/4hsQh" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://oreil.ly/4hsQh</em></a>.</p>
        <p data-type="footnote" id="idm46281563270968"><sup><a href="ch03.html#idm46281563270968-marker" target="_blank" rel="noopener noreferrer">16</a></sup> <code>entitySentiment</code> here is an instance of the Avro-generated <code>EntitySentiment</code> class, which we will be creating later in this section.</p>
        <p data-type="footnote" id="idm46281563237640"><sup><a href="ch03.html#idm46281563237640-marker" target="_blank" rel="noopener noreferrer">17</a></sup> The Gradle Avro plug-in will automatically scan the <em>src/main/avro</em> directory and pass any schema files it finds to the Avro compiler.</p>
        <p data-type="footnote" id="idm46281563169960"><sup><a href="ch03.html#idm46281563169960-marker" target="_blank" rel="noopener noreferrer">18</a></sup> For more information about schema compatibility, check out Gwen Shapira’s 2019 Confluent <a href="https://oreil.ly/kVwHQ" target="_blank" rel="noopener noreferrer">article on the subject</a>.</p>
        <p data-type="footnote" id="idm46281563157304"><sup><a href="ch03.html#idm46281563157304-marker" target="_blank" rel="noopener noreferrer">19</a></sup> The code repository for the registryless Avro Serdes can be found at <a href="https://oreil.ly/m1kk7" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://oreil.ly/m1kk7</em></a>.</p>
        <p data-type="footnote" id="idm46281563128776"><sup><a href="ch03.html#idm46281563128776-marker" target="_blank" rel="noopener noreferrer">20</a></sup> This may not be needed when using future versions of the <code>kafka-streams-avro-serde</code> library, but at the time of writing, the latest version of <code>kafka-streams-avro-serde</code> (6.0.1) conflicts with a dependency in Kafka Streams 2.7.0.</p>
        <p data-type="footnote" id="idm46281563079800"><sup><a href="ch03.html#idm46281563079800-marker" target="_blank" rel="noopener noreferrer">21</a></sup> Instructions for running the Kafka cluster and Schema Registry instance can be found at <a href="https://oreil.ly/DEoaJ" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://oreil.ly/DEoaJ</em></a>.</p>
        <p data-type="footnote" id="idm46281563017528"><sup><a href="ch03.html#idm46281563017528-marker" target="_blank" rel="noopener noreferrer">22</a></sup> Additional setup steps may be required. See the <a href="https://oreil.ly/o6Ofk" target="_blank" rel="noopener noreferrer">README for this chapter’s tutorial</a>.</p>
      </div>
    </div>
    <div class="chapter" id="ch4">
      <h1><span class="label">Chapter 4. </span>Stateful Processing</h1>
      <p>In the previous chapter, we learned how to perform stateless transformations of record streams using the <code>KStream</code> abstraction and a rich set of stateless operators that are available in Kafka Streams.<a data-type="indexterm" data-primary="stateful processing" id="ix_stflpr" target="_blank" rel="noopener noreferrer"></a> Since stateless transformations don’t require any memory of previously seen events, they are easy to reason about and use.<a data-type="indexterm" data-primary="immutability" data-secondary="events as immutable facts" id="idm46281562992776" target="_blank" rel="noopener noreferrer"></a> <a data-type="indexterm" data-primary="facts" data-secondary="events as immutable facts" id="idm46281562991736" target="_blank" rel="noopener noreferrer"></a>We treat every event as an immutable <em>fact</em> and process it independently of other events.</p>
      <p>However, Kafka Streams also gives us the ability to capture and remember information about the events we consume.<a data-type="indexterm" data-primary="events" data-secondary="stateful stream processing and" id="idm46281562989752" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state" id="idm46281562988760" target="_blank" rel="noopener noreferrer"></a> The captured information, or <em>state</em>, allows us to perform more advanced stream processing operations, including joining and aggregating data. In this chapter, we will explore stateful stream processing in detail. Some of the topics we will cover include:</p>
      <ul>
        <li>
          <p>The benefits of stateful stream processing</p></li>
        <li>
          <p>The differences between facts and behaviors</p></li>
        <li>
          <p>What kinds of stateful operators are available in Kafka Streams</p></li>
        <li>
          <p>How state is captured and queried in Kafka Streams</p></li>
        <li>
          <p>How the <code>KTable</code> abstraction can be used to represent local, partitioned state</p></li>
        <li>
          <p>How the <code>GlobalKTable</code> abstraction can be used to represent global, replicated state</p></li>
        <li>
          <p>How to perform stateful operations, including joining and aggregating data</p></li>
        <li>
          <p>How to use interactive queries to expose state</p></li>
      </ul>
      <p>As with the previous chapter, we will explore these concepts using a tutorial-based approach. This chapter’s tutorial is inspired by the video game industry, and we’ll be building a real-time leaderboard that will require us to use many of Kafka Streams’ stateful operators. Furthermore, we’ll spend a lot of time discussing joins since this is one of the most common forms of data enrichment in stateful applications. But before we get to the tutorial, let’s start by looking at some of the benefits of stateful processing.</p>
      <section data-type="sect1" data-pdf-bookmark="Benefits of Stateful Processing">
        <div class="sect1" id="idm46281562977640">
          <h1>Benefits of Stateful Processing</h1>
          <p>Stateful processing helps us understand the <em>relationships between events</em> and leverage these relationships for more advanced stream processing use cases.<a data-type="indexterm" data-primary="stateful processing" data-secondary="benefits of" id="idm46281562975432" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="behaviors" data-secondary="recognizing in event streams" id="idm46281562974488" target="_blank" rel="noopener noreferrer"></a> When we are able to understand how an event relates to other events, we can:</p>
          <ul>
            <li>
              <p>Recognize patterns and behaviors in our event streams</p></li>
            <li>
              <p>Perform aggregations</p></li>
            <li>
              <p>Enrich data in more sophisticated ways using joins</p></li>
          </ul>
          <p>Another benefit of stateful stream processing is that it gives us an additional abstraction for representing data. By replaying an event stream one event at a time, and <span class="keep-together">saving</span> the latest state of each key in an embedded key-value store, we can build a point-in-time representation of continuous and unbounded record streams. <a data-type="indexterm" data-primary="tables" id="idm46281562968728" target="_blank" rel="noopener noreferrer"></a>These point-in-time representations, or snapshots, are referred to as <em>tables</em>, and Kafka Streams includes different types of table abstractions that we’ll learn about in this chapter.</p>
          <p>Tables are not only at the heart of stateful stream processing, but when they are materialized, they can also be queried.<a data-type="indexterm" data-primary="stream-relational processing" id="idm46281562966824" target="_blank" rel="noopener noreferrer"></a> This ability to query a real-time snapshot of a fast-moving event stream is what makes Kafka Streams a <em>stream-relational</em> processing platform,<sup><a data-type="noteref" id="idm46281562965464-marker" href="ch04.html#idm46281562965464" target="_blank" rel="noopener noreferrer">1</a></sup> and enables us to not only build stream processing applications, but also low-latency, event-driven microservices as well.</p>
          <p>Finally, stateful stream processing allows us to understand our data using more sophisticated mental models.<a data-type="indexterm" data-primary="events" data-secondary="event-first thinking" id="idm46281562963288" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="behaviors" data-secondary="differences between facts and" id="idm46281562962312" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="facts" data-secondary="events representing" id="idm46281562961304" target="_blank" rel="noopener noreferrer"></a> One particularly interesting view comes from Neil Avery, who discusses the differences between facts and behaviors in his discussion of <a href="https://oreil.ly/Q-hop" target="_blank" rel="noopener noreferrer">event-first thinking</a>:</p>
          <blockquote>
            <p>An event represents a fact, something happened; it is immutable…</p>
          </blockquote>
          <p>Stateless applications, like the ones we discussed in the previous chapter, are fact-driven.<a data-type="indexterm" data-primary="immutability" data-secondary="events as immutable facts" id="idm46281562958152" target="_blank" rel="noopener noreferrer"></a> Each event is treated as an independent and atomic fact, which can be <span class="keep-together">processed</span> using immutable semantics (think of inserts in a never-ending stream), and then subsequently forgotten.</p>
          <p>However, in addition to leveraging stateless operators to filter, branch, merge, and transform facts, we can ask even more advanced questions of our data if we learn how to model <em>behaviors</em> using stateful operators.<a data-type="indexterm" data-primary="behaviors" data-secondary="modeling using stateful operators" id="idm46281562955224" target="_blank" rel="noopener noreferrer"></a> So what are behaviors? According to Neil:</p>
          <blockquote>
            <p>The accumulation of facts captures behavior.</p>
          </blockquote>
          <p>You see, events (or facts) rarely occur in isolation in the real world. Everything is interconnected, and by capturing and remembering facts, we can begin to understand their meaning.<a data-type="indexterm" data-primary="behaviors" data-secondary="capturing with accumulation of facts" id="idm46281562952648" target="_blank" rel="noopener noreferrer"></a> This is possible by understanding events in their larger historical context, or by looking at other, related events that have been captured and stored by our application.</p>
          <p>A popular example is shopping cart abandonment, which is a behavior comprised of multiple facts: a user adds one or more items to a shopping cart, and then a session is terminated either manually (e.g., the user logs off) or automatically (e.g., due to a long period of inactivity). Processing either fact independently tells us very little about where the user is in the checkout process. However, collecting, remembering, and analyzing each of the facts (which is what stateful processing enables) allows us to recognize and react to the <em>behavior</em>, and provides much greater business value than viewing the world as a series of unrelated events.</p>
          <p>Now that we understand the benefits of stateful stream processing, and the differences between facts and behaviors, let’s get a preview of the stateful operators in Kafka Streams.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Preview of Stateful Operators">
        <div class="sect1" id="idm46281562977016">
          <h1>Preview of Stateful Operators</h1>
          <p>Kafka Streams includes several stateful operators that we can use in our processor<a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="stateful operators" id="idm46281562947752" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful operators" data-secondary="preview of" id="idm46281562946504" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="preview of stateful operators" id="idm46281562945560" target="_blank" rel="noopener noreferrer"></a> topologies. <a data-type="xref" href="#C4_RELATIONSHIPS" target="_blank" rel="noopener noreferrer">Table&nbsp;4-1</a> includes an overview of several operators that we will be working with in this book.</p>
          <table id="C4_RELATIONSHIPS" class="fix-table-list">
            <caption>
              <span class="label">Table 4-1. </span>Stateful operators and their purpose
            </caption>
            <thead>
              <tr>
                <th>Use case</th>
                <th>Purpose</th>
                <th>Operators</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div>
                    <p>Joining data</p>
                  </div></td>
                <td>
                  <div>
                    <p>Enrich an event with additional <a data-type="indexterm" data-primary="join operator" id="idm46281562937432" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="leftJoin operator" id="idm46281562936728" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="outerJoin operator" id="idm46281562936056" target="_blank" rel="noopener noreferrer"></a>information or context that was captured in a separate stream or table</p>
                  </div></td>
                <td>
                  <div>
                    <ul>
                      <li>
                        <p><code>join</code> (inner join)</p></li>
                      <li>
                        <p><code>leftJoin</code></p></li>
                      <li>
                        <p><code>outerJoin</code></p></li>
                    </ul>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>Aggregating data</p>
                  </div></td>
                <td>
                  <div>
                    <p>Compute a <a data-type="indexterm" data-primary="aggregate operator" id="idm46281562929048" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="count operator" id="idm46281562928312" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="reduce operator" id="idm46281562927640" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="aggregations" data-secondary="operators for" id="idm46281562926968" target="_blank" rel="noopener noreferrer"></a>continuously updating mathematical or combinatorial transformation of related events</p>
                  </div></td>
                <td>
                  <div>
                    <ul>
                      <li>
                        <p><code>aggregate</code></p></li>
                      <li>
                        <p><code>count</code></p></li>
                      <li>
                        <p><code>reduce</code></p></li>
                    </ul>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p>Windowing data</p>
                  </div></td>
                <td>
                  <div>
                    <p>Group events that have close temporal proximity</p>
                  </div></td>
                <td>
                  <div>
                    <ul>
                      <li>
                        <p><code>windowedBy</code></p></li>
                    </ul>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p class="pagebreak-before">Furthermore, we<a data-type="indexterm" data-primary="windowedBy operator" id="idm46281562916712" target="_blank" rel="noopener noreferrer"></a> can <em>combine</em> stateful operators in Kafka Streams to understand even more complex relationships/behaviors between events. <a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="windowed joins" id="idm46281562915304" target="_blank" rel="noopener noreferrer"></a>For example, performing a <em>windowed join</em> allows us to understand how discrete event streams relate during a certain period of time. As we’ll see in the next chapter, <em>windowed aggregations</em> are another useful way of combining stateful operators.</p>
          <p>Now, compared to the stateless operators we encountered in the previous chapter, stateful operators are more complex under the hood and have additional compute and storage<sup><a data-type="noteref" id="idm46281562912456-marker" href="ch04.html#idm46281562912456" target="_blank" rel="noopener noreferrer">2</a></sup> requirements. For this reason, we will spend some time learning about the inner workings of stateful processing in Kafka Streams before we start using the stateful operators listed in <a data-type="xref" href="#C4_RELATIONSHIPS" target="_blank" rel="noopener noreferrer">Table&nbsp;4-1</a>.</p>
          <p>Perhaps the most important place to begin is by looking at how state is stored and queried in Kafka Streams.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="State Stores">
        <div class="sect1" id="C4_STATE_STORES">
          <h1>State Stores</h1>
          <p>We have already<a data-type="indexterm" data-primary="stateful processing" data-secondary="state stores" id="ix_stflprstst" target="_blank" rel="noopener noreferrer"></a> established that stateful operations require our application to maintain some memory of previously seen events. For example, an application that counts the number of error logs it sees needs to keep track of a single number for each key: a rolling count that gets updated whenever a new error log is consumed. This count represents the historical context of a record and, along with the record key, becomes part of the application’s state.<a data-type="indexterm" data-primary="storage" data-seealso="state stores" id="idm46281562906424" target="_blank" rel="noopener noreferrer"></a></p>
          <p>To support stateful operations, we need a way of storing and retrieving the remembered data, or state, required by each stateful operator in our application (e.g., <code>count</code>, <code>aggregate</code>, <code>join</code>, etc.).<a data-type="indexterm" data-primary="state stores" id="ix_ststo" target="_blank" rel="noopener noreferrer"></a> The storage abstraction that addresses these needs in Kafka Streams is called a <em>state store</em>, and since a single Kafka Streams application can leverage <em>many</em> stateful operators, a single application may contain several state stores.</p>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p>This section provides some lower-level information about how state is captured and stored in Kafka Streams. If you are eager to get started with the tutorial, feel free to skip ahead to <a data-type="xref" href="#C4_TUTORIAL" target="_blank" rel="noopener noreferrer">“Introducing Our Tutorial: Video Game Leaderboard”</a> and revisit this section later.</p>
          </div>
          <p>There are many state store implementations and configuration possibilities available in Kafka Streams, each with specific advantages, trade-offs, and use cases. Whenever you use a stateful operator in your Kafka Streams application, it’s helpful to consider which type of state store is needed by the operator, and also how to configure the state store based on your optimization criteria (e.g., are you optimizing for high throughput, operational simplicity, fast recovery times in the event of failure, etc.). In most cases, Kafka Streams will choose a sensible default if you don’t explicitly specify a state store type or override a state store’s configuration properties.</p>
          <p>Since the variation in state store types and configurations makes this quite a deep topic, we will initially focus our discussion on the common characteristics of all of the default state store implementations, and then take a look at the two broad categories of state stores: persistent and in-memory stores. More in-depth discussions of state stores will be covered in <a data-type="xref" href="ch06.html#ch6" target="_blank" rel="noopener noreferrer">Chapter&nbsp;6</a>, and as we encounter specific topics in our <span class="keep-together">tutorial</span>.</p>
          <section data-type="sect2" data-pdf-bookmark="Common Characteristics">
            <div class="sect2" id="idm46281562895896">
              <h2>Common Characteristics</h2>
              <p>The default state store implementations included in Kafka Streams share some common properties.<a data-type="indexterm" data-primary="state stores" data-secondary="common characteristics" id="idm46281562894184" target="_blank" rel="noopener noreferrer"></a> We will discuss these commonalities in this section to get a better idea of how state stores work.</p>
              <section data-type="sect3" data-pdf-bookmark="Embedded">
                <div class="sect3" id="idm46281562892840">
                  <h3>Embedded</h3>
                  <p>The default state store implementations that are included in Kafka Streams are <em>embedded</em> within your Kafka Streams application at the task level (we first discussed tasks in <a data-type="xref" href="ch02.html#C2_STREAM_THREADS" target="_blank" rel="noopener noreferrer">“Tasks and Stream Threads”</a>).<a data-type="indexterm" data-primary="embedded state stores" id="idm46281562890296" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="common characteristics" data-tertiary="embedded state stores" id="idm46281562889592" target="_blank" rel="noopener noreferrer"></a> The advantage of embedded state stores, as opposed to using an external storage engine, is that the latter would require a network call whenever state needed to be accessed, and would therefore introduce unnecessary latency and processing bottlenecks. Furthermore, since state stores are embedded at the task level, a whole class of concurrency issues for accessing shared state are eliminated.</p>
                  <p>Additionally, if state stores were remote, you’d have to worry about the availability of the remote system separately from your Kafka Streams application. Allowing Kafka Streams to manage a local state store ensures it will always be available and reduces the error surface quite a bit.<a data-type="indexterm" data-primary="local state stores" id="idm46281562887144" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="centralized remote state stores" id="idm46281562886440" target="_blank" rel="noopener noreferrer"></a> A <em>centralized</em> remote store would be even worse, since it would become a single point of failure for all of your application instances. Therefore, Kafka Streams’ strategy of colocating an application’s state alongside the application itself not only improves performance (as discussed in the previous paragraph), but also availability.</p>
                  <p>All of the default state stores leverage RocksDB under the hood.<a data-type="indexterm" data-primary="RocksDB" id="idm46281562884408" target="_blank" rel="noopener noreferrer"></a> RocksDB is a fast, embedded key-value store that was originally developed at Facebook. Since it supports arbitrary byte streams for storing key-value pairs, it works well with Kafka, which also decouples serialization from storage.<a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="decoupling of serialization from storage" id="idm46281562883320" target="_blank" rel="noopener noreferrer"></a> Furthermore, both reads and writes are extremely fast, thanks to a rich set of optimizations that were made to the forked <a data-type="indexterm" data-primary="LevelDB" id="idm46281562882056" target="_blank" rel="noopener noreferrer"></a>LevelDB code.<sup><a data-type="noteref" id="idm46281562881256-marker" href="ch04.html#idm46281562881256" target="_blank" rel="noopener noreferrer">3</a></sup></p>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Multiple access modes">
                <div class="sect3" id="idm46281562880088">
                  <h3>Multiple access modes</h3>
                  <p>State stores support multiple access modes and query patterns.<a data-type="indexterm" data-primary="state stores" data-secondary="common characteristics" data-tertiary="multiple access modes" id="idm46281562878568" target="_blank" rel="noopener noreferrer"></a> Processor topologies require read and write access to state stores.<a data-type="indexterm" data-primary="interactive queries" id="idm46281562877192" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="access modes for state stores" id="idm46281562876520" target="_blank" rel="noopener noreferrer"></a> However, when building microservices using Kafka Streams’ <em>interactive queries</em> feature, which we will discuss later in <a data-type="xref" href="#C4_IQ" target="_blank" rel="noopener noreferrer">“Interactive Queries”</a>, clients require only <em>read</em> access to the underlying state. This ensures that state is never mutable outside of the processor topology, and is accomplished through a dedicated read-only wrapper that clients can use to safely query the state of a Kafka Streams application.</p>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Fault tolerant">
                <div class="sect3" id="C4_FAULT_TOLERANT">
                  <h3>Fault tolerant</h3>
                  <p>By default, state stores are backed by changelog topics in Kafka.<sup><a data-type="noteref" id="idm46281562871912-marker" href="ch04.html#idm46281562871912" target="_blank" rel="noopener noreferrer">4</a></sup> In the event of failure, state <a data-type="indexterm" data-primary="state stores" data-secondary="common characteristics" data-tertiary="fault tolerant state stores" id="idm46281562871064" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="fault tolerance" data-secondary="fault tolerant state stores" id="idm46281562869880" target="_blank" rel="noopener noreferrer"></a>stores can be restored by replaying the individual events from the underlying changelog topic to reconstruct the state of an application. Furthermore, Kafka Streams allows users to enable standby replicas for reducing the amount of time it takes to rebuild an application’s state.<a data-type="indexterm" data-primary="redundant state stores" id="idm46281562868488" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="shadow copies" id="idm46281562867816" target="_blank" rel="noopener noreferrer"></a> These standby replicas (sometimes called <em>shadow copies</em>) make state stores <em>redundant</em>, which is an important characteristic of highly available systems. In addition, applications that allow their state to be queried can rely on standby replicas to serve query traffic when other application instances go down, which also contributes to high availability.</p>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Key-based">
                <div class="sect3" id="idm46281562865704">
                  <h3>Key-based</h3>
                  <p>Operations that leverage state stores are key-based.<a data-type="indexterm" data-primary="key-based state stores" id="idm46281562864312" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="common characteristics" data-tertiary="key-based" id="idm46281562863608" target="_blank" rel="noopener noreferrer"></a> A record’s key defines the relationship between the current event and other events. The underlying data structure will vary depending on the type of state store you decide to use,<sup><a data-type="noteref" id="idm46281562862072-marker" href="ch04.html#idm46281562862072" target="_blank" rel="noopener noreferrer">5</a></sup> but each implementation can be conceptualized as some form of key-value store, where keys may be simple, or even compounded (i.e., multidimensional) in some cases.<sup><a data-type="noteref" id="idm46281562860232-marker" href="ch04.html#idm46281562860232" target="_blank" rel="noopener noreferrer">6</a></sup></p>
                  <div data-type="note" epub:type="note">
                    <h6>Note</h6>
                    <p>To complicate things slightly, Kafka Streams explicitly refers to certain types of state stores as key-value stores, even though all of the default state stores are key-based.<a data-type="indexterm" data-primary="key-value stores" id="idm46281562858392" target="_blank" rel="noopener noreferrer"></a> When we refer to key-value stores in this chapter and elsewhere in this book, we are referring to nonwindowed state stores (windowed state stores will be discussed in the next chapter).</p>
                  </div>
                  <p>Now that we understand the commonalities between the default state stores in Kafka Streams, let’s look at two broad categories of state stores to understand the differences between certain implementations.</p>
                </div>
              </section>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Persistent Versus In-Memory Stores">
            <div class="sect2" id="C4_PVM">
              <h2>Persistent Versus In-Memory Stores</h2>
              <p>One of the most important differentiators between various state store implementations <a data-type="indexterm" data-primary="state stores" data-secondary="common characteristics" data-tertiary="persistent versus in-memory stores" id="idm46281562854328" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="persistent state stores" data-secondary="versus in-memory stores" data-secondary-sortas="in-memory" id="idm46281562853112" target="_blank" rel="noopener noreferrer"></a>is whether or not the state store is <em>persistent</em>, or if it simply stores remembered information <em>in-memory</em> (RAM).<a data-type="indexterm" data-primary="in-memory versus persistent stores" id="idm46281562850968" target="_blank" rel="noopener noreferrer"></a> Persistent state stores flush state to disk asynchronously (to a configurable <em>state directory</em>), which <a data-type="indexterm" data-primary="state directory" id="idm46281562849704" target="_blank" rel="noopener noreferrer"></a>has two primary benefits:</p>
              <ul>
                <li>
                  <p>State can exceed the size of available memory.</p></li>
                <li>
                  <p>In the event of failure, persistent stores can be restored quicker than in-memory stores.</p></li>
              </ul>
              <p>To clarify the first point, a persistent state store may keep some of its state in-memory, while writing to disk when the size of the state gets too big (this is called <em>spilling to disk</em>) or when the write buffer exceeds a configured value.<a data-type="indexterm" data-primary="spilling to disk" id="idm46281562845560" target="_blank" rel="noopener noreferrer"></a> Second, since the application state is persisted to disk, Kafka Streams does not need to replay the entire topic to rebuild the state store whenever the state is lost (e.g., due to system failure, instance migration, etc.). It just needs to replay whatever data is missing between the time the application went down and when it came back up.</p>
              <div data-type="tip">
                <h6>Tip</h6>
                <p>The state store directory used for persistent stores can be set using the <code>StreamsConfig.STATE_DIR_CONFIG</code> property.<a data-type="indexterm" data-primary="StreamsConfig.STATE_DIR_CONFIG property" id="idm46281562842984" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state directory" data-secondary="setting using StreamsConfig.STATE_DIR_CONFIG property" id="idm46281562842216" target="_blank" rel="noopener noreferrer"></a> The default location is <em>/tmp/kafka-streams</em>, but it is highly recommended that you override this to a directory outside of <em>/tmp</em>.</p>
              </div>
              <p>The downside is that persistent state stores are operationally more complex and can be slower than a pure in-memory store, which always pulls data from RAM. The additional operational complexity comes from the secondary storage requirement (i.e., disk-based storage) and, if you need to tune the state store, understanding RocksDB and its configurations (the latter may not be an issue for most applications).</p>
              <p>Regarding the performance gains of an in-memory state store, these may not be drastic enough to warrant their use (since failure recovery takes longer). Adding more partitions to parallelize work is always an option if you need to squeeze more performance out of your application. Therefore, my recommendation is to start with persistent stores and only switch to in-memory stores if you have measured a noticeable performance improvement and, when quick recovery is concerned (e.g., in the event your application state is lost), you are using standby replicas to reduce recovery time.</p>
              <p>Now that we have some understanding of what state stores are, and how they enable stateful/behavior-driven processing, let’s take a look at this chapter’s tutorial and see some of these ideas in action.<a data-type="indexterm" data-primary="state stores" data-startref="ix_ststo" id="idm46281562837576" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="state stores" data-startref="ix_stflprstst" id="idm46281562836600" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Introducing Our Tutorial: Video Game Leaderboard">
        <div class="sect1" id="C4_TUTORIAL">
          <h1>Introducing Our Tutorial: Video Game Leaderboard</h1>
          <p>In this chapter, we will learn about <em>stateful processing</em> by implementing a video game leaderboard with Kafka Streams.<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="introduction to" id="ix_stflprVGL" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="introduction to" id="ix_vidldr" target="_blank" rel="noopener noreferrer"></a> The video game industry is a prime example of where stream processing excels, since both gamers and game systems require low-latency processing and immediate feedback. This is one reason why companies like Activision (the company behind games like <em>Call of Duty</em> and remasters of <em>Crash Bandicoot</em> and <em>Spyro</em>) use Kafka Streams for processing video game telemetry.<sup><a data-type="noteref" id="idm46281562828440-marker" href="ch04.html#idm46281562828440" target="_blank" rel="noopener noreferrer">7</a></sup></p>
          <p>The leaderboard we will be building will require us to model data in ways that we haven’t explored yet. Specifically, we’ll be looking at how to use Kafka Streams’ table abstractions to model data as a sequence of updates. Then, we’ll dive into the topics of joining and aggregating data, which are useful whenever you need to understand or compute the relationship between multiple events. This knowledge will help you solve more complicated business problems with Kafka Streams.</p>
          <p>Once we’ve created our real-time leaderboard using a new set of stateful operators, we will demonstrate how to query Kafka Streams for the latest leaderboard information using <em>interactive queries</em>.<a data-type="indexterm" data-primary="interactive queries" id="idm46281562825080" target="_blank" rel="noopener noreferrer"></a> Our discussion of this feature will teach you how to build event-driven microservices with Kafka Streams, which in turn will broaden the type of clients we can share data with from our stream processing applications.<sup><a data-type="noteref" id="idm46281562823976-marker" href="ch04.html#idm46281562823976" target="_blank" rel="noopener noreferrer">8</a></sup></p>
          <p>Without further ado, let’s take a look at the architecture of our video game leaderboard. <a data-type="xref" href="#C4_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-1</a> shows the topology design we’ll be implementing in this chapter. Additional information about each step is included after the diagram.</p>
          <figure>
            <div id="C4_TOPOLOGY" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0401.png" style="width: 25rem">
              <h6><span class="label">Figure 4-1. </span>The topology that we will be implementing in our stateful video game leaderboard application</h6>
            </div>
          </figure>
          <dl class="calloutlist">
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12">
            </dt>
            <dd>
              <p>Our Kafka cluster contains three topics:</p>
              <ul>
                <li>
                  <p>The <code>score-events</code> topic contains game scores. The records are unkeyed and are therefore distributed in a round-robin fashion across the topic’s <span class="keep-together">partitions</span>.</p></li>
                <li>
                  <p>The <code>players</code> topic contains player profiles. Each record is keyed by a player ID.</p></li>
                <li>
                  <p>The <code>products</code> topic contains product information for various video games. Each record is keyed by a product ID.</p></li>
              </ul>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12">
            </dt>
            <dd>
              <p>We need to enrich our score events data with detailed player information. We can accomplish this using a join.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12">
            </dt>
            <dd>
              <p>Once we’ve enriched the <code>score-events</code> data with player data, we need to add detailed product information to the resulting stream. This can also be accomplished using a join.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12">
            </dt>
            <dd>
              <p>Since grouping data is a prerequisite for aggregating, we need to group the enriched stream.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12">
            </dt>
            <dd>
              <p>We need to calculate the top three high scores for each game. We can use Kafka Streams’ aggregation operators for this purpose.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12">
            </dt>
            <dd>
              <p>Finally, we need to expose the high scores for each game externally. We will accomplish this by building a RESTful microservice using the <em>interactive queries</em> feature in Kafka Streams.</p>
            </dd>
          </dl>
          <p>With our topology design in hand, we can now move on to the project setup.<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="introduction to" data-startref="ix_stflprVGL" id="idm46281562802920" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="introduction to" data-startref="ix_vidldr" id="idm46281562801336" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Project Setup">
        <div class="sect1" id="idm46281562799976">
          <h1>Project Setup</h1>
          <p>The code for this chapter<a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="project setup" id="idm46281562798440" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="project setup" id="idm46281562797448" target="_blank" rel="noopener noreferrer"></a> is located at <a href="https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git</em></a>.</p>
          <p>If you would like to reference the code as we work our way through each topology step, clone the repo and change to the directory containing this chapter’s tutorial. The following command will do the trick:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ git clone git@github.com:mitch-seymour/mastering-kafka-streams-and-ksqldb.git
$ cd mastering-kafka-streams-and-ksqldb/chapter-04/video-game-leaderboard</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>You can build the project anytime by running the following command:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ ./gradlew build --info</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Now that our project is set up, let’s start implementing our video game leaderboard.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Data Models">
        <div class="sect1" id="idm46281562791880">
          <h1>Data Models</h1>
          <p>As always, we’ll start by defining our data models.<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="data models" id="ix_stflprVGLDM" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="data models" data-secondary="for video game leaderboard" id="ix_damod" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="data models" id="ix_vidldrDM" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="plain old Java objects (POJOs)" data-secondary="for video game leaderboard data model" data-secondary-sortas="video" id="idm46281562786248" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Java" data-secondary="POJOs for video game leaderboard data model" id="idm46281562785000" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Gson library" id="idm46281562783960" target="_blank" rel="noopener noreferrer"></a> Since the source topics contain JSON data, we will define our data models using POJO data classes, which we will serialize and deserialize using our JSON serialization<a data-type="indexterm" data-primary="JSON" data-secondary="serialization/deserialization using Gson library" id="idm46281562782968" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="JSON, using Gson library" id="idm46281562781992" target="_blank" rel="noopener noreferrer"></a> library of choice (throughout this book, we use Gson, but you could easily use Jackson or another library).<sup><a data-type="noteref" id="idm46281562780760-marker" href="ch04.html#idm46281562780760" target="_blank" rel="noopener noreferrer">9</a></sup></p>
          <p>I like to group my data models in a dedicated package in my project, for example, <code>com.magicalpipelines.model</code>. A filesystem view of where the data classes for this tutorial are located is shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">src/
└── main
    └── java
        └── com
            └── magicalpipelines
                └── model
                    ├── ScoreEvent.java <a class="co" id="co_stateful_processing_CO1-1" href="#callout_stateful_processing_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    ├── Player.java <a class="co" id="co_stateful_processing_CO1-2" href="#callout_stateful_processing_CO1-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    └── Product.java <a class="co" id="co_stateful_processing_CO1-3" href="#callout_stateful_processing_CO1-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateful_processing_CO1-1" href="#co_stateful_processing_CO1-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>ScoreEvent.java</code> data class will be used to represent records in the <code>score-events</code> topic.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO1-2" href="#co_stateful_processing_CO1-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>Player.java</code> data class will be used to represent records in the <code>players</code> topic.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO1-3" href="#co_stateful_processing_CO1-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>Product.java</code> data class will be used to represent records in the <code>products</code> topic.</p>
            </dd>
          </dl>
          <p>Now that we know which data classes we need to implement, let’s create a data class for each topic. <a data-type="xref" href="#C4_DATA_CLASSES" target="_blank" rel="noopener noreferrer">Table&nbsp;4-2</a> shows the resulting POJOs that we have implemented for this tutorial.</p>
          <table id="C4_DATA_CLASSES">
            <caption>
              <span class="label">Table 4-2. </span>Example records and data classes for each topic
            </caption>
            <thead>
              <tr>
                <th>Kafka topic</th>
                <th>Example record</th>
                <th>Data class</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div>
                    <p><code>score-events</code></p>
                  </div></td>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">{
  "score": 422,
  "product_id": 6,
  "player_id": 1
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">public class ScoreEvent {
  private Long playerId;
  private Long productId;
  private Double score;
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p><code>players</code></p>
                  </div></td>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">{
  "id": 2,
  "name": "Mitch"
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">public class Player {
  private Long id;
  private String name;
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p><code>products</code></p>
                  </div></td>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">{
  "id": 1,
  "name": "Super Smash Bros"
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">public class Product {
  private Long id;
  private String name;
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p>We have already discussed serialization and deserialization in detail in <a data-type="xref" href="ch03.html#C3_SERDES_SECTION" target="_blank" rel="noopener noreferrer">“Serialization/Deserialization”</a>. In that chapter’s tutorial, we implemented our own custom serializer, deserializer, and Serdes. We won’t spend more time on that here, but you can check out the code for this tutorial to see how we’ve implemented the Serdes for each of the data classes shown in <a data-type="xref" href="#C4_DATA_CLASSES" target="_blank" rel="noopener noreferrer">Table&nbsp;4-2</a>.<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="data models" data-startref="ix_stflprVGLDM" id="idm46281562741704" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="data models" data-startref="ix_vidldrDM" id="idm46281562740168" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="data models" data-secondary="for video game leaderboard" data-startref="ix_damod" id="idm46281562738936" target="_blank" rel="noopener noreferrer"></a></p>
          </div>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Adding the Source Processors">
        <div class="sect1" id="idm46281562737448">
          <h1>Adding the Source Processors</h1>
          <p>Once we’ve defined our data classes, we can set up our source processors. In this topology, we need three <a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="adding source processors" id="ix_stflprVGLsrcprc" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="adding source processors" id="ix_vidldrsrcprc" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="source processors" data-secondary="adding to video game leaderboard project" id="ix_srcprc" target="_blank" rel="noopener noreferrer"></a>of them since we will be reading from three source topics. The first thing we need to do when adding a source processor is determine which Kafka Streams abstraction we should use for representing the data in the underlying topic.</p>
          <p>Up until now, we have only worked with the <code>KStream</code> abstraction, which is used to represent stateless record streams. However, our topology requires us to use both the <code>products</code> and <code>players</code> topics as lookups, so this is a good indication that a table-like abstraction may be appropriate for these topics.<sup><a data-type="noteref" id="idm46281562728968-marker" href="ch04.html#idm46281562728968" target="_blank" rel="noopener noreferrer">10</a></sup> Before we start mapping our topics to Kafka Streams abstractions, let’s first review the difference between <code>KStream</code>, <code>KTable</code>, and <code>GlobalKTable</code> representations of a Kafka topic. As we review each abstraction, we’ll fill in the appropriate abstraction for each topic in <a data-type="xref" href="#C4_ABSTRACTION_TABLE" target="_blank" rel="noopener noreferrer">Table&nbsp;4-3</a>.</p>
          <table id="C4_ABSTRACTION_TABLE">
            <caption>
              <span class="label">Table 4-3. </span>The topic-abstraction mapping that we’ll update in the following sections
            </caption>
            <thead>
              <tr>
                <th>Kafka topic</th>
                <th>Abstraction</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <p><code>score-events</code></p></td>
                <td>
                  <p>???</p></td>
              </tr>
              <tr>
                <td>
                  <p><code>players</code></p></td>
                <td>
                  <p>???</p></td>
              </tr>
              <tr>
                <td>
                  <p><code>products</code></p></td>
                <td>
                  <p>???</p></td>
              </tr>
            </tbody>
          </table>
          <section data-type="sect2" data-pdf-bookmark="KStream">
            <div class="sect2" id="idm46281562716728">
              <h2>KStream</h2>
              <p>When deciding which abstraction to use, it helps to determine the nature of the topic, the topic configuration, and the keyspace of records in the underlying source topic.<a data-type="indexterm" data-primary="source processors" data-secondary="adding to video game leaderboard project" data-tertiary="KStream" id="idm46281562714744" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="KStream class" data-secondary="source processor for video game leaderboard project" id="idm46281562713464" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="adding source processors" data-tertiary="KStream" id="idm46281562712488" target="_blank" rel="noopener noreferrer"></a> Even though stateful Kafka Streams applications use one or more table abstractions, it is also very common to use stateless <code>KStreams</code> alongside a <code>KTable</code> or <code>GlobalKTable</code> when the mutable table semantics aren’t needed for one or more data sources.</p>
              <p>In this tutorial, our <code>score-events</code> topic contains raw score events, which are unkeyed (and therefore, distributed in a round-robin fashion) in an uncompacted topic. Since tables are key-based, this is a strong indication that we should be using a <code>KStream</code> for our unkeyed <code>score-events</code> topic. We could change our keying strategy upstream (i.e., in whatever application is producing to our source topic), but that’s not always possible. Furthermore, our application cares about the <em>highest score</em> for each player, not the <em>latest score</em>, so table semantics (i.e., retain only the most recent record for a given key) don’t translate well for how we intend to use the <code>score-events</code> topic, even if it were keyed.</p>
              <p>Therefore, we will use a <code>KStream</code> for the <code>score-events</code> topic, so let’s update the table in <a data-type="xref" href="#C4_ABSTRACTION_TABLE" target="_blank" rel="noopener noreferrer">Table&nbsp;4-3</a> to reflect this decision as follows:</p>
              <table id="C4_ABSTRACTION_TABLE_2">
                <thead>
                  <tr>
                    <th>Kafka topic</th>
                    <th>Abstraction</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <p><code>score-events</code></p></td>
                    <td>
                      <p><code>KStream</code></p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>players</code></p></td>
                    <td>
                      <p>???</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>products</code></p></td>
                    <td>
                      <p>???</p></td>
                  </tr>
                </tbody>
              </table>
              <p>The remaining two topics, <code>players</code> and <code>products</code>, are keyed, and we only care about the latest record for each unique key in the topic. Hence, the <code>KStream</code> abstraction isn’t ideal for these topics. So, let’s move on and see if the <code>KTable</code> abstraction is appropriate for either of these topics.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="KTable">
            <div class="sect2" id="C4_KTABLE">
              <h2>KTable</h2>
              <p>The <code>players</code> topic is a compacted topic that contains player profiles, and each record is keyed by the player ID.<a data-type="indexterm" data-primary="KTable class" data-secondary="source processor for video game leaderboard project" id="idm46281562690536" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="source processors" data-secondary="adding to video game leaderboard project" data-tertiary="KTable" id="idm46281562689528" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="adding source processors" data-tertiary="KTable" id="idm46281562688280" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="tables" data-secondary="KTable source processor for video game leaderboard" id="idm46281562687080" target="_blank" rel="noopener noreferrer"></a> Since we only care about the latest state of a player, it makes sense to represent this topic using a table-based abstraction (either <code>KTable</code> or <span class="keep-together"><code>GlobalKTable</code>).</span></p>
              <p>One important thing to look at when deciding between using a <code>KTable</code> or <code>GlobalKTable</code> is the keyspace. If the keyspace is very large (i.e., has high cardinality/lots of unique keys), or is expected to grow into a very large keyspace, then it makes more sense to use a <code>KTable</code> so that you can distribute fragments of the entire state across all of your running application instances. By partitioning the state in this way, we can lower the local storage overhead for each individual Kafka Streams instance.</p>
              <p>Perhaps a more important consideration when choosing between a <code>KTable</code> or <code>GlobalKTable</code> is whether or not you need time synchronized processing.<a data-type="indexterm" data-primary="tables" data-secondary="choosing between KTable and GlobalKTable" id="idm46281562680904" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="GlobalKTable class" data-secondary="choosing between KTable and GlobalKTable" id="idm46281562679896" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="synchronized processing using KTables" id="idm46281562678920" target="_blank" rel="noopener noreferrer"></a> A <code>KTable</code> is time synchronized, so when Kafka Streams is reading from multiple sources (e.g., in the case of a join), it will look at the timestamp to determine which record to process next.<a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="KTables versus GlobalKTables" id="idm46281562677544" target="_blank" rel="noopener noreferrer"></a> This means a join will reflect what the combined record would have been at a certain time, and this makes the join behavior more predictable. On the other hand, <code>GlobalKTables</code> are <em>not</em> time synchronized, and are “completely populated before any processing is done.”<sup><a data-type="noteref" id="idm46281562675192-marker" href="ch04.html#idm46281562675192" target="_blank" rel="noopener noreferrer">11</a></sup> Therefore, joins are always made against the most up-to-date version of a <code>GlobalKTable</code>, which changes the semantics of the program.</p>
              <p>In this case, we’re not going to focus too much on the second consideration since we’ve reserved the next chapter for our discussion of time and the role it plays in Kafka Streams. With regards to the keyspace, <code>players</code> contains a record for each unique player in our system. While this may be small depending on where we are in the life cycle of our company or product, it is a number we expect to grow significantly over time, so we’ll use a <code>KTable</code> abstraction for this topic.<a data-type="indexterm" data-primary="state" data-secondary="partitioning across application instances with time synchronization" id="idm46281562672008" target="_blank" rel="noopener noreferrer"></a></p>
              <p><a data-type="xref" href="#C4_KTABLE_IMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-2</a> shows how using a <code>KTable</code> leads to the underlying state being distributed across multiple running application instances.</p>
              <figure>
                <div id="C4_KTABLE_IMG" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0402.png" style="width: 14rem">
                  <h6><span class="label">Figure 4-2. </span>A <code>KTable</code> should be used when you want to partition state across multiple application instances and need time synchronized processing</h6>
                </div>
              </figure>
              <p>Our updated abstraction table now looks like this:</p>
              <table id="C4_ABSTRACTION_TABLE_3">
                <thead>
                  <tr>
                    <th>Kafka topic</th>
                    <th>Abstraction</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <p><code>score-events</code></p></td>
                    <td>
                      <p><code>KStream</code></p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>players</code></p></td>
                    <td>
                      <p><code>KTable</code></p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>products</code></p></td>
                    <td>
                      <p>???</p></td>
                  </tr>
                </tbody>
              </table>
              <p>We have one topic left: the <code>products</code> topic. This topic is relatively small, so we should be able to replicate the state in full across all of our application instances. Let’s take a look at the abstraction that allows us to do this: <code>GlobalKTable</code>.</p>
            </div>
          </section>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="GlobalKTable">
            <div class="sect2" id="idm46281562692696">
              <h2>GlobalKTable</h2>
              <p>The <code>products</code> topic is similar to the <code>players</code> topic in terms of configuration (it’s compacted) and its bounded keyspace (we maintain the latest record for each unique product ID, and there are only a fixed number of products that we track).<a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="adding source processors" data-tertiary="GlobalKTable" id="idm46281562653320" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="GlobalKTable class" data-secondary="source processor for video game leaderboard project" id="idm46281562652040" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="source processors" data-secondary="adding to video game leaderboard project" data-tertiary="GlobalKTable" id="idm46281562651064" target="_blank" rel="noopener noreferrer"></a> However, the <code>products</code> topic has much lower cardinality (i.e., fewer unique keys) than the <span class="keep-together"><code>players</code></span> topic, and even if our leaderboard tracked high scores for several hundred games, this still translates to a state space small enough to fit entirely in-memory.</p>
              <p>In addition to being smaller, the data in the <code>products</code> topic is also relatively static. Video games take a long time to build, so we don’t expect a lot of updates to our <span class="keep-together"><code>products</code></span> topic.</p>
              <p>These two characteristics (small and static data) are what <code>GlobalKTables</code> were designed for. Therefore, we will use a <code>GlobalKTable</code> for our <code>products</code> topic. As a result, each of our Kafka Streams instances will store a full copy of the product information, which, as we’ll see later, makes performing joins much easier.</p>
              <p><a data-type="xref" href="#C4_GLOBALKTABLE_IMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-3</a> shows how each Kafka Streams instance maintains a full copy of the <span class="keep-together"><code>products</code></span> table.</p>
              <figure>
                <div id="C4_GLOBALKTABLE_IMG" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0403.png" style="width: 14rem">
                  <h6><span class="label">Figure 4-3. </span>A <code>GlobalKTable</code> should be used when your keyspace is small, you want to avoid the co-partitioning requirements of a join (we will discuss co-partitioning in <span class="keep-together"><a data-type="xref" href="#CP_COPARTITIONING" target="_blank" rel="noopener noreferrer">“Co-Partitioning”</a></span>), and when time synchronization is not needed</h6>
                </div>
              </figure>
              <p class="pagebreak-before">We can now make the final update to our topic-abstraction mapping:</p>
              <table id="C4_ABSTRACTION_TABLE_4">
                <thead>
                  <tr>
                    <th>Kafka topic</th>
                    <th>Abstraction</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <p><code>score-events</code></p></td>
                    <td>
                      <p><code>KStream</code></p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>players</code></p></td>
                    <td>
                      <p><code>KTable</code></p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>products</code></p></td>
                    <td>
                      <p><code>GlobalKTable</code></p></td>
                  </tr>
                </tbody>
              </table>
              <p>Now that we’ve decided which abstraction to use for each of our source topics, we can register the streams and tables.<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="adding source processors" data-startref="ix_stflprVGLsrcprc" id="idm46281562629096" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="adding source processors" data-startref="ix_vidldrsrcprc" id="idm46281562627592" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="source processors" data-secondary="adding to video game leaderboard project" data-startref="ix_srcprc" id="idm46281562626344" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Registering Streams and Tables">
        <div class="sect1" id="idm46281562656120">
          <h1>Registering Streams and Tables</h1>
          <p>Registering streams and tables is simple. The following code block shows how to use the high-level <a data-type="indexterm" data-primary="registration" data-secondary="streams and tables" id="idm46281562623192" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="tables" data-secondary="registering" id="idm46281562622216" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="registering" id="idm46281562621272" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="registering streams and tables" id="idm46281562620328" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="registering streams and tables" id="idm46281562619400" target="_blank" rel="noopener noreferrer"></a>DSL to create a <code>KStream</code>, <code>KTable</code>, and <code>GlobalKTable</code> using the appropriate builder methods:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">StreamsBuilder builder = new StreamsBuilder();

KStream&lt;byte[], ScoreEvent&gt; scoreEvents =
    builder.stream(
        "score-events",
        Consumed.with(Serdes.ByteArray(), JsonSerdes.ScoreEvent())); <a class="co" id="co_stateful_processing_CO2-1" href="#callout_stateful_processing_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

KTable&lt;String, Player&gt; players =
    builder.table(
        "players",
        Consumed.with(Serdes.String(), JsonSerdes.Player())); <a class="co" id="co_stateful_processing_CO2-2" href="#callout_stateful_processing_CO2-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

GlobalKTable&lt;String, Product&gt; products =
    builder.globalTable(
        "products",
        Consumed.with(Serdes.String(), JsonSerdes.Product())); <a class="co" id="co_stateful_processing_CO2-3" href="#callout_stateful_processing_CO2-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateful_processing_CO2-1" href="#co_stateful_processing_CO2-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Use a <code>KStream</code> to represent data in the <code>score-events</code> topic, which is currently unkeyed.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO2-2" href="#co_stateful_processing_CO2-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Create a partitioned (or sharded) table for the <code>players</code> topic, using the <code>KTable</code> abstraction.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO2-3" href="#co_stateful_processing_CO2-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Create a <code>GlobalKTable</code> for the <code>products</code> topic, which will be replicated in full to each application instance.</p>
            </dd>
          </dl>
          <p>By registering the source topics, we have now implemented the first step of our leaderboard topology (see <a data-type="xref" href="#C4_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-1</a>). Let’s move on to the next step: joining streams and tables.</p>
        </div>
      </section>
      <section data-type="sect1" class="less_space" data-pdf-bookmark="Joins">
        <div class="sect1" id="idm46281562598008">
          <h1>Joins</h1>
          <p>The most common<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="joins" id="ix_stflprVGLjoin" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="joins" id="ix_vidldrjoin" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" id="ix_joinKS" target="_blank" rel="noopener noreferrer"></a> method for combining datasets in the relational world is through joins.<sup><a data-type="noteref" id="idm46281562592008-marker" href="ch04.html#idm46281562592008" target="_blank" rel="noopener noreferrer">12</a></sup> In relational systems, data is often highly dimensional and scattered across many different tables. It is common to see these same patterns in Kafka as well, either because events are sourced from multiple locations, developers are comfortable or used to relational data models, or because certain Kafka integrations (e.g., the JDBC Kafka Connector, Debezium, Maxwell, etc.) bring both the raw data and the data models of the source systems with them.</p>
          <p>Regardless of how data becomes scattered in Kafka, being able to combine data in separate streams and tables based on <em>relationships</em> opens the door for more advanced data enrichment opportunities in Kafka Streams.<a data-type="indexterm" data-primary="relationships between data in separate tables and streams" id="idm46281562588472" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="tables" data-secondary="relationships between, combining data based on" id="idm46281562587800" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="relationships between, combining data based on" id="idm46281562586824" target="_blank" rel="noopener noreferrer"></a> Furthermore, the join method for combining datasets is very different from simply merging streams, as we saw in <a data-type="xref" href="ch03.html#C3_MERGE" target="_blank" rel="noopener noreferrer">Figure&nbsp;3-6</a>. When we use the <code>merge</code> operator in Kafka Streams, records on both sides of the merge are unconditionally combined into a single stream. Simple merge operations are therefore stateless since they do not need additional context about the events being merged.</p>
          <p>Joins, however, can<a data-type="indexterm" data-primary="merging streams" data-secondary="joins as conditional merges" id="idm46281562583832" target="_blank" rel="noopener noreferrer"></a> be thought of as a special kind of <em>conditional merge</em> that cares about the relationship between events, and where the records are not copied verbatim into the output stream but rather combined. Furthermore, these relationships must be captured, stored, and referenced at merge time to facilitate joining, which makes joining a stateful operation. <a data-type="xref" href="#C4_JOIN" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-4</a> shows a simplified depiction of how one type of join works (there are several types of joins, as we’ll see in <a data-type="xref" href="#C4_JOIN_TYPES" target="_blank" rel="noopener noreferrer">Table&nbsp;4-5</a>).</p>
          <p>As with relational systems, Kafka Streams includes support for multiple kinds of joins.<a data-type="indexterm" data-primary="messages, joining" id="idm46281562579816" target="_blank" rel="noopener noreferrer"></a> So, before we learn how to join our <code>score-events</code> stream with our <code>players</code> table, let’s first familiarize ourselves with the various join operators that are available to us so that we can select the best option for our particular use case.</p>
          <figure>
            <div id="C4_JOIN" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0404.png" style="width: 21rem">
              <h6><span class="label">Figure 4-4. </span>Joining messages</h6>
            </div>
          </figure>
          <section data-type="sect2" data-pdf-bookmark="Join Operators">
            <div class="sect2" id="idm46281562575832">
              <h2>Join Operators</h2>
              <p>Kafka Streams includes three different join operators for joining streams and tables.<a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="join operators" id="idm46281562574232" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="joins" data-tertiary="operators for" id="idm46281562572984" target="_blank" rel="noopener noreferrer"></a> Each operator is detailed in <a data-type="xref" href="#C4_JOIN_OPERATORS" target="_blank" rel="noopener noreferrer">Table&nbsp;4-4</a>.</p>
              <table id="C4_JOIN_OPERATORS">
                <caption>
                  <span class="label">Table 4-4. </span>Join operators
                </caption>
                <thead>
                  <tr>
                    <th>Operator</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><span class="keep-together"><code>join</code></span></td>
                    <td>
                      <p>Inner join. The join is triggered when the input records on both sides of the join share the same key.<a data-type="indexterm" data-primary="join operator" id="idm46281562565800" target="_blank" rel="noopener noreferrer"></a></p></td>
                  </tr>
                  <tr>
                    <td><span class="keep-together"><code>leftJoin</code></span></td>
                    <td>
                      <p>Left join.<a data-type="indexterm" data-primary="leftJoin operator" id="idm46281562563320" target="_blank" rel="noopener noreferrer"></a> The join semantics are different depending on the type of join:</p>
                      <ul>
                        <li>
                          <p>For stream-table joins: a join is triggered when a record on the <em>left side</em> of the join is received. If there is no record with the same key on the right side of the join, then the right value is set to null.</p></li>
                        <li>
                          <p>For stream-stream and table-table joins: same semantics as a stream-stream left join, except an input on the right side of the join can also trigger a lookup. If the right side triggers the join and there is no matching key on the left side, then the join will not produce a result.</p></li>
                      </ul></td>
                  </tr>
                  <tr>
                    <td><span class="keep-together"><code>outerJoin</code></span></td>
                    <td>
                      <p>Outer join.<a data-type="indexterm" data-primary="outerJoin operator" id="idm46281562558440" target="_blank" rel="noopener noreferrer"></a> The join is triggered when a record on <em>either side</em> of the join is received. If there is no matching record with the same key on the opposite side of the join, then the corresponding value is set to null.</p></td>
                  </tr>
                </tbody>
              </table>
              <div data-type="note" epub:type="note">
                <h6>Note</h6>
                <p>When discussing the differences between join operators, we refer to different <em>sides of the join</em>. <a data-type="indexterm" data-primary="sides of a join" id="idm46281562555240" target="_blank" rel="noopener noreferrer"></a>Just remember, the <em>right side</em> of the join is always passed as a parameter to the relevant join operator. For example:</p>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">KStream&lt;String, ScoreEvent&gt; scoreEvents = ...;
KTable&lt;String, Player&gt; players = ...;

scoreEvents.join(players, ...); <a class="co" id="co_stateful_processing_CO3-1" href="#callout_stateful_processing_CO3-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
                <dl class="calloutlist">
                  <dt>
                    <a class="co" id="callout_stateful_processing_CO3-1" href="#co_stateful_processing_CO3-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                  </dt>
                  <dd>
                    <p><code>scoreEvents</code> is the <em>left side</em> of the join. <code>players</code> is the <em>right side</em> of the join.</p>
                  </dd>
                </dl>
              </div>
              <p>Now, let’s look at the type of joins we can create with these operators.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Join Types">
            <div class="sect2" id="idm46281562545800">
              <h2>Join Types</h2>
              <p>Kafka Streams supports many different types of joins, as shown in <a data-type="xref" href="#C4_JOIN_TYPES" target="_blank" rel="noopener noreferrer">Table&nbsp;4-5</a>. The <span class="keep-together">co-partitioning</span> column<a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="join types" id="idm46281562542872" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="joins" data-tertiary="types of" id="idm46281562541592" target="_blank" rel="noopener noreferrer"></a> refers to something we will discuss in <a data-type="xref" href="#CP_COPARTITIONING" target="_blank" rel="noopener noreferrer">“Co-Partitioning”</a>. For now, it’s sufficient to understand that co-partitioning is simply an extra set of requirements that are needed to actually perform the join.</p>
              <table id="C4_JOIN_TYPES" class="fix-table-list">
                <caption>
                  <span class="label">Table 4-5. </span>Join types
                </caption>
                <thead>
                  <tr>
                    <th>Type</th>
                    <th>Windowed</th>
                    <th>Operators</th>
                    <th>Co-partitioning required</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <div>
                        <p><code>KStream-KStream</code></p>
                      </div></td>
                    <td>
                      <div>
                        <p>Yes<sup><a data-type="noteref" id="idm46281562532472-marker" href="ch04.html#idm46281562532472" target="_blank" rel="noopener noreferrer">a</a></sup></p>
                      </div></td>
                    <td>
                      <div>
                        <ul>
                          <li>
                            <p><code>join</code></p></li>
                          <li>
                            <p><code>leftJoin</code></p></li>
                          <li>
                            <p><code>outerJoin</code></p></li>
                        </ul>
                      </div></td>
                    <td>
                      <div>
                        <p>Yes</p>
                      </div></td>
                  </tr>
                  <tr>
                    <td>
                      <div>
                        <p><code>KTable-KTable</code></p>
                      </div></td>
                    <td>
                      <div>
                        <p>No</p>
                      </div></td>
                    <td>
                      <div>
                        <ul>
                          <li>
                            <p><code>join</code></p></li>
                          <li>
                            <p><code>leftJoin</code></p></li>
                          <li>
                            <p><code>outerJoin</code></p></li>
                        </ul>
                      </div></td>
                    <td>
                      <div>
                        <p>Yes</p>
                      </div></td>
                  </tr>
                  <tr>
                    <td>
                      <div>
                        <p><code>KStream-KTable</code></p>
                      </div></td>
                    <td>
                      <div>
                        <p>No</p>
                      </div></td>
                    <td>
                      <div>
                        <ul>
                          <li>
                            <p><code>join</code></p></li>
                          <li>
                            <p><code>leftJoin</code></p></li>
                        </ul>
                      </div></td>
                    <td>
                      <div>
                        <p>Yes</p>
                      </div></td>
                  </tr>
                  <tr>
                    <td>
                      <div>
                        <p><code>KStream-GlobalKTable</code></p>
                      </div></td>
                    <td>
                      <div>
                        <p>No</p>
                      </div></td>
                    <td>
                      <div>
                        <ul>
                          <li>
                            <p><code>join</code></p></li>
                          <li>
                            <p><code>leftJoin</code></p></li>
                        </ul>
                      </div></td>
                    <td>
                      <div>
                        <p>No</p>
                      </div></td>
                  </tr>
                </tbody>
                <tbody>
                  <tr class="footnotes">
                    <td colspan="4">
                      <p data-type="footnote" id="idm46281562532472"><sup><a href="ch04.html#idm46281562532472-marker" target="_blank" rel="noopener noreferrer">a</a></sup> One key thing to note is that <code>KStream-KStream</code> joins are windowed. We will discuss this in detail in the next chapter.</p></td>
                  </tr>
                </tbody>
              </table>
              <p>The two types of joins we need to perform in this chapter are:</p>
              <ul>
                <li>
                  <p><code>KStream-KTable</code> to<a data-type="indexterm" data-primary="KStream class" data-secondary="joins in video game leaderboard project" id="idm46281562505080" target="_blank" rel="noopener noreferrer"></a> join the <code>score-events</code> <code>KStream</code> and the <code>players</code> <code>KTable</code></p></li>
                <li>
                  <p><code>KStream-GlobalKTable</code> to join the output of the previous join with the <code>products</code> <code>GlobalKTable</code></p></li>
              </ul>
              <p>We will use<a data-type="indexterm" data-primary="GlobalKTable class" data-secondary="joins in video game leaderboard project" id="idm46281562499800" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="KTable class" data-secondary="joins in video game leaderboard project" id="idm46281562498824" target="_blank" rel="noopener noreferrer"></a> an inner join, using the <code>join</code> operator for each of the joins since we only want the join to be triggered when there’s a match on both sides. However, before we do that, we can see that the first join we’ll be creating (<code>KStream-KTable</code>) shows that co-partitioning is required. Let’s take a look at what that means before we write any more code.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Co-Partitioning">
            <div class="sect2" id="CP_COPARTITIONING">
              <h2>Co-Partitioning</h2>
              <blockquote>
                <p>If a tree falls in a forest and no one is around to hear it, does it make a sound?</p>
                <p data-type="attribution">Aphorism</p>
              </blockquote>
              <p>This famous thought experiment raises a question about what role an observer has in the occurrence of an event (in this case, sound being made in a forest). <a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="co-partitioning" id="idm46281562492712" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="events" data-secondary="effects of observer on processing" id="idm46281562491464" target="_blank" rel="noopener noreferrer"></a>Similarly, in Kafka Streams, we must always be aware of the effect an observer has on the <em>processing of an event</em>.<a data-type="indexterm" data-primary="observability, joins and" id="idm46281562489880" target="_blank" rel="noopener noreferrer"></a></p>
              <p>In <a data-type="xref" href="ch02.html#C2_STREAM_THREADS" target="_blank" rel="noopener noreferrer">“Tasks and Stream Threads”</a>, we learned that each partition is assigned to a single Kafka Streams task, and these tasks will act as the observers in our analogy since they are responsible for actually consuming and processing events.<a data-type="indexterm" data-primary="tasks" data-secondary="and stream threads" data-secondary-sortas="stream" id="idm46281562487656" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="threads (stream)" id="idm46281562486440" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stream threads" id="idm46281562485768" target="_blank" rel="noopener noreferrer"></a> Because there’s no guarantee that events on different partitions will be handled by the same Kafka Streams task, we have a potential observability problem.</p>
              <p><a data-type="xref" href="#C4_OBSERVE" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-5</a> shows the basic observability issue. If related events are processed by different tasks, then we cannot accurately determine the relationship between events because we have two separate observers. Since the whole purpose of joining data is to combine related events, an observability problem will make our joins fail when they should otherwise succeed.</p>
              <p>In order to understand the relationship between events (through joins) or compute aggregations on a sequence of events, we need to ensure that related events are routed to the same partition, and so are handled by the same task.<a data-type="indexterm" data-primary="co-partitioning" id="idm46281562482712" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="partitions" data-secondary="co-partitioning" id="idm46281562482008" target="_blank" rel="noopener noreferrer"></a></p>
              <p>To ensure related events are routed to the same partition, we must ensure the following <em>co-partitioning requirements</em> are met:</p>
              <ul>
                <li>
                  <p>Records on both sides must be keyed by the same field, and must be partitioned on that key using the same partitioning strategy.</p></li>
                <li>
                  <p>The input topics on both sides of the join must contain the same number of partitions. (This is the one requirement that is checked at startup. If this requirement is not met, then a <code>TopologyBuilderException</code> will be thrown.)</p></li>
              </ul>
              <figure>
                <div id="C4_OBSERVE" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0405.png" style="width: 17rem">
                  <h6><span class="label">Figure 4-5. </span>If we want to join related records but these records aren’t always processed by the same task, then we have an observability problem</h6>
                </div>
              </figure>
              <p>In this tutorial, we meet all of the requirements to perform a <code>KTable-KTable</code> join except the first one. Recall that records in the <code>score-events</code> topic are unkeyed, but we’ll be joining with the <code>players</code> <code>KTable</code>, which is keyed by player ID. Therefore, we need to rekey the data in <code>score-events</code> by player ID as well, <em>prior to performing the join</em>.<a data-type="indexterm" data-primary="selectKey operator" id="idm46281562471800" target="_blank" rel="noopener noreferrer"></a> This can be accomplished using the <code>selectKey</code> operator, as shown in <a data-type="xref" href="#C4_SELECT_KEY" target="_blank" rel="noopener noreferrer">Example&nbsp;4-1</a>.</p>
              <div id="C4_SELECT_KEY" data-type="example">
                <h5><span class="label">Example 4-1. </span>The <code>selectKey</code> operator allows us to rekey records; this is often needed to meet the co-partitioning requirements for performing certain types of joins</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">KStream&lt;String, ScoreEvent&gt; scoreEvents =
  builder
    .stream(
      "score-events",
      Consumed.with(Serdes.ByteArray(), JsonSerdes.ScoreEvent()))
    .selectKey((k, v) -&gt; v.getPlayerId().toString()); <a class="co" id="co_stateful_processing_CO4-1" href="#callout_stateful_processing_CO4-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO4-1" href="#co_stateful_processing_CO4-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p><code>selectKey</code> is used to rekey records.<a data-type="indexterm" data-primary="rekeying records" id="idm46281562461992" target="_blank" rel="noopener noreferrer"></a> In this case, it helps us meet the first co-partitioning requirement of ensuring records on both sides of the join (the <code>score-events</code> data and <code>players</code> data) are keyed by the same field.</p>
                </dd>
              </dl>
              <p>A visualization of how records are rekeyed is shown in <a data-type="xref" href="#C4_REKEY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-6</a>.</p>
              <figure>
                <div id="C4_REKEY" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0406.png" style="width: 16rem">
                  <h6><span class="label">Figure 4-6. </span>Rekeying messages ensures related records appear on the same partition</h6>
                </div>
              </figure>
              <div data-type="note" epub:type="note">
                <h6>Note</h6>
                <p>When we add a key-changing operator to our topology, the underlying data will be <em>marked for repartitioning</em>. <a data-type="indexterm" data-primary="partitions" data-secondary="data marked for repartitioning" id="idm46281562455288" target="_blank" rel="noopener noreferrer"></a>This means that as soon as we add a downstream operator that reads the new key, Kafka Streams will:</p>
                <ul>
                  <li>
                    <p>Send the rekeyed data to an internal repartition topic</p></li>
                  <li>
                    <p>Reread the newly rekeyed data back into Kafka Streams</p></li>
                </ul>
                <p>This process ensures related records (i.e., records that share the same key) will be processed by the same task in subsequent topology steps. However, the network trip required for rerouting data to a special repartition topic means that rekey operations can be expensive.</p>
              </div>
              <p>What about our <code>KStream-GlobalKTable</code> for joining the <code>products</code> topic? As shown in <a data-type="xref" href="#C4_JOIN_TYPES" target="_blank" rel="noopener noreferrer">Table&nbsp;4-5</a>, co-partitioning is not required for <code>GlobalKTable</code> joins since the state is fully replicated across each instance of our Kafka Streams app. Therefore, we will never encounter this kind of observability problem with a <code>GlobalKTable</code> join.</p>
              <p>We’re almost ready to join our streams and tables. But first, let’s take a look at how records are actually combined during a join operation.</p>
            </div>
          </section>
          <section data-type="sect2" class="less_space" data-pdf-bookmark="Value Joiners">
            <div class="sect2" id="idm46281562495848">
              <h2>Value Joiners</h2>
              <p>When performing a join using traditional SQL, we simply need to use the join operator in conjunction with a <code>SELECT</code> clause <a data-type="indexterm" data-primary="value joiners" id="idm46281562443944" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="value joiners" id="idm46281562443320" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="joins" data-tertiary="value joiners" id="idm46281562442104" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="SELECT statements" data-secondary="joins and" id="idm46281562440920" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="projection" data-secondary="specifying for combined join record" id="idm46281562439976" target="_blank" rel="noopener noreferrer"></a>to specify the shape (or <em>projection</em>) of the combined join record. For example:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">SELECT a.customer_name, b.purchase_amount <a class="co" id="co_stateful_processing_CO5-1" href="#callout_stateful_processing_CO5-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
FROM customers a
LEFT JOIN purchases b
ON a.customer_id = b.customer_id</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO5-1" href="#co_stateful_processing_CO5-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The projection of the combined join record includes two columns.</p>
                </dd>
              </dl>
              <p>However, in Kafka Streams, we need to use a <code>ValueJoiner</code> to specify how different records should be combined. A <code>ValueJoiner</code> simply takes each record that is involved in the join, and produces a new, combined record. Looking at the first join, in which we need to join the <code>score-events</code> <code>KStream</code> with the <code>players</code> <code>KTable</code>, the behavior of the value joiner could be expressed using the following pseudocode:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">(scoreEvent, player) -&gt; combine(scoreEvent, player);</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>But we can do much better than that. It’s more typical to have a dedicated data class that does one of the following:</p>
              <ul>
                <li>
                  <p>Wraps each of the values involved in the join</p></li>
                <li>
                  <p>Extracts the relevant fields from each side of the join, and saves the extracted values in class properties</p></li>
              </ul>
              <p>We will explore both approaches next. First, let’s start with a simple wrapper class for the <code>score-events -&gt; players</code> join. <a data-type="xref" href="#C4_SCORE_WITH_PLAYER" target="_blank" rel="noopener noreferrer">Example&nbsp;4-2</a> shows a simple implementation of a data class that wraps the record on each side of the join.</p>
              <div id="C4_SCORE_WITH_PLAYER" data-type="example">
                <h5><span class="label">Example 4-2. </span>The data class that we’ll use to construct the joined <code>score-events -&gt; players</code> record</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">public class ScoreWithPlayer {
  private ScoreEvent scoreEvent;
  private Player player;

  public ScoreWithPlayer(ScoreEvent scoreEvent, Player player) {<a class="co" id="co_stateful_processing_CO6-1" href="#callout_stateful_processing_CO6-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    this.scoreEvent = scoreEvent; <a class="co" id="co_stateful_processing_CO6-2" href="#callout_stateful_processing_CO6-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    this.player = player;
  }

  // accessors omitted from brevity
}</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO6-1" href="#co_stateful_processing_CO6-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The constructor contains a parameter for each side of the join. The left side of the join contains <code>ScoreEvent</code>, and the right side contains a <code>Player</code>.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO6-2" href="#co_stateful_processing_CO6-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We simply save a reference to each record involved in the join inside of our wrapper class.</p>
                </dd>
              </dl>
              <p>We can use our new wrapper class as the return type in our <code>ValueJoiner</code>. <a data-type="xref" href="#C4_VJ1" target="_blank" rel="noopener noreferrer">Example&nbsp;4-3</a> shows an example implementation of a <code>ValueJoiner</code> that combines <span class="keep-together">a <code>ScoreEvent</code></span> (from the <code>score-events</code> <code>KStream</code>) and a <code>Player</code> (from the <code>players</code> <span class="keep-together"><code>KTable</code>)</span> into a <code>ScoreWithPlayer</code> instance.</p>
              <div id="C4_VJ1" data-type="example">
                <h5><span class="label">Example 4-3. </span>The <code>ValueJoiner</code> for combining <code>score-events</code> and <code>players</code></h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">ValueJoiner&lt;ScoreEvent, Player, ScoreWithPlayer&gt; scorePlayerJoiner =
        (score, player) -&gt; new ScoreWithPlayer(score, player); <a class="co" id="co_stateful_processing_CO7-1" href="#callout_stateful_processing_CO7-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO7-1" href="#co_stateful_processing_CO7-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We could also simply use a static method reference here, such as <span class="keep-together"><code>ScoreWithPlayer::new</code>.</span></p>
                </dd>
              </dl>
              <p>Let’s move on to the second join. This join needs to combine a <code>ScoreWithPlayer</code> (from the output of the first join) with a <code>Product</code> (from the <code>products</code> <code>GlobalKTable</code>). We could reuse the wrapper pattern again, but we could also simply extract the properties we need from each side of the join, and discard the rest.</p>
              <p>The following code block shows an implementation of a data class that follows the second pattern. We simply extract the values we want and save them to the appropriate class properties:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">public class Enriched {
  private Long playerId;
  private Long productId;
  private String playerName;
  private String gameName;
  private Double score;

  public Enriched(ScoreWithPlayer scoreEventWithPlayer, Product product) {
    this.playerId = scoreEventWithPlayer.getPlayer().getId();
    this.productId = product.getId();
    this.playerName = scoreEventWithPlayer.getPlayer().getName();
    this.gameName = product.getName();
    this.score = scoreEventWithPlayer.getScoreEvent().getScore();
  }

  // accessors omitted from brevity
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>With this new data class in place, we can build our <code>ValueJoiner</code> for the <code>KStream-GlobalKTable</code> join using the code shown in <a data-type="xref" href="#C4_VJ3" target="_blank" rel="noopener noreferrer">Example&nbsp;4-4</a>.</p>
              <div id="C4_VJ3" data-type="example">
                <h5><span class="label">Example 4-4. </span>A ValueJoiner, expressed as a lambda, that we will use for the join</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">ValueJoiner&lt;ScoreWithPlayer, Product, Enriched&gt; productJoiner =
    (scoreWithPlayer, product) -&gt; new Enriched(scoreWithPlayer, product);</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <p>Now that we’ve told Kafka Streams <em>how</em> to combine our join records, we can actually perform the joins.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="KStream to KTable Join (players Join)">
            <div class="sect2" id="idm46281562446136">
              <h2>KStream to KTable Join (players Join)</h2>
              <p>It’s time to join our <code>score-events</code> <code>KStream</code> with our <code>players</code> <code>KTable</code>. Since<a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="joins" data-tertiary="KStream-KTable join" id="idm46281562382888" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="KStream-KTable joins" id="idm46281562381544" target="_blank" rel="noopener noreferrer"></a> we only want to trigger the join when the <code>ScoreEvent</code> record can be matched to a <code>Player</code> record (using the record key), we’ll perform an <a data-type="indexterm" data-primary="join operator" data-secondary="using for KStream-KTable join" id="idm46281562379272" target="_blank" rel="noopener noreferrer"></a>inner join using the <code>join</code> operator, as shown here:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">Joined&lt;String, ScoreEvent, Player&gt; playerJoinParams =
  Joined.with( <a class="co" id="co_stateful_processing_CO8-1" href="#callout_stateful_processing_CO8-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    Serdes.String(),
    JsonSerdes.ScoreEvent(),
    JsonSerdes.Player()
  );

KStream&lt;String, ScoreWithPlayer&gt; withPlayers =
  scoreEvents.join( <a class="co" id="co_stateful_processing_CO8-2" href="#callout_stateful_processing_CO8-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    players,
    scorePlayerJoiner, <a class="co" id="co_stateful_processing_CO8-3" href="#callout_stateful_processing_CO8-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    playerJoinParams
  );</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO8-1" href="#co_stateful_processing_CO8-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The join parameters define how the keys and values for the join records should be serialized.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO8-2" href="#co_stateful_processing_CO8-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>join</code> operator performs an inner join.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO8-3" href="#co_stateful_processing_CO8-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>This is the <code>ValueJoiner</code> we created in <a data-type="xref" href="#C4_VJ1" target="_blank" rel="noopener noreferrer">Example&nbsp;4-3</a>. A new <code>ScoreWithPlayer</code> value is created from the two join records. Check out the <code>ScoreWithPlayer</code> data class in <a data-type="xref" href="#C4_SCORE_WITH_PLAYER" target="_blank" rel="noopener noreferrer">Example&nbsp;4-2</a> to see how the left and right side of the join values are passed to the constructor.</p>
                </dd>
              </dl>
              <p>It’s that simple. Furthermore, if you were to run the code at this point and then list all of the topics that are available in your Kafka cluster, you would see that Kafka Streams <a data-type="indexterm" data-primary="topics" data-secondary="internal topics created from KStream-KTable join" id="idm46281562358952" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="repartition topic" id="idm46281562358024" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="changelog topics" data-secondary="internal, created for KStream-KTable join" id="idm46281562357352" target="_blank" rel="noopener noreferrer"></a>created two new internal topics for us.</p>
              <p class="pagebreak-before">These topics are:</p>
              <ul>
                <li>
                  <p>A repartition topic to handle the rekey operation that we performed in <a data-type="xref" href="#C4_SELECT_KEY" target="_blank" rel="noopener noreferrer">Example&nbsp;4-1</a>.</p></li>
                <li>
                  <p>A changelog topic for backing the state store, which is used by the join operator. This is part of the fault-tolerance behavior that we initially discussed in <a data-type="xref" href="#C4_FAULT_TOLERANT" target="_blank" rel="noopener noreferrer">“Fault tolerant”</a>.</p></li>
              </ul>
              <p>You can verify with the <code>kafka-topics</code> console script:<sup><a data-type="noteref" id="idm46281562350664-marker" href="ch04.html#idm46281562350664" target="_blank" rel="noopener noreferrer">13</a></sup></p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">$ kafka-topics --bootstrap-server kafka:9092 --list

players
products
score-events
dev-KSTREAM-KEY-SELECT-0000000001-repartition <a class="co" id="co_stateful_processing_CO9-1" href="#callout_stateful_processing_CO9-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
dev-players-STATE-STORE-0000000002-changelog <a class="co" id="co_stateful_processing_CO9-2" href="#callout_stateful_processing_CO9-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO9-1" href="#co_stateful_processing_CO9-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>An internal repartition topic that was created by Kafka Streams. It is prefixed with the application ID of our Kafka Streams application (<code>dev</code>).</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO9-2" href="#co_stateful_processing_CO9-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>An internal changelog topic that was created by Kafka Streams. As with the repartition topic, this changelog topic is also prefixed with the application ID of our Kafka Streams application.</p>
                </dd>
              </dl>
              <p>OK, we’re ready to move on to the second join.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="KStream to GlobalKTable Join (products Join)">
            <div class="sect2" id="idm46281562385992">
              <h2>KStream to GlobalKTable Join (products Join)</h2>
              <p>As we discussed in the co-partitioning requirements, records on either side of a <code>GlobalKTable</code> join do not need to share the same key.<a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="KStream-GlobalKTable join" id="idm46281562336392" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="joins" data-tertiary="KStream-GlobalKTable join" id="idm46281562335176" target="_blank" rel="noopener noreferrer"></a> Since the local task has a full copy of the table, we can actually perform a join using some attribute of the record value itself on the stream side of the join,<sup><a data-type="noteref" id="idm46281562333624-marker" href="ch04.html#idm46281562333624" target="_blank" rel="noopener noreferrer">14</a></sup> which is more efficient than having to rekey records through a repartition topic just to ensure related records are handled by the same task.<a data-type="indexterm" data-primary="GlobalKTable class" data-secondary="KStream-GlobalKTable join" id="idm46281562332360" target="_blank" rel="noopener noreferrer"></a></p>
              <p>To perform a <code>KStream-GlobalKTable</code> join, we need to create something called a <code>KeyValueMapper</code>, whose purpose is to specify how to map a <code>KStream</code> record to a <code>GlobalKTable</code> record.<a data-type="indexterm" data-primary="KeyValueMapper" id="idm46281562328936" target="_blank" rel="noopener noreferrer"></a>For this tutorial, we can simply extract the product ID from the <code>ScoreWithPlayer</code> value to map these records to a <code>Product</code>, as shown here:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KeyValueMapper&lt;String, ScoreWithPlayer, String&gt; keyMapper =
  (leftKey, scoreWithPlayer) -&gt; {
    return String.valueOf(scoreWithPlayer.getScoreEvent().getProductId());
  };</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>With our <code>KeyValueMapper</code> in place, and also the <code>ValueJoiner</code> that we created in <a data-type="xref" href="#C4_VJ3" target="_blank" rel="noopener noreferrer">Example&nbsp;4-4</a>, we can now perform the join:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KStream&lt;String, Enriched&gt; withProducts =
  withPlayers.join(products, keyMapper, productJoiner);</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>This completes the second and third steps of our leaderboard topology (see <a data-type="xref" href="#C4_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-1</a>). The next thing we need to tackle is grouping the enriched records so that we can perform an aggregation.<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="joins" data-startref="ix_stflprVGLjoin" id="idm46281562322136" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="joins" data-startref="ix_vidldrjoin" id="idm46281562320584" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-startref="ix_joinKS" id="idm46281562319352" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Grouping Records">
        <div class="sect1" id="idm46281562597416">
          <h1>Grouping Records</h1>
          <p>Before you perform any stream or table aggregations in Kafka Streams, you must first group the <code>KStream</code> or <code>KTable</code> that you plan to aggregate.<a data-type="indexterm" data-primary="grouping records" id="idm46281562315624" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="grouping records" id="idm46281562314920" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="grouping records" id="idm46281562313688" target="_blank" rel="noopener noreferrer"></a> The purpose of grouping is the same as rekeying records prior to joining: to ensure the related records are processed by the same observer, or Kafka Streams task.</p>
          <p>There are some slight differences between grouping streams and tables, so we will take a look at each.</p>
          <section data-type="sect2" data-pdf-bookmark="Grouping Streams">
            <div class="sect2" id="idm46281562311800">
              <h2>Grouping Streams</h2>
              <p>There are two operators<a data-type="indexterm" data-primary="streams" data-secondary="grouping" id="idm46281562310072" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="grouping records" data-secondary="grouping streams" id="idm46281562309064" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="grouping records" data-tertiary="grouping streams" id="idm46281562308120" target="_blank" rel="noopener noreferrer"></a> that can be used for grouping a <code>KStream</code>:</p>
              <ul>
                <li>
                  <p><code>groupBy</code></p></li>
                <li>
                  <p><code>groupByKey</code></p></li>
              </ul>
              <p>Using <code>groupBy</code> is similar to the process of rekeying a stream using <code>selectKey</code>, since this operator<a data-type="indexterm" data-primary="groupBy operator" id="idm46281562302568" target="_blank" rel="noopener noreferrer"></a> is a key-changing operator and causes Kafka Streams to mark the stream for repartitioning. If a downstream operator is added that reads the new key, Kafka Streams will automatically create a repartition topic and route the data back to Kafka to complete the rekeying process.</p>
              <p><a data-type="xref" href="#C4_GROUP_BY" target="_blank" rel="noopener noreferrer">Example&nbsp;4-5</a> shows how to use the <code>groupBy</code> operator to group a <code>KStream</code>.</p>
              <div id="C4_GROUP_BY" data-type="example">
                <h5><span class="label">Example 4-5. </span>Use the <code>groupBy</code> operator to rekey and group a <code>KStream</code> at the same time</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">KGroupedStream&lt;String, Enriched&gt; grouped =
  withProducts.groupBy(
      (key, value) -&gt; value.getProductId().toString(), <a class="co" id="co_stateful_processing_CO10-1" href="#callout_stateful_processing_CO10-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
      Grouped.with(Serdes.String(), JsonSerdes.Enriched())); <a class="co" id="co_stateful_processing_CO10-2" href="#callout_stateful_processing_CO10-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO10-1" href="#co_stateful_processing_CO10-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We can use a lambda to select the new key, since the <code>groupBy</code> operator expects a <code>KeyValueMapper</code>, which <a data-type="indexterm" data-primary="KeyValueMapper" data-secondary="expected with groupBy operator" id="idm46281562288968" target="_blank" rel="noopener noreferrer"></a>happens to be a functional interface.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO10-2" href="#co_stateful_processing_CO10-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p><code>Grouped</code> allows us to pass in some additional options for grouping, including the key and value Serdes to use when serializing the records.<a data-type="indexterm" data-primary="Grouped object" id="idm46281562284888" target="_blank" rel="noopener noreferrer"></a></p>
                </dd>
              </dl>
              <p>However, if your <a data-type="indexterm" data-primary="groupByKey operator" id="idm46281562283448" target="_blank" rel="noopener noreferrer"></a>records don’t need to be rekeyed, then it is preferable to use the <code>groupByKey</code> operator instead. <code>groupByKey</code> will <em>not</em> mark the stream for repartitioning, and will therefore be more performant since it avoids the additional network calls associated with sending data back to Kafka for repartitioning. The <code>groupByKey</code> implementation is shown here:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KGroupedStream&lt;String, Enriched&gt; grouped =
    withProducts.groupByKey(
      Grouped.with(Serdes.String(),
      JsonSerdes.Enriched()));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Since we want to calculate the high scores for each <em>product ID</em>, and since our enriched stream is currently keyed by <em>player ID</em>, we will use the <code>groupBy</code> variation shown in <a data-type="xref" href="#C4_GROUP_BY" target="_blank" rel="noopener noreferrer">Example&nbsp;4-5</a> in the leaderboard topology.</p>
              <p>Regardless of which operator you use for grouping a stream, Kafka Streams will return a new type that we haven’t discussed before: <code>KGroupedStream</code>. <code>KGroupedStream</code> is just an intermediate representation of a stream that allows us to perform aggregations.<a data-type="indexterm" data-primary="KGroupedStream" id="idm46281562275816" target="_blank" rel="noopener noreferrer"></a> We will look at aggregations shortly, but first, let’s take a look at how to group <code>KTables</code>.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Grouping Tables">
            <div class="sect2" id="idm46281562311176">
              <h2>Grouping Tables</h2>
              <p>Unlike grouping streams, there is only one operator available for grouping tables: <code>groupBy</code>.<a data-type="indexterm" data-primary="grouping records" data-secondary="grouping tables" id="idm46281562272504" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="grouping records" data-tertiary="grouping tables" id="idm46281562271496" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="tables" data-secondary="grouping" id="idm46281562270312" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="groupBy operator" data-secondary="using with tables" id="idm46281562269368" target="_blank" rel="noopener noreferrer"></a> Furthermore, instead of returning a <code>KGroupedStream</code>, invoking <code>groupBy</code> on a <code>KTable</code> returns a different intermediate representation: <code>KGroupedTable</code>.<a data-type="indexterm" data-primary="KGroupedTable" id="idm46281562266536" target="_blank" rel="noopener noreferrer"></a> Otherwise, the process of grouping <code>KTables</code> is identical to grouping a <code>KStream</code>. For example, if we wanted to group the <code>players</code> <code>KTable</code> so that we could later perform some aggregation (e.g., count the number of players), then we could use the following code:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KGroupedTable&lt;String, Player&gt; groupedPlayers =
    players.groupBy(
        (key, value) -&gt; KeyValue.pair(key, value),
        Grouped.with(Serdes.String(), JsonSerdes.Player()));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>The preceding code block isn’t needed for this tutorial since we don’t need to group the <code>players</code> table, but we are showing it here to demonstrate the concept. We now know how to group streams and tables, and have completed step 4 of our processor topology (see <a data-type="xref" href="#C4_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-1</a>). Next, we’ll learn how to perform aggregations in Kafka Streams.</p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Aggregations">
        <div class="sect1" id="idm46281562260952">
          <h1>Aggregations</h1>
          <p>One of the final steps required for our leaderboard topology is to calculate the high scores for each game.<a data-type="indexterm" data-primary="aggregations" id="idm46281562259288" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="aggregations" id="idm46281562258584" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="aggregations" id="idm46281562257672" target="_blank" rel="noopener noreferrer"></a> Kafka Streams gives us a set of operators that makes performing these kinds of aggregations very easy:</p>
          <ul>
            <li>
              <p><code>aggregate</code></p></li>
            <li>
              <p><code>reduce</code></p></li>
            <li>
              <p><code>count</code></p></li>
          </ul>
          <p>At a high level, aggregations are just a way of combining multiple input values into a single output value.<a data-type="indexterm" data-primary="aggregate operator" id="idm46281562251944" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="reduce operator" id="idm46281562251240" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="count operator" id="idm46281562250568" target="_blank" rel="noopener noreferrer"></a> We tend to think of aggregations as mathematical operations, but they don’t have to be. While <code>count</code> is a mathematical operation that computes the number of events per key, both the <code>aggregate</code> and <code>reduce</code> operators are more generic, and can combine values using any combinational logic you specify.</p>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p><code>reduce</code> is very similar to <code>aggregate</code>. The difference lies in the return type. The <code>reduce</code> operator requires the output of an aggregation to be of the same type as the input, while the <code>aggregate</code> operator can specify a different type for the output record.</p>
          </div>
          <p>Furthermore, aggregations can be applied to both streams and tables. The semantics are a little different across each, since streams are immutable while tables are mutable. <a data-type="indexterm" data-primary="immutability" data-secondary="of streams" data-secondary-sortas="streams" id="idm46281562244648" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="initializers (aggregate and reduce operators)" id="idm46281562243400" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="adders (aggregate and reduce operators)" id="idm46281562242632" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="subtractors (aggregate and reduce operators)" id="idm46281562241944" target="_blank" rel="noopener noreferrer"></a>This translates into slightly different versions of the <code>aggregate</code> and <code>reduce</code> <span class="keep-together">operators</span>, with the streams version accepting two parameters: an <em>initializer</em> and an <em>adder</em>, and the table version accepting three parameters: an <em>initializer</em>, <em>adder</em>, and <span class="keep-together"><em>subtractor</em></span>.<sup><a data-type="noteref" id="idm46281562236952-marker" href="ch04.html#idm46281562236952" target="_blank" rel="noopener noreferrer">15</a></sup></p>
          <p>Let’s take a look at how to aggregate streams by creating our high scores aggregation.</p>
          <section data-type="sect2" data-pdf-bookmark="Aggregating Streams">
            <div class="sect2" id="idm46281562235960">
              <h2>Aggregating Streams</h2>
              <p>In this section, we’ll learn how to apply aggregations to record streams, which involves creating a function for<a data-type="indexterm" data-primary="streams" data-secondary="aggregating" id="idm46281562234504" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="aggregations" data-secondary="aggregating streams" id="idm46281562233528" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="aggregations" data-tertiary="aggregating streams" id="idm46281562232584" target="_blank" rel="noopener noreferrer"></a> initializing a new aggregate value (called an <em>initializer</em>) and a function for performing subsequent aggregations as new records come in for a given key (called an <em>adder</em> function). First, we’ll learn about initializers.</p>
              <section data-type="sect3" data-pdf-bookmark="Initializer">
                <div class="sect3" id="idm46281562230040">
                  <h3>Initializer</h3>
                  <p>When a new key is seen by our Kafka Streams topology, we need some way of initializing the aggregation.<a data-type="indexterm" data-primary="aggregations" data-secondary="aggregating streams" data-tertiary="Initializer interface" id="idm46281562228200" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="initializers (aggregate and reduce operators)" data-secondary="Initializer interface" id="idm46281562226952" target="_blank" rel="noopener noreferrer"></a> The interface that helps us with this is <code>Initializer</code>, and like many of the classes in the Kafka Streams API, <code>Initializer</code> is a functional interface (i.e., contains a single method), and therefore can be defined as a lambda.</p>
                  <p>For example, if you were to look at the internals of the <code>count</code> aggregation, you’d see an initializer that sets the initial value of the aggregation to <code>0</code>:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">Initializer&lt;Long&gt; countInitializer = () -&gt; 0L; <a class="co" id="co_stateful_processing_CO11-1" href="#callout_stateful_processing_CO11-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO11-1" href="#co_stateful_processing_CO11-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The initializer is defined as a lambda, since the <code>Initializer</code> interface is a functional interface.</p>
                    </dd>
                  </dl>
                  <p>For more complex aggregations, you can provide your own custom initializer instead. For example, to implement a video game leaderboard, we need some way to compute the top three high scores for a given game. To do this, we can create a separate class that will include the logic for tracking the top three scores, and provide a new instance of this class whenever an aggregation needs to be initialized.</p>
                  <p>In this tutorial, we will create a custom class called <code>HighScores</code> to act as our aggregation class. This class will need some underlying data structure to hold the top three scores for a given video game. One approach is to use a <code>TreeSet</code>, which is an ordered set included in the Java standard library, and is therefore pretty convenient for holding high scores (which are inherently ordered).</p>
                  <p>An initial implementation of our data class that we’ll use for the high scores aggregation is shown here:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">public class HighScores {

  private final TreeSet&lt;Enriched&gt; highScores = new TreeSet&lt;&gt;();

}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <p>Now we need to tell Kafka Streams how to initialize our new data class. Initializing a class is simple; we just need to instantiate it:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">Initializer&lt;HighScores&gt; highScoresInitializer = HighScores::new;</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <p>Once we have an initializer for our aggregation, we need to implement the logic for actually performing the aggregation (in this case, keeping track of the top three high scores for each video game).</p>
                </div>
              </section>
              <section data-type="sect3" class="pagebreak-before less_space" data-pdf-bookmark="Adder">
                <div class="sect3" id="idm46281562229416">
                  <h3>Adder</h3>
                  <p>The next thing we need to do in order to build a stream aggregator is to define the logic for combining two aggregates.<a data-type="indexterm" data-primary="adders (aggregate and reduce operators)" data-secondary="Aggregator interface" id="idm46281562209640" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="aggregations" data-secondary="aggregating streams" data-tertiary="adder" id="idm46281562208648" target="_blank" rel="noopener noreferrer"></a> This is accomplished using the <code>Aggregator</code> interface, which, like <code>Initializer</code>, is a functional interface that can be implemented using a lambda. The implementing function needs to accept three parameters:</p>
                  <ul>
                    <li>
                      <p>The record key</p></li>
                    <li>
                      <p>The record value</p></li>
                    <li>
                      <p>The current aggregate value</p></li>
                  </ul>
                  <p>We can create our high scores aggregator with the following code:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">Aggregator&lt;String, Enriched, HighScores&gt; highScoresAdder =
        (key, value, aggregate) -&gt; aggregate.add(value);</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <p>Note that <code>aggregate</code> is a <code>HighScores</code> instance, and since our aggregator invokes the <code>HighScores.add</code> method, we simply need to implement that in our <code>HighScores</code> class. As you can see in the following code block, the code is extremely simple, with the <code>add</code> method simply appending a new high score to the internal <code>TreeSet</code>, and then removing the lowest score if we have more than three high scores:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">public class HighScores {
  private final TreeSet&lt;Enriched&gt; highScores = new TreeSet&lt;&gt;();

  public HighScores add(final Enriched enriched) {
    highScores.add(enriched); <a class="co" id="co_stateful_processing_CO12-1" href="#callout_stateful_processing_CO12-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

    if (highScores.size() &gt; 3) { <a class="co" id="co_stateful_processing_CO12-2" href="#callout_stateful_processing_CO12-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
      highScores.remove(highScores.last());
    }

    return this;
  }
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO12-1" href="#co_stateful_processing_CO12-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Whenever our adder method (<code>HighScores.add</code>) is called by Kafka Streams, we simply add the new record to the underlying <code>TreeSet</code>, which will sort each entry automatically.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO12-2" href="#co_stateful_processing_CO12-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>If we have more than three high scores in our <code>TreeSet</code>, remove the lowest score.</p>
                    </dd>
                  </dl>
                  <p>In order for the <code>TreeSet</code> to know how to sort <code>Enriched</code> objects (and therefore, be able to identify the <code>Enriched</code> record with the lowest score to remove when our <code>highScores</code> aggregate exceeds three values), we will implement the <code>Comparable</code> interface, as shown in <a data-type="xref" href="#C4_COMP_TO" target="_blank" rel="noopener noreferrer">Example&nbsp;4-6</a>.</p>
                  <div id="C4_COMP_TO" data-type="example">
                    <h5><span class="label">Example 4-6. </span>The updated <code>Enriched</code> class, which implements the <code>Comparable</code> interface</h5>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">public class Enriched implements Comparable&lt;Enriched&gt; { <a class="co" id="co_stateful_processing_CO13-1" href="#callout_stateful_processing_CO13-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

  @Override
  public int compareTo(Enriched o) { <a class="co" id="co_stateful_processing_CO13-2" href="#callout_stateful_processing_CO13-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    return Double.compare(o.score, score);
  }

  // omitted for brevity
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO13-1" href="#co_stateful_processing_CO13-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>We will update our <code>Enriched</code> class so that it implements <code>Comparable</code>, since determining the top three high scores will involve comparing one <code>Enriched</code> object to another.<a data-type="indexterm" data-primary="Comparable interface" id="idm46281562172552" target="_blank" rel="noopener noreferrer"></a></p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO13-2" href="#co_stateful_processing_CO13-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Our implementation of the <code>compareTo</code> method uses the <code>score</code> property as a method of comparing two different <code>Enriched</code> objects.</p>
                    </dd>
                  </dl>
                  <p>Now that we have both our initializer and adder function, we can perform the aggregation using the code in <a data-type="xref" href="#C4_AGGREGATE" target="_blank" rel="noopener noreferrer">Example&nbsp;4-7</a>.</p>
                  <div id="C4_AGGREGATE" data-type="example">
                    <h5><span class="label">Example 4-7. </span>Use Kafka Streams’ aggregate operator to perform our high scores aggregation</h5>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">KTable&lt;String, HighScores&gt; highScores =
    grouped.aggregate(highScoresInitializer, highScoresAdder);</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div>
                </div>
              </section>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Aggregating Tables">
            <div class="sect2" id="idm46281562164264">
              <h2>Aggregating Tables</h2>
              <p>The process of aggregating tables is pretty similar to aggregating streams. We need an <em>initializer</em> and an <em>adder</em> function.<a data-type="indexterm" data-primary="tables" data-secondary="aggregating" id="idm46281562161880" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="aggregations" data-secondary="aggregating tables" id="idm46281562160872" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="aggregations" data-tertiary="aggregating tables" id="idm46281562159928" target="_blank" rel="noopener noreferrer"></a> However, tables are mutable, and need to be able to update an aggregate value whenever a key is deleted.<sup><a data-type="noteref" id="idm46281562158440-marker" href="ch04.html#idm46281562158440" target="_blank" rel="noopener noreferrer">16</a></sup> We also need a third parameter, called a <em>subtractor</em> function.</p>
              <section data-type="sect3" data-pdf-bookmark="Subtractor">
                <div class="sect3" id="idm46281562156440">
                  <h3>Subtractor</h3>
                  <p>While this isn’t necessary for the leaderboard example, let’s assume we want to count the number of players in our <code>players</code> <code>KTable</code>.<a data-type="indexterm" data-primary="subtractors (aggregate and reduce operators)" id="idm46281562153656" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="aggregations" data-secondary="aggregating tables" data-tertiary="subtractor" id="idm46281562152888" target="_blank" rel="noopener noreferrer"></a> We could use the <code>count</code> operator, but to demonstrate how to build a subtractor function, we’ll build our own aggregate function that is essentially equivalent to the <code>count</code> operator. A basic implementation of an aggregate that uses a subtractor function (as well as an initializer and adder function, which<a data-type="indexterm" data-primary="KGroupedTable" id="idm46281562150408" target="_blank" rel="noopener noreferrer"></a> is required for both <code>KStream</code> and <code>KTable</code> aggregations), is shown here:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">KGroupedTable&lt;String, Player&gt; groupedPlayers =
  players.groupBy(
      (key, value) -&gt; KeyValue.pair(key, value),
      Grouped.with(Serdes.String(), JsonSerdes.Player()));

groupedPlayers.aggregate(
    () -&gt; 0L, <a class="co" id="co_stateful_processing_CO14-1" href="#callout_stateful_processing_CO14-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    (key, value, aggregate) -&gt; aggregate + 1L, <a class="co" id="co_stateful_processing_CO14-2" href="#callout_stateful_processing_CO14-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    (key, value, aggregate) -&gt; aggregate - 1L); <a class="co" id="co_stateful_processing_CO14-3" href="#callout_stateful_processing_CO14-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO14-1" href="#co_stateful_processing_CO14-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The initializer function initializes the aggregate to 0.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO14-2" href="#co_stateful_processing_CO14-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The adder function increments the current count when a new key is seen.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO14-3" href="#co_stateful_processing_CO14-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The subtractor function decrements the current count when a key is removed.</p>
                    </dd>
                  </dl>
                  <p>We have now completed step 5 of our leaderboard topology (<a data-type="xref" href="#C4_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-1</a>). We’ve written a decent amount of code, so let’s see how the individual code fragments fit together in the next section.</p>
                </div>
              </section>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Putting It All Together">
        <div class="sect1" id="idm46281562132856">
          <h1>Putting It All Together</h1>
          <p>Now that we’ve constructed the individual processing steps on our leaderboard topology, let’s put it all together. <a data-type="xref" href="#C4_IMPL" target="_blank" rel="noopener noreferrer">Example&nbsp;4-8</a> shows how the topology steps we’ve created so far come together.<a data-type="indexterm" data-primary="processor topologies" data-secondary="video game leaderboard application" id="idm46281562130360" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="putting it all together" id="ix_stflprVGLall" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="putting it all together" id="ix_vidldrall" target="_blank" rel="noopener noreferrer"></a></p>
          <div id="C4_IMPL" data-type="example">
            <h5><span class="label">Example 4-8. </span>The processor topology for our video game leaderboard application</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">// the builder is used to construct the topology
StreamsBuilder builder = new StreamsBuilder();

// register the score events stream
KStream&lt;String, ScoreEvent&gt; scoreEvents = <a class="co" id="co_stateful_processing_CO15-1" href="#callout_stateful_processing_CO15-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    builder
        .stream(
          "score-events",
          Consumed.with(Serdes.ByteArray(), JsonSerdes.ScoreEvent()))
        .selectKey((k, v) -&gt; v.getPlayerId().toString()); <a class="co" id="co_stateful_processing_CO15-2" href="#callout_stateful_processing_CO15-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

// create the partitioned players table
KTable&lt;String, Player&gt; players = <a class="co" id="co_stateful_processing_CO15-3" href="#callout_stateful_processing_CO15-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    builder.table("players", Consumed.with(Serdes.String(), JsonSerdes.Player()));

// create the global product table
GlobalKTable&lt;String, Product&gt; products = <a class="co" id="co_stateful_processing_CO15-4" href="#callout_stateful_processing_CO15-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
    builder.globalTable(
      "products",
      Consumed.with(Serdes.String(), JsonSerdes.Product()));

// join params for scoreEvents - players join
Joined&lt;String, ScoreEvent, Player&gt; playerJoinParams =
    Joined.with(Serdes.String(), JsonSerdes.ScoreEvent(), JsonSerdes.Player());

// join scoreEvents - players
ValueJoiner&lt;ScoreEvent, Player, ScoreWithPlayer&gt; scorePlayerJoiner =
    (score, player) -&gt; new ScoreWithPlayer(score, player);
KStream&lt;String, ScoreWithPlayer&gt; withPlayers =
    scoreEvents.join(players, scorePlayerJoiner, playerJoinParams); <a class="co" id="co_stateful_processing_CO15-5" href="#callout_stateful_processing_CO15-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>

// map score-with-player records to products
KeyValueMapper&lt;String, ScoreWithPlayer, String&gt; keyMapper =
  (leftKey, scoreWithPlayer) -&gt; {
    return String.valueOf(scoreWithPlayer.getScoreEvent().getProductId());
  };

// join the withPlayers stream to the product global ktable
ValueJoiner&lt;ScoreWithPlayer, Product, Enriched&gt; productJoiner =
  (scoreWithPlayer, product) -&gt; new Enriched(scoreWithPlayer, product);
KStream&lt;String, Enriched&gt; withProducts =
  withPlayers.join(products, keyMapper, productJoiner); <a class="co" id="co_stateful_processing_CO15-6" href="#callout_stateful_processing_CO15-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>

// Group the enriched product stream
KGroupedStream&lt;String, Enriched&gt; grouped =
  withProducts.groupBy( <a class="co" id="co_stateful_processing_CO15-7" href="#callout_stateful_processing_CO15-7"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
      (key, value) -&gt; value.getProductId().toString(),
      Grouped.with(Serdes.String(), JsonSerdes.Enriched()));

// The initial value of our aggregation will be a new HighScores instance
Initializer&lt;HighScores&gt; highScoresInitializer = HighScores::new;

// The logic for aggregating high scores is implemented in the HighScores.add method
Aggregator&lt;String, Enriched, HighScores&gt; highScoresAdder =
  (key, value, aggregate) -&gt; aggregate.add(value);

// Perform the aggregation, and materialize the underlying state store for querying
KTable&lt;String, HighScores&gt; highScores =
    grouped.aggregate( <a class="co" id="co_stateful_processing_CO15-8" href="#callout_stateful_processing_CO15-8"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/8.png" alt="8" width="12" height="12"></a>
        highScoresInitializer,
        highScoresAdder);</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-1" href="#co_stateful_processing_CO15-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Read the <code>score-events</code> into a <code>KStream</code>.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-2" href="#co_stateful_processing_CO15-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Rekey the messages to meet the co-partitioning requirements needed for the join.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-3" href="#co_stateful_processing_CO15-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Read the <code>players</code> topic into a <code>KTable</code> since the keyspace is large (allowing us to shard the state across multiple application instances) and since we want time synchronized processing for the <code>score-events -&gt; players</code> join.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-4" href="#co_stateful_processing_CO15-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Read the <code>products</code> topic as a <code>GlobalKTable</code>, since the keyspace is small and we don’t need time synchronized processing.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-5" href="#co_stateful_processing_CO15-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Join the <code>score-events</code> stream and the <code>players</code> table.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-6" href="#co_stateful_processing_CO15-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Join the enriched <code>score-events</code> with the <code>products</code> table.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-7" href="#co_stateful_processing_CO15-7" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Group the enriched stream. This is a prerequisite for aggregating.</p>
            </dd>
            <dt>
              <a class="co" id="callout_stateful_processing_CO15-8" href="#co_stateful_processing_CO15-8" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/8.png" alt="8" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Aggregate the grouped stream. The aggregation logic lives in the <code>HighScores</code> class.</p>
            </dd>
          </dl>
          <p>Let’s add the necessary configuration for our application and start streaming:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "dev");
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");

KafkaStreams streams = new KafkaStreams(builder.build(), props);
streams.start();</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>At this point, our application is ready to start receiving records and calculating high scores for our leaderboard. However, we still have one final step to tackle in order to expose the leaderboard results to external clients. Let’s move on to the final step of our processor topology, and learn how to expose the state of our Kafka Streams application using interactive queries.<a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="putting it all together" data-startref="ix_stflprVGLall" id="idm46281562079832" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="putting it all together" data-startref="ix_vidldrall" id="idm46281562078296" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Interactive Queries">
        <div class="sect1" id="C4_IQ">
          <h1>Interactive Queries</h1>
          <p>One of the defining features of Kafka Streams is its ability to expose application state, both locally and to the outside world.<a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" id="ix_vidldrIQ" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="interactive queries" id="ix_stflprVGLIQ" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" id="ix_intqry" target="_blank" rel="noopener noreferrer"></a> The latter makes it easy to build event-driven microservices with extremely low latency. In this tutorial, we can use interactive queries to expose our high score aggregations.</p>
          <p>In order to do this, we need to <em>materialize</em> the state store. We’ll learn how to do this in the next section.</p>
          <section data-type="sect2" data-pdf-bookmark="Materialized Stores">
            <div class="sect2" id="idm46281562070184">
              <h2>Materialized Stores</h2>
              <p>We already know that stateful operators like <code>aggregate</code>, <code>count</code>, <code>reduce</code>, etc., leverage state stores to manage internal state.<a data-type="indexterm" data-primary="state stores" data-secondary="materialized" id="idm46281562067016" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="materialized state stores" id="idm46281562066040" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-secondary="materialized state stores" id="idm46281562065352" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" data-tertiary="materialized state stores" id="idm46281562064392" target="_blank" rel="noopener noreferrer"></a> However, if you look closely at our method for aggregating high scores in <a data-type="xref" href="#C4_AGGREGATE" target="_blank" rel="noopener noreferrer">Example&nbsp;4-7</a>, you won’t see any mention of a state store. This variant of the <code>aggregate</code> method uses an <em>internal state store</em> that is only accessed by the processor topology.<a data-type="indexterm" data-primary="internal state store" id="idm46281562061320" target="_blank" rel="noopener noreferrer"></a></p>
              <p>If we want to enable read-only access of the underlying state store for ad hoc queries, we can use one of the overloaded methods to force the materialization of the state store locally. <em>Materialized state stores</em> differ from internal state stores in that they are explicitly named and are queryable outside of the processor topology. This is where the <code>Materialized</code> class comes in handy. <a data-type="xref" href="#C4_MATERIALIZED" target="_blank" rel="noopener noreferrer">Example&nbsp;4-9</a> shows how to materialize a persistent key-value store using the <code>Materialized</code> class, which will allow us to query the store using interactive queries.<a data-type="indexterm" data-primary="KeyValueStore" id="idm46281562057720" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Materialized class" id="idm46281562057016" target="_blank" rel="noopener noreferrer"></a></p>
              <div id="C4_MATERIALIZED" data-type="example">
                <h5><span class="label">Example 4-9. </span>Materialized state store with minimal configuration</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">KTable&lt;String, HighScores&gt; highScores =
    grouped.aggregate(
        highScoresInitializer,
        highScoresAdder,
        Materialized.&lt;String, HighScores, KeyValueStore&lt;Bytes, byte[]&gt;&gt; <a class="co" id="co_stateful_processing_CO16-1" href="#callout_stateful_processing_CO16-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            as("leader-boards") <a class="co" id="co_stateful_processing_CO16-2" href="#callout_stateful_processing_CO16-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            .withKeySerde(Serdes.String()) <a class="co" id="co_stateful_processing_CO16-3" href="#callout_stateful_processing_CO16-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            .withValueSerde(JsonSerdes.HighScores()));</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO16-1" href="#co_stateful_processing_CO16-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>This variation of the <code>Materialized.as</code> method includes three generics:</p>
                  <ul>
                    <li>
                      <p>The key type of the store (in this case, <code>String</code>)</p></li>
                    <li>
                      <p>The value type of the store (in this case, <code>HighScores</code>)</p></li>
                    <li>
                      <p>The type of state store (in this case, we’ll use a simple key-value store, represented by <code>KeyValueStore&lt;Bytes, byte[]&gt;</code>)</p></li>
                  </ul>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO16-2" href="#co_stateful_processing_CO16-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Provide an explicit name for the store to make it available for querying outside of the processor topology.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO16-3" href="#co_stateful_processing_CO16-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We can customize the materialized state store using a variety of parameters, including the key and value Serdes, as well as other options that we will explore in <a data-type="xref" href="ch06.html#ch6" target="_blank" rel="noopener noreferrer">Chapter&nbsp;6</a>.</p>
                </dd>
              </dl>
              <p>Once we’ve materialized our <code>leader-boards</code> state store, we are almost ready to expose this data via ad hoc queries. The first thing we need to do, however, is to retrieve the store from Kafka Streams.</p>
            </div>
          </section>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Accessing Read-Only State Stores">
            <div class="sect2" id="idm46281562033176">
              <h2>Accessing Read-Only State Stores</h2>
              <p>When we need to access a state store in read-only mode, we need<a data-type="indexterm" data-primary="state stores" data-secondary="read-only, accessing" id="idm46281562031400" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-secondary="read-only state stores, accessing" id="idm46281562030424" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" data-tertiary="accessing read-only state stores" id="idm46281562029416" target="_blank" rel="noopener noreferrer"></a> two pieces of <span class="keep-together">information</span>:</p>
              <ul>
                <li>
                  <p>The name of the state store</p></li>
                <li>
                  <p>The type of state store</p></li>
              </ul>
              <p>As we saw in <a data-type="xref" href="#C4_MATERIALIZED" target="_blank" rel="noopener noreferrer">Example&nbsp;4-9</a>, the name of our state store is <code>leader-boards</code>. We need to retrieve the appropriate read-only wrapper for our underlying state <a data-type="indexterm" data-primary="QueryableStoreTypes class" id="idm46281562023400" target="_blank" rel="noopener noreferrer"></a>store using the <code>QueryableStoreTypes</code> factory class. There are multiple state stores supported, including:</p>
              <ul>
                <li>
                  <p><code>QueryableStoreTypes.keyValueStore()</code></p></li>
                <li>
                  <p><code>QueryableStoreTypes.timestampedKeyValueStore()</code></p></li>
                <li>
                  <p><code>QueryableStoreTypes.windowStore()</code></p></li>
                <li>
                  <p><code>QueryableStoreTypes.timestampedWindowStore()</code></p></li>
                <li>
                  <p><code>QueryableStoreTypes.sessionStore()</code></p></li>
              </ul>
              <p>In our case, we’re using a simple key-value store, so we need the <code>QueryableStoreType.keyValueStore()</code> method. With both the state store name and the state store type, we can instantiate an instance of a queryable state store to be used in interactive queries, by <a data-type="indexterm" data-primary="KafkaStreams.store method" id="idm46281562015464" target="_blank" rel="noopener noreferrer"></a>using the <code>KafkaStreams.store()</code> method, as shown in <a data-type="xref" href="#C4_GET_STORE" target="_blank" rel="noopener noreferrer">Example&nbsp;4-10</a>.</p>
              <div id="C4_GET_STORE" data-type="example">
                <h5><span class="label">Example 4-10. </span>Instantiate a key-value store that can be used for performing interactive queries</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">ReadOnlyKeyValueStore&lt;String, HighScores&gt; stateStore =
    streams.store(
        StoreQueryParameters.fromNameAndType(
            "leader-boards",
            QueryableStoreTypes.keyValueStore()));</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <p>When we have our state store instance, we can query it. The next section discusses the different query types available in key-value stores.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Querying Nonwindowed Key-Value Stores">
            <div class="sect2" id="C4_QUERY_NON_WINDOW">
              <h2>Querying Nonwindowed Key-Value Stores</h2>
              <p>Each state store type supports different kinds of queries. For example, windowed <a data-type="indexterm" data-primary="interactive queries" data-secondary="querying non-windowed key-value stores" id="idm46281562008504" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="key-value stores" data-secondary="non-windowed, querying" id="idm46281562007512" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" data-tertiary="querying non-windowed key-value stores" id="idm46281562006568" target="_blank" rel="noopener noreferrer"></a>stores (e.g., <code>ReadOnlyWindowStore</code>) support key lookups using time ranges, while simple key-value stores (<code>ReadOnlyKeyValueStore</code>) support point lookups, range scans, and count queries.</p>
              <p>We will discuss windowed state stores in the next chapter, so for now, let’s demonstrate the kinds of queries we can make to our <code>leader-boards</code> store.</p>
              <p>The easiest way to determine which query types are available for your state store type is to check the underlying interface. As we can see from the interface definition in the following snippet, simple key-value stores support several different types of queries:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">public interface ReadOnlyKeyValueStore&lt;K, V&gt; {

    V get(K key);

    KeyValueIterator&lt;K, V&gt; range(K from, K to);

    KeyValueIterator&lt;K, V&gt; all();

    long approximateNumEntries();
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Let’s take a look at each of these query types, starting with the first: point lookups (<code>get()</code>).</p>
              <section data-type="sect3" data-pdf-bookmark="Point lookups">
                <div class="sect3" id="idm46281562000696">
                  <h3>Point lookups</h3>
                  <p>Perhaps the most common query type, point lookups simply involve querying the state store for an individual key.<a data-type="indexterm" data-primary="interactive queries" data-secondary="querying non-windowed key-value stores" data-tertiary="point lookups" id="idm46281561999208" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="key-value stores" data-secondary="non-windowed, querying" data-tertiary="point lookups" id="idm46281561997944" target="_blank" rel="noopener noreferrer"></a> To perform this type of query, we can use the <code>get</code> method to retrieve the value for a given key. For example:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">HighScores highScores = stateStore.get(key);</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <p>Note that a point lookup will return either a deserialized instance of the value (in this case, a <code>HighScores</code> object, since that is what we’re storing in our state store) or <code>null</code> if the key is not found.</p>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Range scans">
                <div class="sect3" id="idm46281561993848">
                  <h3>Range scans</h3>
                  <p>Simple key-value stores also support range scan queries.<a data-type="indexterm" data-primary="key-value stores" data-secondary="non-windowed, querying" data-tertiary="range scans" id="idm46281561992280" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="range scans" id="idm46281561991032" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-secondary="querying non-windowed key-value stores" data-tertiary="range scans" id="idm46281561990360" target="_blank" rel="noopener noreferrer"></a> Range scans return an iterator for an inclusive range of keys. It’s very important to close the iterator once you are finished with it to avoid memory leaks.</p>
                  <p>The following code block shows how to execute a range query, iterate over each result, and close the iterator:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">KeyValueIterator&lt;String, HighScores&gt; range = stateStore.range(1, 7); <a class="co" id="co_stateful_processing_CO17-1" href="#callout_stateful_processing_CO17-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

while (range.hasNext()) {
    KeyValue&lt;String, HighScores&gt; next = range.next(); <a class="co" id="co_stateful_processing_CO17-2" href="#callout_stateful_processing_CO17-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

    String key = next.key;
    HighScores highScores = next.value; <a class="co" id="co_stateful_processing_CO17-3" href="#callout_stateful_processing_CO17-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>

    // do something with high scores object
}

range.close(); <a class="co" id="co_stateful_processing_CO17-4" href="#callout_stateful_processing_CO17-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO17-1" href="#co_stateful_processing_CO17-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Returns an iterator that can be used for iterating through each key in the selected range.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO17-2" href="#co_stateful_processing_CO17-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Get the next element in the iteration.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO17-3" href="#co_stateful_processing_CO17-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The <code>HighScores</code> value is available in the <code>next.value</code> property.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_stateful_processing_CO17-4" href="#co_stateful_processing_CO17-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>It’s very important to close the iterator to avoid memory leaks. Another way of closing is to use a try-with-resources statement when getting the iterator.</p>
                    </dd>
                  </dl>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="All entries">
                <div class="sect3" id="idm46281561968408">
                  <h3>All entries</h3>
                  <p>Similar to a range scan, the <code>all()</code> query returns an iterator of key-value pairs, and is similar to an unfiltered <code>SELECT *</code> query.<a data-type="indexterm" data-primary="key-value stores" data-secondary="non-windowed, querying" data-tertiary="all entries" id="idm46281561966168" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-secondary="querying non-windowed key-value stores" data-tertiary="all entries" id="idm46281561965000" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="SELECT statements" data-secondary="SELECT *" id="idm46281561963768" target="_blank" rel="noopener noreferrer"></a> However, this query type will return an iterator for all of the entries in our state store, instead of those within a specific key range only. As with range queries, it’s important to close the iterator once you’re finished with it to avoid memory leaks. The following code shows how to execute an <code>all()</code> query. Iterating through the results and closing the iterator is the same as the range scan query, so we have omitted that logic for brevity:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">KeyValueIterator&lt;String, HighScores&gt; range = stateStore.all();</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Number of entries">
                <div class="sect3" id="C4_NUM_ENTRIES">
                  <h3>Number of entries</h3>
                  <p>Finally, the last query type is similar to a <code>COUNT(*)</code> query, and returns the approximate number of entries in the underlying state store.<a data-type="indexterm" data-primary="key-value stores" data-secondary="non-windowed, querying" data-tertiary="number of entries" id="idm46281561958760" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-secondary="querying non-windowed key-value stores" data-tertiary="number of entries" id="idm46281561957512" target="_blank" rel="noopener noreferrer"></a></p>
                  <div data-type="note" epub:type="note">
                    <h6>Note</h6>
                    <p>When using RocksDB persistent stores, the returned value is approximate since calculating a precise<a data-type="indexterm" data-primary="RocksDB" data-secondary="number of entries queries on" id="idm46281561955240" target="_blank" rel="noopener noreferrer"></a> count can be expensive and, when it comes to RocksDB-backed stores, challenging as well. Taken from the <a href="https://oreil.ly/1r9GD" target="_blank" rel="noopener noreferrer">RocksDB FAQ</a>:</p>
                    <blockquote>
                      <p>Obtaining an accurate number of keys [in] LSM databases like RocksDB is a challenging problem as they have duplicate keys and deletion entries (i.e., tombstones) that will require a full compaction in order to get an accurate number of keys. In addition, if the RocksDB database contains merge operators, it will also make the estimated number of keys less accurate.</p>
                    </blockquote>
                    <p>On the other hand, if using an in-memory store, the count will be exact.</p>
                  </div>
                  <p>To execute this type of query against a simple key-value store, we could run the <span class="keep-together">following</span> code:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">long approxNumEntries = stateStore.approximateNumEntries();</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <p>Now that we know how to query simple key-value stores, let’s see where we can actually execute these queries from.</p>
                </div>
              </section>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Local Queries">
            <div class="sect2" id="idm46281562009848">
              <h2>Local Queries</h2>
              <p>Each instance of a Kafka Streams application can query its own local state.<a data-type="indexterm" data-primary="local queries" id="idm46281561947784" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-secondary="local queries" id="idm46281561947080" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" data-tertiary="local queries" id="idm46281561946136" target="_blank" rel="noopener noreferrer"></a> However, it’s important to remember that unless you are materializing a <code>GlobalKTable</code> or running a single instance of your Kafka Streams app,<sup><a data-type="noteref" id="idm46281561944440-marker" href="ch04.html#idm46281561944440" target="_blank" rel="noopener noreferrer">17</a></sup> the local state will only represent a partial view of the entire application state (this is the nature of a <code>KTable</code>, as discussed in <a data-type="xref" href="#C4_KTABLE" target="_blank" rel="noopener noreferrer">“KTable”</a>).</p>
              <p>Luckily for us, Kafka Streams provides some additional methods that make it easy to connect distributed state stores, and to execute <em>remote queries</em>, which allow us to query the entire state of our application. We’ll learn about remote queries next.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Remote Queries">
            <div class="sect2" id="C4_REMOTE_QUERIES">
              <h2>Remote Queries</h2>
              <p>In order to query the full state of <a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" data-tertiary="remote queries" id="ix_vidldrIQrem" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="remote queries" id="ix_remqry" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-secondary="remote queries" id="ix_intqryrem" target="_blank" rel="noopener noreferrer"></a>our application, we need to:</p>
              <ul>
                <li>
                  <p>Discover which instances contain the various fragments of our application state</p></li>
                <li>
                  <p>Add a remote procedure call (RPC) or REST <em>service</em> to expose the local state to other running application instances<sup><a data-type="noteref" id="idm46281561932440-marker" href="ch04.html#idm46281561932440" target="_blank" rel="noopener noreferrer">18</a></sup></p></li>
                <li>
                  <p>Add an RPC or REST <em>client</em> for querying remote state stores from a running application instance<a data-type="indexterm" data-primary="RPCs (remote procedure calls), service/client for remote queries" id="idm46281561930536" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="REST APIs" data-secondary="service/client for remote queries" id="idm46281561929864" target="_blank" rel="noopener noreferrer"></a></p></li>
              </ul>
              <p>Regarding the last two points, you have a lot of flexibility in choosing which server and client components you want to use for inter-instance communication. In this tutorial, we’ll use <a href="https://javalin.io" target="_blank" rel="noopener noreferrer">Javalin</a> to implement a REST service due to its simple API.<a data-type="indexterm" data-primary="Javalin, using to implement REST service" id="idm46281561927032" target="_blank" rel="noopener noreferrer"></a> We will also use <a href="https://oreil.ly/okhttp" target="_blank" rel="noopener noreferrer">OkHttp</a>, developed by Square, for<a data-type="indexterm" data-primary="OkHttp, using to implement REST client" id="idm46281561925416" target="_blank" rel="noopener noreferrer"></a> our REST client for its ease of use. Let’s add these dependencies to our application by updating our <em>build.gradle</em> file with the following:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">dependencies {

  // required for interactive queries (server)
  implementation 'io.javalin:javalin:3.12.0'

  // required for interactive queries (client)
  implementation 'com.squareup.okhttp3:okhttp:4.9.0'

  // other dependencies
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Now let’s tackle the issue of instance discovery. <a data-type="indexterm" data-primary="instance discovery" id="idm46281561922824" target="_blank" rel="noopener noreferrer"></a>We need some way of broadcasting which instances are running at any given point in time and where they are running.<a data-type="indexterm" data-primary="APPLICATION_SERVER_CONFIG parameter" id="idm46281561921864" target="_blank" rel="noopener noreferrer"></a> The latter can be accomplished using the <span class="keep-together"><code>APPLICATION_SERVER_CONFIG</code></span> parameter to specify a host and port pair in Kafka Streams, as shown here:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">Properties props = new Properties();

props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, "myapp:8080"); <a class="co" id="co_stateful_processing_CO18-1" href="#callout_stateful_processing_CO18-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

// other Kafka Streams properties omitted for brevity

KafkaStreams streams = new KafkaStreams(builder.build(), props);</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO18-1" href="#co_stateful_processing_CO18-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Configure an endpoint. This will be communicated to other running application instances through Kafka’s consumer group protocol. It’s important to use an IP and port pair that other instances can use to communicate with your application (i.e., <code>localhost</code> would not work since it would resolve to different IPs depending on the instance).</p>
                </dd>
              </dl>
              <p>Note that setting the <code>APPLICATION_SERVER_CONFIG</code> parameter config doesn’t actually tell Kafka Streams to start listening on whatever port you configure. In fact, Kafka Streams does not include a built-in RPC service. However, this host/port information is transmitted to other running instances of your Kafka Streams application and is made available through dedicated API methods, which we will discuss later. But first, let’s set up our REST service to start listening on the appropriate port (<code>8080</code> in this example).</p>
              <p>In terms of code maintainability, it makes sense to define our leaderboard REST service in a dedicated file, separate from the topology definition. The following code block shows a simple implementation of the leaderboard service:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">class LeaderboardService {
  private final HostInfo hostInfo; <a class="co" id="co_stateful_processing_CO19-1" href="#callout_stateful_processing_CO19-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  private final KafkaStreams streams; <a class="co" id="co_stateful_processing_CO19-2" href="#callout_stateful_processing_CO19-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

  LeaderboardService(HostInfo hostInfo, KafkaStreams streams) {
    this.hostInfo = hostInfo;
    this.streams = streams;
  }

  ReadOnlyKeyValueStore&lt;String, HighScores&gt; getStore() { <a class="co" id="co_stateful_processing_CO19-3" href="#callout_stateful_processing_CO19-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    return streams.store(
        StoreQueryParameters.fromNameAndType(
            "leader-boards",
            QueryableStoreTypes.keyValueStore()));
  }

  void start() {
    Javalin app = Javalin.create().start(hostInfo.port()); <a class="co" id="co_stateful_processing_CO19-4" href="#callout_stateful_processing_CO19-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>

    app.get("/leaderboard/:key", this::getKey); <a class="co" id="co_stateful_processing_CO19-5" href="#callout_stateful_processing_CO19-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
  }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO19-1" href="#co_stateful_processing_CO19-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p><code>HostInfo</code> is a simple wrapper class in Kafka Streams that contains a hostname and port.<a data-type="indexterm" data-primary="HostInfo wrapper class" id="idm46281561897144" target="_blank" rel="noopener noreferrer"></a> We’ll see how to instantiate this shortly.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO19-2" href="#co_stateful_processing_CO19-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>We need to keep track of the local Kafka Streams instance. We will use some API methods on this instance in the next code block.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO19-3" href="#co_stateful_processing_CO19-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Add a dedicated method for retrieving the state store that contains the leaderboard aggregations. This follows the same method for retrieving a read-only state store wrapper that we saw in <a data-type="xref" href="#C4_GET_STORE" target="_blank" rel="noopener noreferrer">Example&nbsp;4-10</a>.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO19-4" href="#co_stateful_processing_CO19-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Start the Javalin-based web service on the configured port.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO19-5" href="#co_stateful_processing_CO19-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Adding endpoints with Javalin is easy. We simply map a URL path to a method, which we will implement shortly. Path parameters, which are specified with a leading colon (e.g., <code>:key</code>), allow us to create dynamic endpoints. This is ideal for a point lookup query.</p>
                </dd>
              </dl>
              <p>Now, let’s implement the <code>/leaderboard/:key</code> endpoint, which will show the high scores for a given key (which in this case is a product ID). As we recently learned, we can use a point lookup to retrieve a single value from our state store. A naive implementation is shown in the following:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">void getKey(Context ctx) {
    String productId = ctx.pathParam("key");
    HighScores highScores = getStore().get(productId); <a class="co" id="co_stateful_processing_CO20-1" href="#callout_stateful_processing_CO20-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    ctx.json(highScores.toList()); <a class="co" id="co_stateful_processing_CO20-2" href="#callout_stateful_processing_CO20-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO20-1" href="#co_stateful_processing_CO20-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Use a point lookup to retrieve a value from the local state store.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO20-2" href="#co_stateful_processing_CO20-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Note: the <code>toList()</code> method is available in the source code.</p>
                </dd>
              </dl>
              <p>Unfortunately, this isn’t sufficient. Consider the example where we have two running instances of our Kafka Streams application. Depending on <em>which</em> instance we query and <em>when</em> we issue the query (state can move around whenever there is a consumer rebalance), we may not be able to retrieve the requested value. <a data-type="xref" href="#C4_LOCAL" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-7</a> shows this conundrum.</p>
              <figure>
                <div id="C4_LOCAL" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0407.png" style="width: 20rem">
                  <h6><span class="label">Figure 4-7. </span>When state is partitioned across multiple application instances, local queries are not sufficient</h6>
                </div>
              </figure>
              <p>Fortunately, Kafka Streams<a data-type="indexterm" data-primary="queryMetadataForKey method" id="idm46281561867848" target="_blank" rel="noopener noreferrer"></a> provides a method called <code>queryMetadataForKey</code>,<sup><a data-type="noteref" id="idm46281561866616-marker" href="ch04.html#idm46281561866616" target="_blank" rel="noopener noreferrer">19</a></sup> which allows us to discover the application instance (local or remote) that a specific key lives on. An improved implementation of our <code>getKey</code> method is shown in <a data-type="xref" href="#C4_REMOTE_EG" target="_blank" rel="noopener noreferrer">Example&nbsp;4-11</a>.</p>
              <div id="C4_REMOTE_EG" data-type="example">
                <h5><span class="label">Example 4-11. </span>An updated implementation of the <code>getKey</code> method, which leverages remote queries to pull data from different application instances</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">void getKey(Context ctx) {

  String productId = ctx.pathParam("key");

  KeyQueryMetadata metadata =
      streams.queryMetadataForKey(
        "leader-boards", productId, Serdes.String().serializer()); <a class="co" id="co_stateful_processing_CO21-1" href="#callout_stateful_processing_CO21-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

  if (hostInfo.equals(metadata.activeHost())) {
    HighScores highScores = getStore().get(productId); <a class="co" id="co_stateful_processing_CO21-2" href="#callout_stateful_processing_CO21-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

    if (highScores == null) { <a class="co" id="co_stateful_processing_CO21-3" href="#callout_stateful_processing_CO21-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
      // game was not found
      ctx.status(404);
      return;
    }

    // game was found, so return the high scores
    ctx.json(highScores.toList()); <a class="co" id="co_stateful_processing_CO21-4" href="#callout_stateful_processing_CO21-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
    return;
  }

  // a remote instance has the key
  String remoteHost = metadata.activeHost().host();
  int remotePort = metadata.activeHost().port();
  String url =
    String.format(
        "http://%s:%d/leaderboard/%s",
        remoteHost, remotePort, productId); <a class="co" id="co_stateful_processing_CO21-5" href="#callout_stateful_processing_CO21-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>

  OkHttpClient client = new OkHttpClient();
  Request request = new Request.Builder().url(url).build();

  try (Response response = client.newCall(request).execute()) { <a class="co" id="co_stateful_processing_CO21-6" href="#callout_stateful_processing_CO21-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
    ctx.result(response.body().string());
  } catch (Exception e) {
    ctx.status(500);
  }
}</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO21-1" href="#co_stateful_processing_CO21-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p><code>queryMetadataForKey</code> allows us to find which host a specific key should live on.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO21-2" href="#co_stateful_processing_CO21-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>If the local instance has the key, just query the local state store.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO21-3" href="#co_stateful_processing_CO21-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>queryMetadataForKey</code> method doesn’t actually check to see if the key exists. It uses the default stream partitioner<sup><a data-type="noteref" id="idm46281561840392-marker" href="ch04.html#idm46281561840392" target="_blank" rel="noopener noreferrer">20</a></sup> to determine where the key <em>would exist, if it existed</em>. Therefore, we check for null (which is returned if the key isn’t found) and return a <code>404</code> response if it doesn’t exist.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO21-4" href="#co_stateful_processing_CO21-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Return a formatted response containing the high scores.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO21-5" href="#co_stateful_processing_CO21-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>If we made it this far, then the key exists on a remote host, if it exists at all. Therefore, construct a URL using the metadata, which includes the host and port of the Kafka Streams instance that would contain the specified key.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO21-6" href="#co_stateful_processing_CO21-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Invoke the request, and return the result if successful.</p>
                </dd>
              </dl>
              <p>To help visualize what is happening here, <a data-type="xref" href="#C4_REMOTE_QUERY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-8</a> shows how distributed state stores can be connected using a combination of instance discovery and an RPC/REST service.</p>
              <figure>
                <div id="C4_REMOTE_QUERY" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0408.png" style="width: 20rem">
                  <h6><span class="label">Figure 4-8. </span>Remote queries allow us to query the state of other running application instances</h6>
                </div>
              </figure>
              <p>But what if you need to execute a query that doesn’t operate on a single key? For example, what if you need to count the number of entries across all of your distributed state stores? The <code>queryMetadataForKey</code> wouldn’t work well in this case, since it requires us to specify a single key. <a data-type="indexterm" data-primary="allMetadataForStore method" id="idm46281561825944" target="_blank" rel="noopener noreferrer"></a>Instead, we would leverage another Kafka Streams method, called <code>allMetadataForStore</code>, which returns the endpoint for every running Kafka Streams application that shares the same application ID <em>and</em> has at least one active partition for the provided store name.</p>
              <p>Let’s add a new endpoint to our leaderboard service that surfaces the number of high score records across all of the running application instances:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">app.get("/leaderboard/count", this::getCount);</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Now, we’ll implement the <code>getCount</code> method referenced in the preceding code, which leverages the <code>allMetadataForStore</code> method to get the total number of records in each remote state store:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">void getCount(Context ctx) {
  long count = getStore().approximateNumEntries(); <a class="co" id="co_stateful_processing_CO22-1" href="#callout_stateful_processing_CO22-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                                          <a class="co" id="co_stateful_processing_CO22-2" href="#callout_stateful_processing_CO22-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  for (StreamsMetadata metadata : streams.allMetadataForStore("leader-boards")) {
    if (!hostInfo.equals(metadata.hostInfo())) {
      continue; <a class="co" id="co_stateful_processing_CO22-3" href="#callout_stateful_processing_CO22-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    }
    count += fetchCountFromRemoteInstance( <a class="co" id="co_stateful_processing_CO22-4" href="#callout_stateful_processing_CO22-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
      metadata.hostInfo().host(),
      metadata.hostInfo().port());
  }

  ctx.json(count);
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_stateful_processing_CO22-1" href="#co_stateful_processing_CO22-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Initialize the count with the number of entries in the local state store.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO22-2" href="#co_stateful_processing_CO22-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>On the following line, we use the <code>allMetadataForStore</code> method to retrieve the host/port pairs for each Kafka Streams instance that contains a fragment of the state we want to query.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO22-3" href="#co_stateful_processing_CO22-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>If the metadata is for the current host, then continue through the loop since we’ve already pulled the entry count from the local state store.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_stateful_processing_CO22-4" href="#co_stateful_processing_CO22-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>If the metadata does not pertain to the local instance, then retrieve the count from the remote instance. We’ve omitted the implementation details of <code>fetchCountFromRemoteInstance</code> from this text since it is similar to what we saw in <a data-type="xref" href="#C4_REMOTE_EG" target="_blank" rel="noopener noreferrer">Example&nbsp;4-11</a>, where we instantiated a REST client and issued a request against a remove application instance. If you’re interested in the implementation details, please check the source code for this chapter.</p>
                </dd>
              </dl>
              <p>This completes the last step of our leaderboard topology (see <a data-type="xref" href="#C4_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;4-1</a>). We can now run our application, generate some dummy data, and query our leaderboard <span class="keep-together">service</span>.</p>
              <p>The dummy data for each of the source topics is shown in <a data-type="xref" href="#C4_DUMMY_DATA" target="_blank" rel="noopener noreferrer">Example&nbsp;4-12</a>.</p>
              <div data-type="note" epub:type="note">
                <h6>Note</h6>
                <p>For the keyed topics (<code>players</code> and <code>products</code>), the record key is formatted as <span class="keep-together"><code>&lt;key&gt;|&lt;value&gt;</code>.</span> For the <code>score-events</code> topic, the dummy records are simply formatted as <code>&lt;value&gt;</code>.</p>
              </div>
              <div id="C4_DUMMY_DATA" data-type="example">
                <h5><span class="label">Example 4-12. </span>Dummy records that we will be producing to our source topics</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting"># players
1|{"id": 1, "name": "Elyse"}
2|{"id": 2, "name": "Mitch"}
3|{"id": 3, "name": "Isabelle"}
4|{"id": 4, "name": "Sammy"}

# products
1|{"id": 1, "name": "Super Smash Bros"}
6|{"id": 6, "name": "Mario Kart"}

# score-events
{"score": 1000, "product_id": 1, "player_id": 1}
{"score": 2000, "product_id": 1, "player_id": 2}
{"score": 4000, "product_id": 1, "player_id": 3}
{"score": 500, "product_id": 1, "player_id": 4}
{"score": 800, "product_id": 6, "player_id": 1}
{"score": 2500, "product_id": 6, "player_id": 2}
{"score": 9000.0, "product_id": 6, "player_id": 3}
{"score": 1200.0, "product_id": 6, "player_id": 4}</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <p>If we produce this dummy data into the appropriate topics and then query our leaderboard service, we will see that our Kafka Streams application not only processed the high scores, but is now exposing the results of our stateful operations. An example response to an interactive query is shown in the following code block:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">$ curl -s localhost:7000/leaderboard/1 | jq '.'

[
  {
    "playerId": 3,
    "productId": 1,
    "playerName": "Isabelle",
    "gameName": "Super Smash Bros",
    "score": 4000
  },
  {
    "playerId": 2,
    "productId": 1,
    "playerName": "Mitch",
    "gameName": "Super Smash Bros",
    "score": 2000
  },
  {
    "playerId": 1,
    "productId": 1,
    "playerName": "Elyse",
    "gameName": "Super Smash Bros",
    "score": 1000
  }
]</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" class="less_space" data-pdf-bookmark="Summary">
        <div class="sect1" id="idm46281561939944">
          <h1>Summary</h1>
          <p>In this chapter, you learned <a data-type="indexterm" data-primary="interactive queries" data-secondary="remote queries" data-startref="ix_intqryrem" id="idm46281561786968" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" data-tertiary="remote queries" data-startref="ix_vidldrIQrem" id="idm46281561785720" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="remote queries" data-startref="ix_remqry" id="idm46281561784168" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="video game leaderboard tutorial" data-secondary="interactive queries" data-startref="ix_vidldrIQ" id="idm46281561783224" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processing" data-secondary="video game leaderboard tutorial" data-tertiary="interactive queries" data-startref="ix_stflprVGLIQ" id="idm46281561781992" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" data-startref="ix_intqry" id="idm46281561780488" target="_blank" rel="noopener noreferrer"></a>how Kafka Streams captures information about the events it consumes, and how to leverage the remembered information (state) to perform more advanced stream processing tasks, including:</p>
          <ul>
            <li>
              <p>Performing a <code>KStream-KTable</code> join</p></li>
            <li>
              <p>Rekeying messages to meet the co-partitioning requirements for certain join types</p></li>
            <li>
              <p>Performing a <code>KStream-GlobalKTable</code> join</p></li>
            <li>
              <p>Grouping records into intermediate representations (<code>KGroupedStream</code>, <code>KGroupedTable</code>) to prepare our data for aggregating</p></li>
            <li>
              <p>Aggregating streams and tables</p></li>
            <li>
              <p>Using the interactive queries to expose the state of our application using both local and remote queries</p></li>
          </ul>
          <p>In the next chapter, we will discuss another aspect of stateful programming that is concerned with not only <em>what</em> events our application has seen, but <em>when</em> they occurred. Time plays a key role in stateful processing, so understanding the different notions of time and also the several time-based abstractions in the Kafka Streams library will help us expand our knowledge of stateful processing even further.<a data-type="indexterm" data-primary="stateful processing" data-startref="ix_stflpr" id="idm46281561769848" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <div data-type="footnotes">
        <p data-type="footnote" id="idm46281562965464"><sup><a href="ch04.html#idm46281562965464-marker" target="_blank" rel="noopener noreferrer">1</a></sup> For more information about stream-relational processing platforms, please see Robert Yokota’s <a href="https://oreil.ly/7u71d" target="_blank" rel="noopener noreferrer">2018 blog post on the subject</a>.</p>
        <p data-type="footnote" id="idm46281562912456"><sup><a href="ch04.html#idm46281562912456-marker" target="_blank" rel="noopener noreferrer">2</a></sup> In memory, on disk, or some combination of both.</p>
        <p data-type="footnote" id="idm46281562881256"><sup><a href="ch04.html#idm46281562881256-marker" target="_blank" rel="noopener noreferrer">3</a></sup> LevelDB was written at Google, but when Facebook engineers started using it, they found it to be too slow for their embedded workflows. By changing the single-threaded compaction process in LevelDB to a multi-threaded compaction process, and by leveraging bloom filters for reads, both read and write performance were drastically improved.</p>
        <p data-type="footnote" id="idm46281562871912"><sup><a href="ch04.html#idm46281562871912-marker" target="_blank" rel="noopener noreferrer">4</a></sup> We mentioned that state stores are highly configurable, and even fault tolerance can be turned off by disabling the change logging behavior.</p>
        <p data-type="footnote" id="idm46281562862072"><sup><a href="ch04.html#idm46281562862072-marker" target="_blank" rel="noopener noreferrer">5</a></sup> For example, <code>inMemoryKeyValueStore</code> uses a Java <code>TreeMap</code>, which is based on a red-black tree, while all persistent key-value stores use RocksDB.</p>
        <p data-type="footnote" id="idm46281562860232"><sup><a href="ch04.html#idm46281562860232-marker" target="_blank" rel="noopener noreferrer">6</a></sup> For example, window stores are key-value stores, but the keys also include the window time in addition to the record key.</p>
        <p data-type="footnote" id="idm46281562828440"><sup><a href="ch04.html#idm46281562828440-marker" target="_blank" rel="noopener noreferrer">7</a></sup> Tim Berglund and Yaroslav Tkachenko talk about Activision’s use case in the <a href="https://oreil.ly/gNYZZ" target="_blank" rel="noopener noreferrer"><em>Streaming Audio</em> podcast</a>.</p>
        <p data-type="footnote" id="idm46281562823976"><sup><a href="ch04.html#idm46281562823976-marker" target="_blank" rel="noopener noreferrer">8</a></sup> We’ve already seen how Kafka Streams can write directly to output topics, which allows us to <em>push</em> processed/enriched data to downstream applications. However, interactive queries can be used by clients who want to issue ad hoc queries against a Kafka Streams application instead.</p>
        <p data-type="footnote" id="idm46281562780760"><sup><a href="ch04.html#idm46281562780760-marker" target="_blank" rel="noopener noreferrer">9</a></sup> As mentioned in <a data-type="xref" href="ch03.html#ch3" target="_blank" rel="noopener noreferrer">Chapter&nbsp;3</a>, if our topics contained Avro data, we could define our data model in an Avro schema file instead.</p>
        <p data-type="footnote" id="idm46281562728968"><sup><a href="ch04.html#idm46281562728968-marker" target="_blank" rel="noopener noreferrer">10</a></sup> We can also use <code>KStream</code>s for lookup/join operations, but this is always a windowed operation, so we have reserved discussion of this topic until the next chapter.</p>
        <p data-type="footnote" id="idm46281562675192"><sup><a href="ch04.html#idm46281562675192-marker" target="_blank" rel="noopener noreferrer">11</a></sup> Florian Trossbach and Matthias J. Sax go much deeper on this subject in their <a href="https://oreil.ly/dUo3a" target="_blank" rel="noopener noreferrer">“Crossing the Streams: Joins in Apache Kafka” article</a>.</p>
        <p data-type="footnote" id="idm46281562592008"><sup><a href="ch04.html#idm46281562592008-marker" target="_blank" rel="noopener noreferrer">12</a></sup> <code>UNION</code> queries are another method for combining datasets in the relational world. The behavior of the <code>merge</code> operator in Kafka Streams is more closely related to how a <code>UNION</code> query works.</p>
        <p data-type="footnote" id="idm46281562350664"><sup><a href="ch04.html#idm46281562350664-marker" target="_blank" rel="noopener noreferrer">13</a></sup> If you’re not using Confluent Platform, the script is <em>kafka-topics.sh</em>.</p>
        <p data-type="footnote" id="idm46281562333624"><sup><a href="ch04.html#idm46281562333624-marker" target="_blank" rel="noopener noreferrer">14</a></sup> The <code>GlobalKTable</code> side of the join will still use the record key for the lookup.</p>
        <p data-type="footnote" id="idm46281562236952"><sup><a href="ch04.html#idm46281562236952-marker" target="_blank" rel="noopener noreferrer">15</a></sup> Streams are append-only, so do not need a subtractor.</p>
        <p data-type="footnote" id="idm46281562158440"><sup><a href="ch04.html#idm46281562158440-marker" target="_blank" rel="noopener noreferrer">16</a></sup> We haven’t talked about deleting keys yet, but we will cover this topic in <a data-type="xref" href="ch06.html#ch6" target="_blank" rel="noopener noreferrer">Chapter&nbsp;6</a>, when we discuss cleaning up state stores.</p>
        <p data-type="footnote" id="idm46281561944440"><sup><a href="ch04.html#idm46281561944440-marker" target="_blank" rel="noopener noreferrer">17</a></sup> The latter of these is not advisable. Running a single Kafka Streams application would consolidate the entire application state to a single instance, but Kafka Streams is meant to be run in a distributed fashion for maximizing performance and fault tolerance.</p>
        <p data-type="footnote" id="idm46281561932440"><sup><a href="ch04.html#idm46281561932440-marker" target="_blank" rel="noopener noreferrer">18</a></sup> And other clients if desired, e.g., humans.</p>
        <p data-type="footnote" id="idm46281561866616"><sup><a href="ch04.html#idm46281561866616-marker" target="_blank" rel="noopener noreferrer">19</a></sup> Which replaces the <code>metadataForKey</code> method that was widely used in versions &lt; 2.5, but officially deprecated in that release.</p>
        <p data-type="footnote" id="idm46281561840392"><sup><a href="ch04.html#idm46281561840392-marker" target="_blank" rel="noopener noreferrer">20</a></sup> There is an overloaded version of the <code>queryMetadataForKey</code> method that accepts a custom <code>StreamPartitioner</code> as well.</p>
      </div>
    </div>
    <div class="chapter" id="ch5">
      <h1><span class="label">Chapter 5. </span>Windows and Time</h1>
      <p>Time is such an important concept that we measure our lives by the passing of it.<a data-type="indexterm" data-primary="time" id="idm46281561767352" target="_blank" rel="noopener noreferrer"></a> Each year, half a dozen people stand around me, singing happy birthday, and as the last flat note dissipates from the air, a cake is offered at the feet of this mysterious force. I like to think the cake is for me, but it’s for time.<a data-type="indexterm" data-primary="time windows" data-see="windows" id="idm46281561766264" target="_blank" rel="noopener noreferrer"></a></p>
      <p>Not only is time so intricately woven into the physical world, but it also permeates our event streams. In order to unlock the full power of Kafka Streams, we must understand the relationship between events and time. This chapter explores this relationship in detail, and will give us hands-on experience with something called <em>windows</em>.<a data-type="indexterm" data-primary="windows" id="idm46281561764184" target="_blank" rel="noopener noreferrer"></a> Windows allow us to group events into explicit time buckets, and can be used for creating more advanced joins and aggregations (which we first explored in the previous chapter).</p>
      <p>By the end of this chapter, you will understand the following:</p>
      <ul>
        <li>
          <p>The differences between event time, ingestion time, and processing time</p></li>
        <li>
          <p>How to build a custom timestamp extractor for associating events with a particular timestamp and time semantic</p></li>
        <li>
          <p>How time controls the flow of data through Kafka Streams</p></li>
        <li>
          <p>What types of windows are supported in Kafka Streams</p></li>
        <li>
          <p>How to perform windowed joins</p></li>
        <li>
          <p>How to perform windowed aggregations</p></li>
        <li>
          <p>What strategies are available for dealing with late and out-of-order events</p></li>
        <li>
          <p>How to use the <code>suppress</code> operator to process the final results of windows</p></li>
        <li>
          <p>How to query windowed key-value stores</p></li>
      </ul>
      <p>As with previous chapters, we will introduce these concepts through a tutorial. So without further ado, let’s take a look at the application we will be building in this chapter.</p>
      <section data-type="sect1" data-pdf-bookmark="Introducing Our Tutorial: Patient Monitoring Application">
        <div class="sect1" id="idm46281561752936">
          <h1>Introducing Our Tutorial: Patient Monitoring Application</h1>
          <p>Some of the most important use cases for time-centric stream processing are in the medical field. <a data-type="indexterm" data-primary="time" data-secondary="patient monitoring application tutorial" data-tertiary="introduction to" id="idm46281561751288" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="patient monitoring application tutorial" id="ix_PMA" target="_blank" rel="noopener noreferrer"></a>Patient monitoring systems are capable of producing hundreds of measurements per second, and processing/responding to this data quickly is important for treating certain types of medical conditions. This is why Children’s Healthcare of Atlanta uses Kafka Streams and ksqlDB to make real-time predictions about whether or not children with head trauma will need surgical intervention in the near future.<sup><a data-type="noteref" id="idm46281561748520-marker" href="ch05.html#idm46281561748520" target="_blank" rel="noopener noreferrer">1</a></sup></p>
          <p>Inspired by this use case, we will demonstrate several time-centric streaming concepts by building an application to monitor patient vitals. Instead of monitoring for head trauma, we will try to detect the presence of a medical condition called <em>systemic inflammatory response syndrome</em>, or SIRS. According to Bridgette Kadri, a physician’s assistant at the Medical University of South Carolina, there are several vital signs, including body temperature, blood pressure, and heart rate, that can be used as indicators of SIRS. In this tutorial, we will look at two of these measurements: body temperature and heart rate. When both of these vitals reach predefined thresholds (heart rate &gt;= 100 beats per minute, body temperature &gt;= 100.4°F), we will send a record to an <em>alerts</em> topic to notify the appropriate medical personnel.<sup><a data-type="noteref" id="idm46281561744872-marker" href="ch05.html#idm46281561744872" target="_blank" rel="noopener noreferrer">2</a></sup></p>
          <p>Let’s look at the architecture of our patient monitoring application. <a data-type="xref" href="#C5_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-1</a> shows the topology design we’ll be implementing in this chapter.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="topology of the application" id="idm46281561742920" target="_blank" rel="noopener noreferrer"></a> Additional information about each step is included after the diagram.</p>
          <figure>
            <div id="C5_TOPOLOGY" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0501.png" style="width: 30rem">
              <h6><span class="label">Figure 5-1. </span>The topology that we will be implementing for our patient monitoring <span class="keep-together">application</span></h6>
            </div>
          </figure>
          <dl class="calloutlist">
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12">
            </dt>
            <dd>
              <p>Our Kafka cluster contains two topics that capture patient vitals measurements:</p>
              <ul>
                <li>
                  <p>The <code>pulse-events</code> topic is populated by a heartbeat sensor. Every time the sensor picks up a patient’s heartbeat, it appends a record to this topic. Records are keyed by patient ID.</p></li>
                <li>
                  <p>The <code>body-temp-events</code> topic is populated by a wireless body temperature sensor. Every time the patient’s core body temperature is taken, a record is appended to this topic. These records are also keyed by patient ID.</p></li>
              </ul>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12">
            </dt>
            <dd>
              <p>In order to detect elevated heart rates, we need to convert the raw pulse events into a heart rate (measured using <em>beats per minute</em>, or bpm). As we learned in the previous chapter, we must first group the records to satisfy Kafka Streams’ prerequisite for performing aggregations.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12">
            </dt>
            <dd>
              <p>We will use a windowed aggregation to convert the pulse events into a heart rate. Since our unit of measurement is beats per minute, our window size will be 60 seconds.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12">
            </dt>
            <dd>
              <p>We will use the <code>suppress</code> operator to only emit the final computation of the bpm window. We’ll see why this is needed once we discuss this operator later in the chapter.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12">
            </dt>
            <dd>
              <p>In order to detect an infection, we will filter all vitals measurements that breach a set of predefined thresholds (heart rate &gt;= 100 beats per minute, body temperature &gt;= 100.4°F).</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12">
            </dt>
            <dd>
              <p>As we’ll see shortly, windowed aggregations change the record key. Therefore, we’ll need to rekey the heart rate records by patient ID to meet the <span class="keep-together">co-partitioning</span> requirements for joining records.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12">
            </dt>
            <dd>
              <p>We will perform a windowed join to combine the two vitals streams. Since we are performing the join <em>after</em> filtering for elevated bpm and body temperature measures, each joined record will indicate an alerting condition for SIRS.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/8.png" alt="8" width="12" height="12">
            </dt>
            <dd>
              <p>Finally, we will expose the results of our heart rate aggregation via interactive queries. We will also write the output of our joined stream to a topic called <code>alerts</code>.</p>
            </dd>
          </dl>
          <p>Let’s now quickly run through the project setup so you can follow along with this tutorial.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Project Setup">
        <div class="sect1" id="idm46281561719032">
          <h1>Project Setup</h1>
          <p>The code for this chapter is located at <a href="https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git</em></a>.<a data-type="indexterm" data-primary="time" data-secondary="patient monitoring application tutorial" data-tertiary="project setup" id="idm46281561716376" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="project setup" id="idm46281561715144" target="_blank" rel="noopener noreferrer"></a></p>
          <p>If you would like to reference the code as we work our way through each topology step, clone the repo and change to the directory containing this chapter’s tutorial. The following command will do the trick:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ git clone git@github.com:mitch-seymour/mastering-kafka-streams-and-ksqldb.git
$ cd mastering-kafka-streams-and-ksqldb/chapter-05/patient-monitoring</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>You can build the project anytime by running the following command:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ ./gradlew build --info</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Now that our project is set up, let’s start implementing our patient monitoring <span class="keep-together">application</span>.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Data Models">
        <div class="sect1" id="idm46281561710392">
          <h1>Data Models</h1>
          <p>As usual, we’ll start by defining our data models.<a data-type="indexterm" data-primary="time" data-secondary="patient monitoring application tutorial" data-tertiary="defining data models" id="idm46281561708792" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="data models" id="idm46281561707528" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="data models" data-secondary="for patient monitoring application tutorial" id="idm46281561706568" target="_blank" rel="noopener noreferrer"></a> Since each vitals measurement is associated with a timestamp, we will first create a simple interface for each data class to implement. This interface allows us to extract the timestamp from a given record in a consistent manner, and will come in handy when we implement a timestamp extractor later in this chapter. The following code block shows the interface each data class will implement:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">public interface Vital {
 public String getTimestamp();
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Here are the data classes we’ll be working with. Note: the accessor methods (including the interface method, <code>getTimestamp</code>) have been omitted for <span class="keep-together">brevity</span>:</p>
          <table class="border">
            <thead>
              <tr>
                <th>Kafka topic</th>
                <th>Example record</th>
                <th>Data class</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><span class=""><code>pulse-events</code></span></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
{
 "timestamp": "2020-11-05T09:02:00.000Z"
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
public class Pulse implements Vital {
 private String timestamp;
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
              </tr>
              <tr>
                <td><code>body-temp-events</code></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
{
 "timestamp": "2020-11-04T09:02:06.500Z",
 "temperature": 101.2,
 "unit": "F"
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
public class BodyTemp implements Vital {
 private String timestamp;
 private Double temperature;
 private String unit;
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>Now that we have a good understanding of what the source data looks like, we’re almost ready to register the input streams. However, this application requires special attention to time, and so far in this book, we haven’t put much thought into how records are associated with timestamps. So, before we register the input streams, let’s look at the various time semantics in Kafka Streams.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Time Semantics">
        <div class="sect1" id="idm46281561692424">
          <h1>Time Semantics</h1>
          <p>There are several different notions of time in Kafka Streams, and choosing the correct semantic is important when performing time-based operations, including windowed joins and aggregations.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="time semantics" id="ix_PMAtime" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="time" data-secondary="semantics in Kafka Streams" id="idm46281561689224" target="_blank" rel="noopener noreferrer"></a> In this section, we will take some time to understand the different notions of time in Kafka Streams, starting with some simple definitions:</p>
          <dl>
            <dt>
              Event time
            </dt>
            <dd>
              <p>When an event was created at the source. This timestamp can be embedded <span class="keep-together">in the payload</span> of an event, or set directly using the Kafka producer client as of <span class="keep-together">version</span> 0.10.0.<a data-type="indexterm" data-primary="event time" id="idm46281561684904" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              Ingestion time
            </dt>
            <dd>
              <p>When the event is appended to a topic on a Kafka broker. This always occurs after event time.<a data-type="indexterm" data-primary="ingestion time" id="idm46281561682776" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              Processing time
            </dt>
            <dd>
              <p>When the event is processed by your Kafka Streams application. <a data-type="indexterm" data-primary="processing time" id="idm46281561680792" target="_blank" rel="noopener noreferrer"></a>This always occurs after event time <em>and</em> ingestion time. It is less static than event time, and reprocessing the same data (i.e., for bug fixes) will lead to new processing timestamps, and therefore nondeterministic windowing behavior.</p>
            </dd>
          </dl>
          <p>To illustrate where these notions of time are physically manifested in an event stream, see <a data-type="xref" href="#C5_TIME_SEMANTICS" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-2</a>.</p>
          <figure>
            <div id="C5_TIME_SEMANTICS" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0502.png" style="width: 23rem">
              <h6><span class="label">Figure 5-2. </span>The different time semantics in Kafka Streams, as shown through a heartbeat sensor</h6>
            </div>
          </figure>
          <p>Event time is probably the most intuitive notion of time since it describes when the event actually occurred.<a data-type="indexterm" data-primary="event time" data-secondary="embedded event timestamp" id="idm46281561675368" target="_blank" rel="noopener noreferrer"></a> For example, if a heartbeat sensor records a pulse at 9:02 a.m., then the event time is 9:02 a.m.</p>
          <p class="pagebreak-before">The event time is typically embedded in the payload, as shown in the following code example:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">{
 "timestamp": "2020-11-12T09:02:00.000Z", <a class="co" id="co_windows_and_time_CO1-1" href="#callout_windows_and_time_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
 "sensor": "smart-pulse"
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_windows_and_time_CO1-1" href="#co_windows_and_time_CO1-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>An embedded event timestamp that must be extracted.</p>
            </dd>
          </dl>
          <p>Alternatively, Kafka producers allow the default timestamp that gets set for each record to be overridden, which can also be used to achieve event-time semantics. However, for systems that use this method for associating timestamps with events, it’s important to be aware of two Kafka configurations (one at the broker level and one at the topic level) to ensure you don’t accidentally end up with ingestion-time semantics.<a data-type="indexterm" data-primary="message.timestamp.type" id="idm46281561666680" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="log.message.timestamp.type" id="idm46281561665976" target="_blank" rel="noopener noreferrer"></a> The relevant configurations are:</p>
          <ul>
            <li>
              <p><code>log.message.timestamp.type</code> (broker level)</p></li>
            <li>
              <p><code>message.timestamp.type</code> (topic level)</p></li>
          </ul>
          <p>There are two possible values for these configs: <code>CreateTime</code> or <code>LogAppendTime</code>.<a data-type="indexterm" data-primary="LogAppendTime type" id="idm46281561660856" target="_blank" rel="noopener noreferrer"></a> Furthermore, the topic-level config takes precedence over the broker-level config. If the topic is configured with the <code>LogAppendTime</code> timestamp type,<sup><a data-type="noteref" id="idm46281561659464-marker" href="ch05.html#idm46281561659464" target="_blank" rel="noopener noreferrer">3</a></sup> the timestamp that the producer appends to the message will be overwritten with the local system time of the broker whenever a record is appended to the topic (therefore, you’ll be working with ingestion-time semantics, even if that wasn’t your intent). If you want to achieve event-time semantics and you’re relying on the producer timestamp, be sure that you are using <code>CreateTime</code> as the message timestamp type.<a data-type="indexterm" data-primary="CreateTime" id="idm46281561657816" target="_blank" rel="noopener noreferrer"></a></p>
          <p>The benefit<a data-type="indexterm" data-primary="event time" data-secondary="benefits of using" id="idm46281561656856" target="_blank" rel="noopener noreferrer"></a> of using event-time semantics is that this timestamp is more meaningful to the event itself, and is therefore more intuitive for users. Event time also allows time-dependent operations to be deterministic (e.g., when reprocessing data). This is not the case when using processing time.<a data-type="indexterm" data-primary="processing time" data-secondary="use case" id="idm46281561655416" target="_blank" rel="noopener noreferrer"></a> Processing time is usually used if you aren’t leveraging time-based operations, if the time at which the event is processed is more meaningful to the semantics of the application than when the event initially occurred, or if you can’t associate an event with a timestamp for some reason. Interestingly enough, the latter issue, which occurs when an event time can’t be associated with a record, is sometimes addressed by using ingestion time.<a data-type="indexterm" data-primary="ingestion time" data-secondary="using to approximate event time" id="idm46281561653880" target="_blank" rel="noopener noreferrer"></a> In systems where there isn’t a lot of lag between the time an event is created and when the event is subsequently appended to a topic, ingestion time can be used to approximate event time, so it can be a viable alternative when event time can’t be used.<sup><a data-type="noteref" id="idm46281561652520-marker" href="ch05.html#idm46281561652520" target="_blank" rel="noopener noreferrer">4</a></sup></p>
          <p>Now that we’ve learned about the different notions of time in Kafka Streams, how do we actually leverage the time semantic of our choice? We’ll learn about this in the next section.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="time semantics" data-startref="ix_PMAtime" id="idm46281561650680" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Timestamp Extractors">
        <div class="sect1" id="C5_TS_EXTRACTORS">
          <h1>Timestamp Extractors</h1>
          <p>In Kafka Streams, <em>timestamp extractors</em> are responsible <a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="timestamp extractors" id="ix_PMAtstmp" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="timestamp extractors" id="ix_TmExt" target="_blank" rel="noopener noreferrer"></a>for associating a given record with a timestamp, and these timestamps are used in time-dependent operations like windowed joins and windowed aggregations.<a data-type="indexterm" data-primary="TimestampExtractor interface" id="idm46281561644616" target="_blank" rel="noopener noreferrer"></a> Each timestamp extractor implementation must adhere to the following interface:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">public interface TimestampExtractor {
   long extract(
       ConsumerRecord&lt;Object, Object&gt; record, <a class="co" id="co_windows_and_time_CO2-1" href="#callout_windows_and_time_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
       long partitionTime <a class="co" id="co_windows_and_time_CO2-2" href="#callout_windows_and_time_CO2-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
   );
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_windows_and_time_CO2-1" href="#co_windows_and_time_CO2-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The current consumer record being processed.</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO2-2" href="#co_windows_and_time_CO2-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Kafka Streams keeps track of the most recent timestamp it has seen for each partition it consumes from, and passes this timestamp to the <code>extract</code> method using the <code>partitionTime</code> parameter.<a data-type="indexterm" data-primary="partitionTime parameter" id="idm46281561632792" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
          </dl>
          <p>The second parameter, <code>partitionTime</code>, is particularly interesting since it can be used as a fallback if a timestamp cannot be extracted. We’ll dig into that shortly, but first, let’s look at the timestamp extractors that are included in Kafka Streams.</p>
          <section data-type="sect2" data-pdf-bookmark="Included Timestamp Extractors">
            <div class="sect2" id="idm46281561630760">
              <h2>Included Timestamp Extractors</h2>
              <p><code>FailOnInvalidTimestamp</code>, which is the default <a data-type="indexterm" data-primary="timestamp extractors" data-secondary="built into Kafka Streams" id="idm46281561628552" target="_blank" rel="noopener noreferrer"></a>timestamp extractor in Kafka Streams, extracts the timestamp from the consumer record, which is either the event time (when <code>message.timestamp.type</code> is set to <code>CreateTime</code>) or ingestion time (when <code>message.timestamp.type</code> is set to <code>LogAppendTime</code>).<a data-type="indexterm" data-primary="FailOnInvalidTimestamp extractor" id="idm46281561625560" target="_blank" rel="noopener noreferrer"></a> This extractor will throw a <code>StreamsException</code> if the timestamp is invalid. A timestamp is considered invalid if it is negative (which can happen if the record was produced using a message format older than 0.10.0). At the time of this writing, it’s been over four years since version 0.10.0 was released, so negative/invalid timestamps are becoming more of a corner case at this point.</p>
              <p>The <code>LogAndSkipOnInvalidTimestamp</code> extractor can also be used to achieve event-time semantics, but unlike the <code>FailOnInvalidTimestamp</code> extractor, it simply logs a warning when an invalid timestamp is encountered.<a data-type="indexterm" data-primary="LogAndSkipOnInvalidTimestamp extractor" id="idm46281561622776" target="_blank" rel="noopener noreferrer"></a> This will result in the record being skipped, allowing Kafka Streams to continue processing when it encounters invalid timestamps.</p>
              <p>There is another built-in extractor that we can use if we want processing-time semantics. <a data-type="indexterm" data-primary="WallclockTimestampExtractor" id="idm46281561621240" target="_blank" rel="noopener noreferrer"></a>As you can see in the code that follows, the <span class="keep-together"><code>WallclockTimestampExtractor</code></span> simply returns the local system time of your stream processing application:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">public class WallclockTimestampExtractor implements TimestampExtractor {

   @Override
   public long extract(
     final ConsumerRecord&lt;Object, Object&gt; record,
     final long partitionTime
   ) {
       return System.currentTimeMillis(); <a class="co" id="co_windows_and_time_CO3-1" href="#callout_windows_and_time_CO3-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
   }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_windows_and_time_CO3-1" href="#co_windows_and_time_CO3-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>WallclockTimestampExtractor</code>, which is one of the included timestamp extractors in Kafka Streams, simply returns the current system time.</p>
                </dd>
              </dl>
              <p>Regardless of <a data-type="indexterm" data-primary="DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG property" id="idm46281561612648" target="_blank" rel="noopener noreferrer"></a>which timestamp extractor you use, you can override the default <span class="keep-together">timestamp</span> extractor by setting the <code>DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG</code> property as shown in <a data-type="xref" href="#C5_TS_OVERRIDE" target="_blank" rel="noopener noreferrer">Example&nbsp;5-1</a>.</p>
              <div id="C5_TS_OVERRIDE" data-type="example">
                <h5><span class="label">Example 5-1. </span>An example of how to override the default timestamp extractor in Kafka Streams</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">Properties props = new Properties();
props.put( <a class="co" id="co_windows_and_time_CO4-1" href="#callout_windows_and_time_CO4-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG,
  WallclockTimestampExtractor.class
);

// ... other configs

KafkaStreams streams = new KafkaStreams(builder.build(), props);</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_windows_and_time_CO4-1" href="#co_windows_and_time_CO4-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Override the default timestamp extractor.</p>
                </dd>
              </dl>
              <p>When your Kafka Streams application leverages windows, as our patient monitoring application does, using processing-time semantics can have unintended side effects. For example, we want to capture the number of heartbeats within a one-minute window. If we use processing-time semantics (e.g., via <span class="keep-together"><code>WallclockTimestampExtractor</code></span>) for our windowed aggregation, then our window boundaries won’t represent the pulse time at all, but will instead represent the time our Kafka Streams application observed the pulse event. If our application experiences even a few seconds of lag, an event may fall outside of the intended window and therefore could impact our expectations in certain ways (i.e., our ability to detect an elevated heart rate).</p>
              <div data-type="note" epub:type="note">
                <h6>Note</h6>
                <p>When a timestamp is extracted and subsequently associated with a record, the record is said to be <em>stamped</em>.<a data-type="indexterm" data-primary="stamped records" id="idm46281561598984" target="_blank" rel="noopener noreferrer"></a></p>
              </div>
              <p>After reviewing the built-in timestamp extractors, it’s clear that we need a custom timestamp extractor since the event time is embedded in the payloads of each of our vitals data (pulse and body temperature events). We’ll discuss how to build a custom timestamp extractor in the next section.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Custom Timestamp Extractors">
            <div class="sect2" id="idm46281561630136">
              <h2>Custom Timestamp Extractors</h2>
              <p>It’s very common to build a custom timestamp extractor whenever you need event-time semantics and the event timestamp is embedded in the record payload.<a data-type="indexterm" data-primary="timestamp extractors" data-secondary="custom" id="idm46281561595736" target="_blank" rel="noopener noreferrer"></a> The <span class="keep-together">following</span> code block shows a custom timestamp extractor that we will use to extract the timestamp of a patient vitals measurement.<a data-type="indexterm" data-primary="TimestampExtractor interface" id="idm46281561593800" target="_blank" rel="noopener noreferrer"></a> As mentioned before, our custom timestamp extractor implements the <code>TimestampExtractor</code> interface included in Kafka Streams:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">public class VitalTimestampExtractor implements TimestampExtractor {

 @Override
 public long extract(ConsumerRecord&lt;Object, Object&gt; record, long partitionTime) {
   Vital measurement = (Vital) record.value(); <a class="co" id="co_windows_and_time_CO5-1" href="#callout_windows_and_time_CO5-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
   if (measurement != null &amp;&amp; measurement.getTimestamp() != null) { <a class="co" id="co_windows_and_time_CO5-2" href="#callout_windows_and_time_CO5-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
     String timestamp = measurement.getTimestamp(); <a class="co" id="co_windows_and_time_CO5-3" href="#callout_windows_and_time_CO5-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
     return Instant.parse(timestamp).toEpochMilli(); <a class="co" id="co_windows_and_time_CO5-4" href="#callout_windows_and_time_CO5-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
   }
   return partitionTime; <a class="co" id="co_windows_and_time_CO5-5" href="#callout_windows_and_time_CO5-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
 }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_windows_and_time_CO5-1" href="#co_windows_and_time_CO5-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Cast the record value to a <code>Vital</code> object. This is where our interface comes in handy, since it allows us to extract the timestamp from <code>Pulse</code> and <code>BodyTemp</code> records in a consistent way.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO5-2" href="#co_windows_and_time_CO5-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Make sure the record contains a timestamp before we attempt to parse it.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO5-3" href="#co_windows_and_time_CO5-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Extract the timestamp from the record.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO5-4" href="#co_windows_and_time_CO5-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>TimestampExtractor.extract</code> method expects us to return the record’s timestamp in milliseconds. So we perform the timestamp-to-milliseconds conversion here.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO5-5" href="#co_windows_and_time_CO5-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>If we cannot extract a timestamp for some reason, we can fall back to the partition time in order to approximate when the event occurred.</p>
                </dd>
              </dl>
              <p>One thing you should consider when using timestamp extractors is how records without valid timestamps should be handled. The three most common options are to:</p>
              <ul>
                <li>
                  <p>Throw an exception and stop processing (giving the developers an opportunity to resolve the bug)</p></li>
                <li>
                  <p>Fallback to the partition time</p></li>
                <li>
                  <p>Return a negative timestamp, which will allow Kafka Streams to skip over the record and continue processing</p></li>
              </ul>
              <p>In our implementation of the <code>VitalTimestampExtractor</code> we’ve decided to fallback to the partition time, which will resolve to the highest timestamp that has already been observed for the current partition.<a data-type="indexterm" data-primary="partitions" data-secondary="partition time for timestamp extractor" id="idm46281561562072" target="_blank" rel="noopener noreferrer"></a></p>
              <p>Now that we have created our timestamp extractor, let’s register our input streams.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Registering Streams with a Timestamp Extractor">
            <div class="sect2" id="idm46281561560520">
              <h2>Registering Streams with a Timestamp Extractor</h2>
              <p>Registering a set of input streams should be familiar by now, but this time, we’ll pass <a data-type="indexterm" data-primary="timestamp extractors" data-secondary="registering streams with" id="idm46281561558648" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="registration" data-secondary="registering streams with timestamp extractor" id="idm46281561557704" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="registering with timestamp extractor" id="idm46281561556664" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Consumed helper class" id="idm46281561555752" target="_blank" rel="noopener noreferrer"></a>in an extra <code>Consumed</code> parameter that will explicitly set the timestamp extractor to <span class="keep-together">our custom</span> timestamp extractor implementation (<code>VitalTimestampExtractor</code>). <a data-type="xref" href="#C5_REGISTER_STREAMS" target="_blank" rel="noopener noreferrer">Example&nbsp;5-2</a> shows how to register our two source streams using a custom timestamp extractor, which is the first step of our processor topology (see <a data-type="xref" href="#C5_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-1</a>).</p>
              <div id="C5_REGISTER_STREAMS" data-type="example">
                <h5><span class="label">Example 5-2. </span>An example of how to override the timestamp extractor for source streams</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">StreamsBuilder builder = new StreamsBuilder(); <a class="co" id="co_windows_and_time_CO6-1" href="#callout_windows_and_time_CO6-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

Consumed&lt;String, Pulse&gt; pulseConsumerOptions =
   Consumed.with(Serdes.String(), JsonSerdes.Pulse())
       .withTimestampExtractor(new VitalTimestampExtractor()); <a class="co" id="co_windows_and_time_CO6-2" href="#callout_windows_and_time_CO6-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

KStream&lt;String, Pulse&gt; pulseEvents =
    builder.stream("pulse-events", pulseConsumerOptions); <a class="co" id="co_windows_and_time_CO6-3" href="#callout_windows_and_time_CO6-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>

Consumed&lt;String, BodyTemp&gt; bodyTempConsumerOptions =
   Consumed.with(Serdes.String(), JsonSerdes.BodyTemp())
       .withTimestampExtractor(new VitalTimestampExtractor()); <a class="co" id="co_windows_and_time_CO6-4" href="#callout_windows_and_time_CO6-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>

KStream&lt;String, BodyTemp&gt; tempEvents =
        builder.stream("body-temp-events", bodyTempConsumerOptions); <a class="co" id="co_windows_and_time_CO6-5" href="#callout_windows_and_time_CO6-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a></pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_windows_and_time_CO6-1" href="#co_windows_and_time_CO6-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>As always with the DSL, use a <code>StreamsBuilder</code> to construct our processor <span class="keep-together">topology</span>.<a data-type="indexterm" data-primary="StreamsBuilder class" id="idm46281561535656" target="_blank" rel="noopener noreferrer"></a></p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO6-2" href="#co_windows_and_time_CO6-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Use <code>Consumed.withTimestampExtractor</code> to tell Kafka Streams to use our custom timestamp extractor (<code>VitalTimestampExtractor</code>) for extracting vitals timestamps.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO6-3" href="#co_windows_and_time_CO6-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Register the stream for capturing pulse events.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO6-4" href="#co_windows_and_time_CO6-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Use our custom timestamp extractor in the body temperature options as well.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO6-5" href="#co_windows_and_time_CO6-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Register the stream for capturing body temperature events.</p>
                </dd>
              </dl>
              <p>Alternatively, we could override the default timestamp extractor using the method shown in <a data-type="xref" href="#C5_TS_OVERRIDE" target="_blank" rel="noopener noreferrer">Example&nbsp;5-1</a>. Either method is fine, but for now, we’ll stick with setting the extractor for each input stream directly. Now that we’ve registered our source streams, let’s move on to the second and third steps of our processor topology (see <a data-type="xref" href="#C5_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-1</a>): grouping and windowing the <code>pulse-events</code> stream.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="timestamp extractors" data-startref="ix_PMAtstmp" id="idm46281561521352" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="timestamp extractors" data-startref="ix_TmExt" id="idm46281561520008" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Windowing Streams">
        <div class="sect1" id="idm46281561518936">
          <h1>Windowing Streams</h1>
          <p>The <code>pulse-events</code> topic receives data whenever a patient’s heartbeat is recorded.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="windowing streams" id="ix_PMAwinstr" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="windowing" id="ix_strmwin" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" id="ix_winstr" target="_blank" rel="noopener noreferrer"></a> However, for our purposes, we’re interested in the patient’s heart rate, which is measured by the number of beats per minute (bpm). We know that the <code>count</code> operator can be used to count the number of heartbeats, but we need some way of only counting the records that fall within each 60-second window. This is where <em>windowed aggregations</em> come into play. Windowing is a method for grouping records into different time-based subgroups for the purpose of aggregating and joining. Kafka Streams supports a few different types of windows, so let’s take a look at each type to determine which implementation we will need for our patient monitoring system.</p>
          <section data-type="sect2" data-pdf-bookmark="Window Types">
            <div class="sect2" id="C5_WINDOW_TYPES">
              <h2>Window Types</h2>
              <p>Windows are used to group records with close <em>temporal proximity</em>. <a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-tertiary="window types" id="idm46281561509208" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="temporal proximity, windowing records by" id="idm46281561507928" target="_blank" rel="noopener noreferrer"></a>Temporal proximity can mean different things depending on which time semantic you are using. For example, when using event-time semantics, it can mean “records that <em>occurred</em> around the same time,” whereas with processing-time semantics, it means “records that were <em>processed</em> around the same time.” For most cases, “around the same time” is defined by the window size (e.g., five minutes, one hour, etc.), though session windows (which we’ll discuss shortly) utilize an activity period.</p>
              <p>There are four different types of windows in Kafka Streams. We will discuss the characteristics of each window type next, and then use these properties to create a <span class="keep-together">decision</span> tree for deciding which window to use in our tutorial (you can also use the resulting decision tree in your own applications).</p>
              <section data-type="sect3" data-pdf-bookmark="Tumbling windows">
                <div class="sect3" id="idm46281561504216">
                  <h3>Tumbling windows</h3>
                  <p>Tumbling windows are fixed-sized windows that never overlap. <a data-type="indexterm" data-primary="tumbling windows" id="idm46281561502648" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-tertiary="tumbling windows" id="idm46281561501944" target="_blank" rel="noopener noreferrer"></a>They are defined using a single property, the <em>window size</em> (in milliseconds), and have predictable time ranges since they are aligned with the epoch.<sup><a data-type="noteref" id="idm46281561500104-marker" href="ch05.html#idm46281561500104" target="_blank" rel="noopener noreferrer">5</a></sup> The following code shows how to create a tumbling window in Kafka Streams:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">TimeWindows tumblingWindow =
  TimeWindows.of(Duration.ofSeconds(5)); <a class="co" id="co_windows_and_time_CO7-1" href="#callout_windows_and_time_CO7-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO7-1" href="#co_windows_and_time_CO7-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The size of the window is five seconds.</p>
                    </dd>
                  </dl>
                  <p>As you can see in <a data-type="xref" href="#C5_TUMBLING_WINDOW_IMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-3</a>, a tumbling window is very simple to reason about, visually. You don’t have to worry about overlapping window boundaries or records appearing in more than one time bucket.</p>
                  <figure>
                    <div id="C5_TUMBLING_WINDOW_IMG" class="figure">
                      <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0503.png" style="width: 33rem">
                      <h6><span class="label">Figure 5-3. </span>Tumbling window</h6>
                    </div>
                  </figure>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Hopping windows">
                <div class="sect3" id="idm46281561489352">
                  <h3>Hopping windows</h3>
                  <p>Hopping windows are fixed-sized windows<a data-type="indexterm" data-primary="hopping windows" id="idm46281561487688" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-tertiary="hopping windows" id="idm46281561486984" target="_blank" rel="noopener noreferrer"></a> that may overlap.<sup><a data-type="noteref" id="idm46281561485640-marker" href="ch05.html#idm46281561485640" target="_blank" rel="noopener noreferrer">6</a></sup> When configuring a hopping window, you must specify both the <em>window size</em> and the <em>advance interval</em> (how much the window moves forward). When the advance interval is less than the window size, as shown in <a data-type="xref" href="#C5_HOPPING_WINDOW_IMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-4</a>, then windows will overlap, allowing some records to appear in multiple windows. Furthermore, hopping windows have predictable time ranges since they are aligned with the epoch, and the start time is inclusive while the end time is exclusive. The following code depicts how to create a simple hopping window with Kafka Streams:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">TimeWindows hoppingWindow =  TimeWindows
 .of(Duration.ofSeconds(5))  <a class="co" id="co_windows_and_time_CO8-1" href="#callout_windows_and_time_CO8-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
 .advanceBy(Duration.ofSeconds(4));  <a class="co" id="co_windows_and_time_CO8-2" href="#callout_windows_and_time_CO8-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO8-1" href="#co_windows_and_time_CO8-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The size of the window is five seconds.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO8-2" href="#co_windows_and_time_CO8-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The window has a four-second advance interval (or <em>hop</em>).</p>
                    </dd>
                  </dl>
                  <p>A visualization of this hopping window is shown in <a data-type="xref" href="#C5_HOPPING_WINDOW_IMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-4</a>.</p>
                  <figure>
                    <div id="C5_HOPPING_WINDOW_IMG" class="figure">
                      <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0504.png" style="width: 33rem">
                      <h6><span class="label">Figure 5-4. </span>Hopping window</h6>
                    </div>
                  </figure>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Session windows">
                <div class="sect3" id="idm46281561468856">
                  <h3>Session windows</h3>
                  <p>Session windows are variable-sized windows that are determined by periods of activity followed by gaps of inactivity.<a data-type="indexterm" data-primary="session windows" id="idm46281561467128" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-tertiary="session windows" id="idm46281561466424" target="_blank" rel="noopener noreferrer"></a> A single parameter called the <em>inactivity gap</em> is used to define a session window. If the inactivity gap is five seconds, then each record that has a timestamp within five seconds of the previous record with the same key will be merged into the same window. Otherwise, if the timestamp of a new record is greater than the inactivity gap (in this case, five seconds), then a new window will be created. Unlike tumbling and hopping windows, both the lower and upper boundaries are inclusive. The following code snippet shows how to define a session window:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">SessionWindows sessionWindow = SessionWindows
 .with(Duration.ofSeconds(5)); <a class="co" id="co_windows_and_time_CO9-1" href="#callout_windows_and_time_CO9-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO9-1" href="#co_windows_and_time_CO9-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>A session window with an inactivity gap of five seconds.</p>
                    </dd>
                  </dl>
                  <p>As you can see in <a data-type="xref" href="#C5_SESSION_WINDOW_IMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-5</a>, session windows are unaligned (the range is specific to each key) and variable in length. The range is completely dependent on the record timestamps, with hot keys leading to long window ranges, and less active keys leading to shorter windows.</p>
                  <figure>
                    <div id="C5_SESSION_WINDOW_IMG" class="figure">
                      <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0505.png" style="width: 32rem">
                      <h6><span class="label">Figure 5-5. </span>Session window</h6>
                    </div>
                  </figure>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Sliding join windows">
                <div class="sect3" id="C5_JOIN_WINDOW_SECTION">
                  <h3>Sliding join windows</h3>
                  <p>Sliding join windows are fixed-sized windows that are used for joins and created using the <code>JoinWindows</code> class. <a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="sliding join windows" id="idm46281561452712" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="JoinWindows class" id="idm46281561451432" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="sliding join windows" id="idm46281561450760" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-tertiary="sliding join windows" id="idm46281561450088" target="_blank" rel="noopener noreferrer"></a>Two records fall within the same window if the difference between their timestamps is less than or equal to the window size. Thus, similar to session windows, both the lower and upper window boundaries are inclusive. Here is an example of how to create a join window with a duration of five seconds:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">JoinWindows joinWindow = JoinWindows
 .of(Duration.ofSeconds(5)); <a class="co" id="co_windows_and_time_CO10-1" href="#callout_windows_and_time_CO10-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO10-1" href="#co_windows_and_time_CO10-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Timestamps must be less than or equal to five seconds apart to fall within the same window.</p>
                    </dd>
                  </dl>
                  <p>Visually, join windows look a little different than other types of windows since their use in joins allows them to span multiple input streams. <a data-type="xref" href="#C5_JOIN_WINDOW_IMG" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-6</a> illustrates this, and shows how a five-second window is used to determine which records are combined in a windowed join.</p>
                  <figure>
                    <div id="C5_JOIN_WINDOW_IMG" class="figure">
                      <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0506.png" style="width: 35rem">
                      <h6><span class="label">Figure 5-6. </span>Join window</h6>
                    </div>
                  </figure>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Sliding aggregation windows">
                <div class="sect3" id="idm46281561439112">
                  <h3>Sliding aggregation windows</h3>
                  <p>In the previous section, we discussed a special type of sliding window that is used for joins. Since Kafka Streams 2.7.0, sliding windows can also be used with aggregations.<a data-type="indexterm" data-primary="sliding aggregation windows" id="idm46281561437592" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="aggregations" data-secondary="using sliding aggregation windows" id="idm46281561436872" target="_blank" rel="noopener noreferrer"></a> Like sliding join windows, the window boundaries in a sliding aggregation window are aligned to the record timestamps (as opposed to the epoch) and the lower <em>and</em> upper window boundaries are both inclusive. Additionally, records will fall within the same window if the difference between their timestamps is within the specified window size. Here is an example of how to create a sliding window with a duration of five seconds and a grace period of zero seconds:<sup><a data-type="noteref" id="idm46281561434904-marker" href="ch05.html#idm46281561434904" target="_blank" rel="noopener noreferrer">7</a></sup></p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">SlidingWindows slidingWindow =
 SlidingWindows.withTimeDifferenceAndGrace(
   Duration.ofSeconds(5),
   Duration.ofSeconds(0));</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                </div>
              </section>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Selecting a Window">
            <div class="sect2" id="idm46281561433112">
              <h2>Selecting a Window</h2>
              <p>Now that we’ve learned which window types are supported in Kafka Streams, we need to decide which type is appropriate for converting raw pulse events into a heart rate using a windowed aggregation.<a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-tertiary="selecting a window" id="idm46281561431576" target="_blank" rel="noopener noreferrer"></a></p>
              <p>Session windows aren’t a good fit for measuring the heart rate, because the window size could extend indefinitely as long as there is activity on the stream. This doesn’t meet our requirement of having fixed-size windows of 60 seconds. Furthermore, a sliding join window is used for joins only, so we can rule that choice out, as well (though we will use a sliding join window later in this tutorial).</p>
              <p>You could arguably use any of the other window types for our aggregation, but to keep things simple, let’s align our window boundaries with the epoch (which rules out sliding aggregation windows) and avoid overlapping windows (which means we don’t need a hopping window). This leaves us with a tumbling window for our heart rate aggregation. With our window type selected, we are ready to perform a windowed aggregation.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Windowed Aggregation">
            <div class="sect2" id="idm46281561428408">
              <h2>Windowed Aggregation</h2>
              <p>Since we’ve decided to use a tumbling window for the heart rate aggregation, we can build our windowed aggregation. First, we’ll construct the window using the <span class="keep-together"><code>TimeWindows.of</code></span> method, and then we’ll window our stream with the <code>windowedBy</code> operator before performing the aggregation.<a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-tertiary="windowed aggregation" id="idm46281561425208" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="aggregations" data-secondary="windowed" id="idm46281561423960" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windowedBy operator" id="idm46281561423016" target="_blank" rel="noopener noreferrer"></a> The following code shows how to accomplish both of these steps:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">TimeWindows tumblingWindow =
 TimeWindows.of(Duration.ofSeconds(60));

KTable&lt;Windowed&lt;String&gt;, Long&gt; pulseCounts =
 pulseEvents
     .groupByKey() <a class="co" id="co_windows_and_time_CO11-1" href="#callout_windows_and_time_CO11-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
     .windowedBy(tumblingWindow) <a class="co" id="co_windows_and_time_CO11-2" href="#callout_windows_and_time_CO11-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
     .count(Materialized.as("pulse-counts")); <a class="co" id="co_windows_and_time_CO11-3" href="#callout_windows_and_time_CO11-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>

pulseCounts
 .toStream() <a class="co" id="co_windows_and_time_CO11-4" href="#callout_windows_and_time_CO11-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
 .print(Printed.&lt;Windowed&lt;String&gt;, Long&gt;toSysOut().withLabel("pulse-counts")); <a class="co" id="co_windows_and_time_CO11-5" href="#callout_windows_and_time_CO11-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_windows_and_time_CO11-1" href="#co_windows_and_time_CO11-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Grouping records is a prerequisite for performing an aggregation. However, the records in the <code>pulse-events</code> topic are already keyed using the desired scheme (i.e., by patient ID), so we can use the <code>groupByKey</code> operator instead of <code>groupBy</code> to avoid an unnecessary repartition of our data.<a data-type="indexterm" data-primary="groupByKey operator" id="idm46281561407352" target="_blank" rel="noopener noreferrer"></a></p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO11-2" href="#co_windows_and_time_CO11-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Window the stream using a 60-second tumbling window. This will allow us to turn the raw pulse events into a heart rate (measured by beats per minute).</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO11-3" href="#co_windows_and_time_CO11-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Materialize the heart rate for interactive queries (this will be needed by step 8 in our processor topology—see <a data-type="xref" href="#C5_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-1</a>.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO11-4" href="#co_windows_and_time_CO11-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>For debugging purposes only, convert the <code>KTable</code> to a stream so we can print the contents to the console.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_windows_and_time_CO11-5" href="#co_windows_and_time_CO11-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Print the contents of our windowed stream to the console. Print statements are useful for local development purposes, but should be removed before deploying our application to production.<a data-type="indexterm" data-primary="keys" data-secondary="multidimensional keys for windowed KTables" id="idm46281561394872" target="_blank" rel="noopener noreferrer"></a></p>
                </dd>
              </dl>
              <p>One interesting thing to highlight in this code example is that the key of the <code>KTable</code> changed from <code>String</code> to <code>Windowed&lt;String&gt;</code>. This is because the <code>windowedBy</code> operator converts <code>KTable</code>s into <em>windowed</em> <em><code>KTable</code>s</em>, which have multidimensional keys that contain not only the original record key, but also the time range of the window. This makes sense because we need some way of grouping keys into subgroups (windows), and simply using the original key would cause all of the pulse events to be included in the same subgroup. We can see this multidimensional key transformation in action by producing some records to the <code>pulse-events</code> topic and viewing the printed output of the windowed stream. The records we will produce are shown here (the record key and value are demarcated by the <em>|</em> character):</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">1|{"timestamp": "2020-11-12T09:02:00.000Z"}
1|{"timestamp": "2020-11-12T09:02:00.500Z"}
1|{"timestamp": "2020-11-12T09:02:01.000Z"}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>Producing these records leads to the output in <a data-type="xref" href="#C5_WINDOW_OUTPUT" target="_blank" rel="noopener noreferrer">Example&nbsp;5-3</a>. Note that the old key for each of these records, <code>1</code>, has been changed to the following format:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">[&lt;oldkey&gt;@&lt;window_start_ms&gt;/&lt;window_end_ms&gt;]</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <div id="C5_WINDOW_OUTPUT" data-type="example">
                <h5><span class="label">Example 5-3. </span>The output of the <code>print</code> operator, which shows the multidimensional keys in our windowed <code>pulseCounts</code> table</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">[pulse-counts]: [1@1605171720000/1605171780000], 1 <a class="co" id="co_windows_and_time_CO12-1" href="#callout_windows_and_time_CO12-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
[pulse-counts]: [1@1605171720000/1605171780000], 2
[pulse-counts]: [1@1605171720000/1605171780000], 3</pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_windows_and_time_CO12-1" href="#co_windows_and_time_CO12-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The multidimensional record key, which contains not only the original key, but also the window boundaries.</p>
                </dd>
              </dl>
              <p>Apart from the key transformation logic, the preceding output highlights a peculiar behavior of Kafka Streams. The heart rate count, which we compute in our windowed aggregation, gets updated each time a new heartbeat is recorded. We can see this in <a data-type="xref" href="#C5_WINDOW_OUTPUT" target="_blank" rel="noopener noreferrer">Example&nbsp;5-3</a>, since the first heart rate that is emitted is <code>1</code>, followed by <code>2</code>, <code>3</code>, etc. Therefore, downstream operators will see not just the final results of a window (the beats per minute), but also the intermediate results of the window (the number of <em>heartbeats</em> in this 60-second window <em>so far</em>). In applications that are highly tuned for low latency, these intermediate or incomplete window results are useful. However, in our case, we can’t truly know the patient’s heart rate until a window has been closed, so it is misleading at best. Let’s look at why Kafka Streams emits each intermediate result and see if we can tune this behavior.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="windowing streams" data-startref="ix_PMAwinstr" id="idm46281561372680" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="streams" data-secondary="windowing" data-startref="ix_strmwin" id="idm46281561371416" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="windowing streams" data-startref="ix_winstr" id="idm46281561370200" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Emitting Window Results">
        <div class="sect1" id="idm46281561427784">
          <h1>Emitting Window Results</h1>
          <p>The decision of <em>when</em> to emit a window’s computation is a surprisingly complex one for stream processing systems.<a data-type="indexterm" data-primary="windows" data-secondary="emitting results of" id="ix_winres" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="emitting window results" id="ix_PMAwinres" target="_blank" rel="noopener noreferrer"></a> The complexity is caused by two facts:</p>
          <ul>
            <li>
              <p>Unbounded event streams may not always be in <em>timestamp order</em>, especially when using event-time semantics.<sup><a data-type="noteref" id="idm46281561362696-marker" href="ch05.html#idm46281561362696" target="_blank" rel="noopener noreferrer">8</a></sup></p>
              <div data-type="note" epub:type="note">
                <h6>Note</h6>
                <p>Kafka does guarantee events will always be in <em>offset order</em> at the partition level.<a data-type="indexterm" data-primary="offset order of events at partition level" id="idm46281561358648" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="partitions" data-secondary="offset order of events at partition level" id="idm46281561357848" target="_blank" rel="noopener noreferrer"></a> This means that every consumer will always read the events in the same sequence that they were appended to the topic (by ascending offset value).</p>
              </div></li>
            <li>
              <p>Events are sometimes <em>delayed</em>.</p></li>
          </ul>
          <p>The lack of timestamp order means <a data-type="indexterm" data-primary="delayed events" id="idm46281561354424" target="_blank" rel="noopener noreferrer"></a>that we can’t just assume that if we’ve seen a record containing a certain timestamp, we’ve seen all records that should have arrived before that timestamp, and therefore can emit what we think is the final window result.<a data-type="indexterm" data-primary="late-arriving data" id="idm46281561353352" target="_blank" rel="noopener noreferrer"></a> Furthermore, delayed and out-of-order data requires us to make a choice: do we wait a certain amount of time for all of the data to arrive, or do we output the window result whenever it is updated (as seen in <a data-type="xref" href="#C5_WINDOW_OUTPUT" target="_blank" rel="noopener noreferrer">Example&nbsp;5-3</a>)? This is a trade-off between completeness and latency. Since waiting for data is more likely to produce a complete result, this approach optimizes for completeness. On the other hand, propagating updates downstream immediately (even though they may be incomplete) reduces latency. It’s up to you and potentially any service-level agreements (SLAs) you’ve established to decide what to optimize for.</p>
          <p><a data-type="xref" href="#C5_OOO" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-7</a> demonstrates how both of these issues could occur. Patient #1 is attached to a vitals monitoring machine that is having intermittent networking issues. This leads to producer retries, causing some vitals measurements to have a delayed arrival into our Kafka cluster. Furthermore, since we have multiple producers writing to the <code>pulse-events</code> topic (one for each vitals monitoring machine), this also causes some events to be unordered with respect to their event timestamp.</p>
          <figure>
            <div id="C5_OOO" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0508.png" style="width: 26rem">
              <h6><span class="label">Figure 5-7. </span>Data can arrive out of timestamp order for many reasons; one common example is a topic having multiple producers, which introduces a race condition</h6>
            </div>
          </figure>
          <p>One important thing to note is that there doesn’t need to be some failure scenario to result in out-of-order data. This could happen even in normal operating conditions, such as when multiple producers are writing to the same topic.</p>
          <p>As mentioned previously, to overcome these issues, we need to optimize for either latency or completeness. By default, Kafka Streams optimizes for latency, using an approach called <a href="https://oreil.ly/-tii3" target="_blank" rel="noopener noreferrer"><em>continuous refinement</em></a>. Continuous refinement means that whenever a new event is added to the window, Kafka Streams will emit the new computation immediately. This is why we saw each intermediate window result when we produced some records to our patient monitoring application (see <a data-type="xref" href="#C5_WINDOW_OUTPUT" target="_blank" rel="noopener noreferrer">Example&nbsp;5-3</a>). However, with continuous refinement, each result should be seen as potentially incomplete, and an emitted event does <em>not</em> mean we have processed every record that will eventually fall into the window. Furthermore, delayed data can continue causing events to be emitted at unexpected times.</p>
          <p>The next two sections discuss how to address each of these issues in our patient monitoring application. First, let’s look at a strategy for handling delayed data in Kafka Streams. After that, we will learn how to suppress intermediate window calculations using Kafka Streams’ <code>suppress</code> operator.</p>
          <section data-type="sect2" data-pdf-bookmark="Grace Period">
            <div class="sect2" id="idm46281561341832">
              <h2>Grace Period</h2>
              <p>One of the biggest challenges faced by stream processing systems is how to handle delayed data.<a data-type="indexterm" data-primary="windows" data-secondary="emitting results of" data-tertiary="grace period" id="idm46281561339896" target="_blank" rel="noopener noreferrer"></a> Many frameworks, including those that adhere to the influential <a href="https://oreil.ly/wAhJZ" target="_blank" rel="noopener noreferrer">Dataflow model</a> (e.g., Apache Flink), leverage <em>watermarks</em>.<a data-type="indexterm" data-primary="allowed lateness of events" id="idm46281561337368" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="watermarks" id="idm46281561336568" target="_blank" rel="noopener noreferrer"></a> Watermarks are used to estimate when all of the data for a given window should have arrived (usually by configuring the <em>window size</em> and the <em>allowed lateness</em> of events). Users can then specify how late events (as determined by the watermark) should be handled, with a popular default (in Dataflow, Flink, and others) being to discard late events.</p>
              <p>Similar to the watermark approach, Kafka Streams allows us to configure the <em>allowed lateness</em> of events using a <em>grace period</em>.<a data-type="indexterm" data-primary="grace period" id="idm46281561333208" target="_blank" rel="noopener noreferrer"></a> Setting a grace period will keep the window open for a specific amount of time, in order to admit delayed/unordered events to <span class="keep-together">the window.</span> For example, we initially configured our tumbling window using the <span class="keep-together">following</span> code:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">TimeWindows tumblingWindow =
  TimeWindows.of(Duration.ofSeconds(60));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>However, if we want to be able to tolerate five seconds of lag for pulse events (which we will use to calculate a heart rate), we could define our tumbling window with a grace period as well:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">TimeWindows tumblingWindow =
  TimeWindows
    .of(Duration.ofSeconds(60))
    .grace(Duration.ofSeconds(5));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>We could increase the grace period further, but just remember the trade-off: higher grace periods optimize for completeness, because they keep the window open longer (allowing for data to be delayed), but they do this at the expense of higher latency (windows won’t be closed until the grace period ends).</p>
              <p>Now, let’s see how to solve the problem of intermediate results being emitted in our windowed heart rate aggregation.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Suppression">
            <div class="sect2" id="C5_SUPPRESS_SECTION">
              <h2>Suppression</h2>
              <p>As we learned in the previous section, Kafka Streams’ strategy of continuous refinement, which involves emitting<a data-type="indexterm" data-primary="windows" data-secondary="emitting results of" data-tertiary="suppression strategies" id="idm46281561325272" target="_blank" rel="noopener noreferrer"></a> the results of a window whenever new data arrives, is ideal when we are optimizing for low latency and can tolerate incomplete (i.e., intermediate) results being emitted from the window.<sup><a data-type="noteref" id="idm46281561323688-marker" href="ch05.html#idm46281561323688" target="_blank" rel="noopener noreferrer">9</a></sup></p>
              <p>However, in our patient monitoring application, this is undesirable. We cannot calculate a heart rate using less than 60 seconds of data, so we need to only emit the final result of a window. <a data-type="indexterm" data-primary="suppress operator" data-secondary="using to suppress intermediate window results" id="idm46281561322440" target="_blank" rel="noopener noreferrer"></a>This is where the <code>suppress</code> operator comes into play. The <code>suppress</code> operator can be used to only emit the final computation of a window, and to suppress (i.e., temporarily hold intermediate computations in memory) all other events. In order to use the <code>suppress</code> operator, we need to decide three things:</p>
              <ul>
                <li>
                  <p>Which suppression strategy should be used for suppressing intermediate window computations</p></li>
                <li>
                  <p>How much memory should be used for buffering the suppressed events (this is set using a <em>Buffer Config</em>)</p></li>
                <li>
                  <p>What to do when this memory limit is exceeded (this is controlled using a <em>Buffer Full Strategy</em>)</p></li>
              </ul>
              <p>Let’s first look at the two suppression strategies. <a data-type="xref" href="#C5_SUPPRESSION_STRATEGIES" target="_blank" rel="noopener noreferrer">Table&nbsp;5-1</a> describes each strategy available in Kafka Streams. Note that each strategy is available as a method on the <code>Suppressed</code> class.</p>
              <table id="C5_SUPPRESSION_STRATEGIES">
                <caption>
                  <span class="label">Table 5-1. </span>Window suppression strategies
                </caption>
                <thead>
                  <tr>
                    <th>Strategy</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <p><span class="keep-together"><code>Suppressed.untilWindowCloses</code></span></p></td>
                    <td>
                      <p>Only emit the final results of a window.</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><span class="keep-together"><code>Suppressed.untilTimeLimit</code></span></p></td>
                    <td>
                      <p>Emit the results of a window after a configurable amount of time has elapsed since the last event was received.<a data-type="indexterm" data-primary="Suppressed.untilWindowCloses" id="idm46281561306072" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Suppressed.untilTimeLimit" id="idm46281561305336" target="_blank" rel="noopener noreferrer"></a> If another event with the same key arrives before the time limit is up, it replaces the first event in the buffer (note, the timer is not restarted when this happens). <a data-type="indexterm" data-primary="rate limiting" data-secondary="of updates" id="idm46281561304328" target="_blank" rel="noopener noreferrer"></a>This has the effect of <em>rate-limiting</em> updates.</p></td>
                  </tr>
                </tbody>
              </table>
              <p>In our patient monitoring application, we only want to emit the results of our heart rate window once a full 60 seconds has elapsed. Therefore, we will use the <code>Suppressed.untilWindowCloses</code> suppression strategy. However, before we can use this strategy in our processor topology, we need to tell Kafka Streams how to buffer the unemitted results in memory. After all, suppressed records aren’t <em>discarded</em>; instead, the latest unemitted record for each key in a given window is kept in memory until it’s time to emit the result. Memory is a limited resource, so Kafka Streams requires us to be explicit with how it is used for this potentially memory-intensive task of suppressing updates.<sup><a data-type="noteref" id="idm46281561300536-marker" href="ch05.html#idm46281561300536" target="_blank" rel="noopener noreferrer">10</a></sup> In order to define our buffering strategy, we need to use Buffer Configs. <a data-type="xref" href="#C5_BUFFER_CONFIGS" target="_blank" rel="noopener noreferrer">Table&nbsp;5-2</a> contains a description of each Buffer Config available in Kafka Streams.</p>
              <table id="C5_BUFFER_CONFIGS">
                <caption>
                  <span class="label">Table 5-2. </span>Buffer Configs
                </caption>
                <thead>
                  <tr>
                    <th>Buffer Config</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <p><span class="keep-together"><code>BufferConfig.maxBytes()</code></span></p></td>
                    <td>
                      <p>The in-memory buffer for storing suppressed events will be constrained by a configured number of bytes.<a data-type="indexterm" data-primary="BufferConfig.maxBytes method" id="idm46281561292888" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="BufferConfig.maxRecords method" id="idm46281561292120" target="_blank" rel="noopener noreferrer"></a></p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><span class="keep-together"><code>BufferConfig.maxRecords()</code></span></p></td>
                    <td>
                      <p>The in-memory buffer for storing suppressed events will be constrained by a configured number of keys.</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><span class="keep-together"><code>BufferConfig.unbounded()</code></span></p></td>
                    <td>
                      <p>The in-memory buffer for storing suppressed events will use as much heap space as needed to hold the suppressed records in the window.<a data-type="indexterm" data-primary="BufferConfig.unbounded method" id="idm46281561287000" target="_blank" rel="noopener noreferrer"></a> If the application runs out of heap, an <code>OutOfMemoryError</code> (OOM) exception will be thrown.</p></td>
                  </tr>
                </tbody>
              </table>
              <p>Finally, the last requirement for suppressing records is to tell Kafka Streams what to do when the buffer is full.<a data-type="indexterm" data-primary="buffer full strategies" id="idm46281561284792" target="_blank" rel="noopener noreferrer"></a> Kafka Streams exposes a couple of Buffer Full Strategies, each of which is described in <a data-type="xref" href="#C5_BUFFER_FULL" target="_blank" rel="noopener noreferrer">Table&nbsp;5-3</a>.</p>
              <table id="C5_BUFFER_FULL">
                <caption>
                  <span class="label">Table 5-3. </span>Buffer Full Strategies
                </caption>
                <thead>
                  <tr>
                    <th>Buffer Full Strategy</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <p><span class="keep-together"><code>shutDownWhenFull</code></span></p></td>
                    <td>
                      <p>Gracefully shut down the application when the buffer is full. You will never see intermediate window computations when using this strategy.<a data-type="indexterm" data-primary="shutDownWhenFull" id="idm46281561277736" target="_blank" rel="noopener noreferrer"></a></p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><span class="keep-together"><code>emitEarlyWhenFull</code></span></p></td>
                    <td>
                      <p>Emit the oldest results when the buffer is full instead of shutting down the application. You may still see intermediate window computations using this strategy.<a data-type="indexterm" data-primary="emitEarlyWhenFull" id="idm46281561274792" target="_blank" rel="noopener noreferrer"></a></p></td>
                  </tr>
                </tbody>
              </table>
              <p>Now that we understand which suppression strategies, Buffer Configs, and Buffer Full Strategies are available, let’s make a decision about which combination of the three works for us. We don’t want to rate-limit updates, so we won’t use <code>untilTimeLimit</code>. Instead, we want to only emit the final results of the heart rate window, so <code>untilWindowCloses</code> is a better fit here. Second, we expect our keyspace to be relatively small, so we’ll choose the <code>unbounded</code> Buffer Config. Finally, we never want to emit results early since that would lead to an inaccurate heart rate calculation (e.g., if we were to emit the number of heartbeats after only 20 seconds elapsed). Therefore, we will use <code>shutDownWhenFull</code> as our Buffer Config strategy.</p>
              <p>Putting this all together, we can update our patient monitoring topology to use the <code>suppress</code> operator <a data-type="indexterm" data-primary="suppress operator" data-secondary="using for heart rate window in patient monitoring application" id="idm46281561270200" target="_blank" rel="noopener noreferrer"></a>as shown in <a data-type="xref" href="#C5_SUPPRESS" target="_blank" rel="noopener noreferrer">Example&nbsp;5-4</a>.</p>
              <div id="C5_SUPPRESS" data-type="example" class="pagebreak-before">
                <h5><span class="label">Example 5-4. </span>Use the <code>suppress</code> operator to only emit the final results of the heart rate window (<code>tumblingWindow</code>)</h5>
                <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                  <pre data-type="programlisting">TimeWindows tumblingWindow =
  TimeWindows
    .of(Duration.ofSeconds(60))
    .grace(Duration.ofSeconds(5));

KTable&lt;Windowed&lt;String&gt;, Long&gt; pulseCounts =
  pulseEvents
   .groupByKey()
   .windowedBy(tumblingWindow)
   .count(Materialized.as("pulse-counts"))
   .suppress(
     Suppressed.untilWindowCloses(BufferConfig.unbounded().shutDownWhenFull())); <a class="co" id="co_windows_and_time_CO13-1" href="#callout_windows_and_time_CO13-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                  <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                </div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_windows_and_time_CO13-1" href="#co_windows_and_time_CO13-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Suppress the results of the window so that only the final computation is emitted.</p>
                </dd>
              </dl>
              <p>Now that we’ve completed step 4 in our processor topology (see <a data-type="xref" href="#C5_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-1</a>), let’s move on to steps 5 and 6: filtering and rekeying the <code>pulse-events</code> and <code>body-temp-events</code> data.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="emitting window results" data-startref="ix_PMAwinres" id="idm46281561257496" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="emitting results of" data-startref="ix_winres" id="idm46281561256200" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Filtering and Rekeying Windowed KTables">
        <div class="sect1" id="idm46281561368648">
          <h1>Filtering and Rekeying Windowed KTables</h1>
          <p>If you look closely at the <code>KTable</code> in <a data-type="xref" href="#C5_SUPPRESS" target="_blank" rel="noopener noreferrer">Example&nbsp;5-4</a>, you’ll see that windowing caused our key<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="filtering and rekeying windowed KTables" id="idm46281561251928" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="rekeying records" data-secondary="in windowed KTables" data-secondary-sortas="windowed" id="idm46281561250904" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="KTable class" data-secondary="windowed KTables, filtering and rekeying" id="idm46281561249688" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="filtering and rekeying windowed KTables" id="idm46281561248648" target="_blank" rel="noopener noreferrer"></a> to change from a <code>String</code> type to a <code>Windowed&lt;String&gt;</code>.<a data-type="indexterm" data-primary="keys" data-secondary="rekeying windowed KTables" id="idm46281561246696" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="filtering data" data-secondary="filtering and rekeying windowed KTables" id="idm46281561245672" target="_blank" rel="noopener noreferrer"></a> As mentioned before, this is because windowing causes our records to be grouped by an extra dimension: the window range. So, in order to join the <code>body-temp-events</code> stream, we need to rekey the <code>pulse-events</code> stream. We also need to filter both streams of data since we are only interested in records that exceed our predefined thresholds.</p>
          <p>It may seem like we should perform the filter and rekeying operation in any order, and that’s true. However, one piece of advice is to perform filtering as early as you can. We know that rekeying records requires a repartition topic, so if we filter first, then we will reduce the number of reads/writes to this topic, making our application more performant.</p>
          <p>Since both filtering and rekeying were discussed in previous chapters, we won’t do another deep dive into these concepts. But you can see the filtering and rekeying code for our patient monitoring application in the following code block:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KStream&lt;String, Long&gt; highPulse =
 pulseCounts
     .toStream() <a class="co" id="co_windows_and_time_CO14-1" href="#callout_windows_and_time_CO14-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
     .filter((key, value) -&gt; value &gt;= 100) <a class="co" id="co_windows_and_time_CO14-2" href="#callout_windows_and_time_CO14-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
     .map(
         (windowedKey, value) -&gt; {
           return KeyValue.pair(windowedKey.key(), value); <a class="co" id="co_windows_and_time_CO14-3" href="#callout_windows_and_time_CO14-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
      });

KStream&lt;String, BodyTemp&gt; highTemp =
     tempEvents.filter((key, value) -&gt; value.getTemperature() &gt; 100.4); <a class="co" id="co_windows_and_time_CO14-4" href="#callout_windows_and_time_CO14-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_windows_and_time_CO14-1" href="#co_windows_and_time_CO14-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Convert to a stream so we can use the <code>map</code> operator to rekey the records.</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO14-2" href="#co_windows_and_time_CO14-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Filter for only heart rates that exceed our predefined threshold of 100 bpm.</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO14-3" href="#co_windows_and_time_CO14-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Rekey the stream using the original key, which is available in <code>windowedKey.key()</code>. Note that <code>windowedKey</code> is an instance of <code>o⁠r⁠g⁠.⁠a⁠p⁠a⁠c⁠h⁠e⁠.⁠k⁠a⁠f​k⁠a⁠.⁠s⁠t⁠r⁠e⁠a⁠m⁠s⁠.⁠k⁠s⁠t⁠r⁠e⁠a⁠m⁠.⁠W⁠i⁠n⁠d⁠o⁠w⁠e⁠d</code>. This class contains methods for accessing the original key (as we saw with <code>windowedKey.key()</code>) and also the underlying time window (using <code>windowedKey.window()</code>).</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO14-4" href="#co_windows_and_time_CO14-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Filter for only core body temperature readings that exceed our predefined threshold of 100.4°F.</p>
            </dd>
          </dl>
          <p>We have completed steps 5 and 6 in our processor topology, so we are ready to perform a windowed join.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Windowed Joins">
        <div class="sect1" id="idm46281561219832">
          <h1>Windowed Joins</h1>
          <p>As discussed in <a data-type="xref" href="#C5_JOIN_WINDOW_SECTION" target="_blank" rel="noopener noreferrer">“Sliding join windows”</a>, windowed joins require a sliding join window.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="windowed joins" id="idm46281561217400" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="windowed joins" id="idm46281561216488" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="windowed joins" id="idm46281561215272" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="sliding join windows" data-secondary="using in windowed joins" id="idm46281561214328" target="_blank" rel="noopener noreferrer"></a> Sliding join windows compare the timestamps of events on both sides of the join to determine which records should be joined together. Windowed joins are required for <code>KStream-KStream</code> joins since streams are unbounded. Therefore, the data needs to be materialized into a local state store for performing quick lookups of related values.</p>
          <p>We can build a sliding join window and join the pulse rate and body temperature streams like so:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">StreamJoined&lt;String, Long, BodyTemp&gt; joinParams =
    StreamJoined.with(Serdes.String(), Serdes.Long(), JsonSerdes.BodyTemp()); <a class="co" id="co_windows_and_time_CO15-1" href="#callout_windows_and_time_CO15-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

JoinWindows joinWindows =
   JoinWindows
       .of(Duration.ofSeconds(60)) <a class="co" id="co_windows_and_time_CO15-2" href="#callout_windows_and_time_CO15-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
       .grace(Duration.ofSeconds(10)); <a class="co" id="co_windows_and_time_CO15-3" href="#callout_windows_and_time_CO15-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>

ValueJoiner&lt;Long, BodyTemp, CombinedVitals&gt; valueJoiner = <a class="co" id="co_windows_and_time_CO15-4" href="#callout_windows_and_time_CO15-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
    (pulseRate, bodyTemp) -&gt; new CombinedVitals(pulseRate.intValue(), bodyTemp);

KStream&lt;String, CombinedVitals&gt; vitalsJoined =
   highPulse.join(highTemp, valueJoiner, joinWindows, joinParams); <a class="co" id="co_windows_and_time_CO15-5" href="#callout_windows_and_time_CO15-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_windows_and_time_CO15-1" href="#co_windows_and_time_CO15-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Specify the Serdes to be used in the join.</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO15-2" href="#co_windows_and_time_CO15-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Records with timestamps one minute apart or less will fall into the same window, and will therefore be joined.</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO15-3" href="#co_windows_and_time_CO15-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Tolerate a delay of up to 10 seconds.</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO15-4" href="#co_windows_and_time_CO15-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Combine the heart rate and body temp into a <code>CombinedVitals</code> object.</p>
            </dd>
            <dt>
              <a class="co" id="callout_windows_and_time_CO15-5" href="#co_windows_and_time_CO15-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Perform the join.</p>
            </dd>
          </dl>
          <p>Joins are particularly interesting with regard to time because each side of the join could receive records at different rates, and so some additional synchronization is required to ensure events are joined at the appropriate time. Luckily for us, Kafka Streams is able to handle this scenario by allowing the timestamps on each side of the join to determine how data flows (and is subsequently joined) through our processor topology. We’ll explore these ideas in more detail in the next section.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Time-Driven Dataflow">
        <div class="sect1" id="idm46281561187128">
          <h1>Time-Driven Dataflow</h1>
          <p>We’ve already seen how time can impact the behavior of certain operations, including windowed joins and windowed aggregations.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="time-driven data flow" id="ix_PMAtddf" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="time" data-secondary="time-driven data flow" id="ix_timedafl" target="_blank" rel="noopener noreferrer"></a> However, time also controls the flow of data through our streams. It’s important for stream processing applications to synchronize the input streams to ensure correctness, especially when processing historical data from multiple sources.</p>
          <p>To facilitate this synchronization, Kafka Streams creates a single <em>partition group</em> for each stream task.<a data-type="indexterm" data-primary="partitions" data-secondary="partition groups" id="idm46281561181720" target="_blank" rel="noopener noreferrer"></a> A partition group buffers the queued records for each partition being handled by the given task using a priority queue, and includes the algorithm for selecting the next record (across all input partitions) for processing. The record with the lowest timestamp is selected for processing.</p>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p>Stream time is the highest timestamp observed for a particular topic-partition. It is initially unknown and can only increase or stay the same.<a data-type="indexterm" data-primary="stream time" id="idm46281561179144" target="_blank" rel="noopener noreferrer"></a> It advances only when new data is seen. This is different from the other notions of time we discussed since it is internal to Kafka Streams itself.</p>
          </div>
          <p>When a single Kafka Streams task consumes data from more than one partition (e.g., in the case of a join), Kafka Streams will compare<a data-type="indexterm" data-primary="tasks" data-secondary="single task consuming data from more than one partition" id="idm46281561177496" target="_blank" rel="noopener noreferrer"></a> the timestamps for the next unprocessed records (called <em>head records</em>) in each partition (record queue) and will choose the record with the lowest timestamp for processing. The selected record is forwarded to the appropriate source processor in the topology. <a data-type="xref" href="#C5_FLOW_SORT" target="_blank" rel="noopener noreferrer">Figure&nbsp;5-8</a> shows how this works.</p>
          <figure>
            <div id="C5_FLOW_SORT" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0509.png" style="width: 27rem">
              <h6><span class="label">Figure 5-8. </span>Record timestamps are compared to determine how data flows through your Kafka Streams application</h6>
            </div>
          </figure>
          <p>With our join in place, and some reassurances that Kafka Streams will attempt to process our records in timestamp order using time-based flow-control mechanisms,<sup><a data-type="noteref" id="idm46281561172184-marker" href="ch05.html#idm46281561172184" target="_blank" rel="noopener noreferrer">11</a></sup> we are now ready to tackle the final steps of our patient monitoring topology. We’ll briefly recap how to add a sink processor to our topology, and then we’ll learn how to query windowed key-value stores.</p>
          <section data-type="sect2" data-pdf-bookmark="Alerts Sink">
            <div class="sect2" id="idm46281561169880">
              <h2>Alerts Sink</h2>
              <p>In order to make our join results available to downstream consumers, we need to write the enriched data back to Kafka.<a data-type="indexterm" data-primary="time" data-secondary="time-driven data flow" data-tertiary="alerts sink" id="idm46281561168216" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="sinks" data-secondary="alerts sink" id="idm46281561166968" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="alerts sink" id="idm46281561166024" target="_blank" rel="noopener noreferrer"></a> As we’ve seen in previous chapters, adding a sink is extremely easy in Kafka Streams. The following code shows how to add the alerts sink. This sink will be written to whenever our application determines that a patient is at risk for SIRS, as determined by our thresholds and windowed join:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">vitalsJoined.to(
  "alerts",
  Produced.with(Serdes.String(), JsonSerdes.CombinedVitals())
);</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>One benefit of using a timestamp extractor when registering our source streams (see <a data-type="xref" href="#C5_REGISTER_STREAMS" target="_blank" rel="noopener noreferrer">Example&nbsp;5-2</a>) is that our output records will also be associated with the extracted timestamp.<a data-type="indexterm" data-primary="timestamp extractors" data-secondary="benefits of using when registering source streams" id="idm46281561162904" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="joins" data-secondary="for Kafka Streams" data-tertiary="timestamp extraction and" id="idm46281561161992" target="_blank" rel="noopener noreferrer"></a> For unjoined streams/tables, the timestamp is propagated from the initial timestamp extraction that occurred when you registered the source processors. However, if you perform a join, like we’ve done with our patient monitoring application, then Kafka Streams will look at the timestamps for each record involved in the join and choose the maximum value for the output record.<sup><a data-type="noteref" id="idm46281561160232-marker" href="ch05.html#idm46281561160232" target="_blank" rel="noopener noreferrer">12</a></sup></p>
              <p>Registering the alerts sink exposes our patient monitoring data to real-time consumers of the <code>alerts</code> topic. Now, let’s learn how to query windowed key-value stores by exposing the results of our windowed aggregation (which calculates a patient’s heart rate).</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Querying Windowed Key-Value Stores">
            <div class="sect2" id="idm46281561158072">
              <h2>Querying Windowed Key-Value Stores</h2>
              <p>In <a data-type="xref" href="ch04.html#C4_QUERY_NON_WINDOW" target="_blank" rel="noopener noreferrer">“Querying Nonwindowed Key-Value Stores”</a>, we saw how to query nonwindowed key-value stores. However, <em>windowed key-value stores</em> support <a data-type="indexterm" data-primary="time" data-secondary="time-driven data flow" data-tertiary="querying windowed key-value stores" id="idm46281561154984" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="key-value stores" data-secondary="windowed, querying" id="idm46281561153720" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="querying windowed key-value stores" id="idm46281561152776" target="_blank" rel="noopener noreferrer"></a>a different set of queries because the record keys are multidimensional, and consist of both the original key <em>and</em> the window range, as opposed to just the original record key (which is what we see in nonwindowed key-value stores). We’ll start by looking at key and window range scans.</p>
              <section data-type="sect3" data-pdf-bookmark="Key + window range scans">
                <div class="sect3" id="idm46281561150984">
                  <h3>Key + window range scans</h3>
                  <p>There are two different types of range scans that can be used for windowed key-value stores.<a data-type="indexterm" data-primary="key-value stores" data-secondary="windowed, querying" data-tertiary="key + window range scans" id="idm46281561149336" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="range scans" data-secondary="key + window range" id="idm46281561148072" target="_blank" rel="noopener noreferrer"></a> The first type searches for a specific key in a given window range, and therefore requires three parameters:</p>
                  <ul>
                    <li>
                      <p>The key to search for (in the case of our patient monitoring application, this would correspond to the patient ID, e.g., <code>1</code>)</p></li>
                    <li>
                      <p>The lower boundary of the window range, represented as milliseconds from the epoch<sup><a data-type="noteref" id="idm46281561144200-marker" href="ch05.html#idm46281561144200" target="_blank" rel="noopener noreferrer">13</a></sup> (e.g., <code>1605171720000</code>, which translates to <code>2020-11-12T09:02:00.00Z</code>)</p></li>
                    <li>
                      <p>The upper boundary of the window range, represented as milliseconds from the epoch (e.g., <code>1605171780000</code>, which translates to <code>2020-11-12T09:03:00Z</code>)</p></li>
                  </ul>
                  <p>An example of how to execute this type of range scan, and how to extract the relevant properties from the result, is shown here:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">String key = 1;
Instant fromTime = Instant.parse("2020-11-12T09:02:00.00Z");
Instant toTime = Instant.parse("2020-11-12T09:03:00Z");

WindowStoreIterator&lt;Long&gt; range = getBpmStore().fetch(key, fromTime, toTime); <a class="co" id="co_windows_and_time_CO16-1" href="#callout_windows_and_time_CO16-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
while (range.hasNext()) {
 KeyValue&lt;Long, Long&gt; next = range.next(); <a class="co" id="co_windows_and_time_CO16-2" href="#callout_windows_and_time_CO16-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
 Long timestamp = next.key; <a class="co" id="co_windows_and_time_CO16-3" href="#callout_windows_and_time_CO16-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
 Long count = next.value; <a class="co" id="co_windows_and_time_CO16-4" href="#callout_windows_and_time_CO16-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
  // do something with the extracted values
}

range.close(); <a class="co" id="co_windows_and_time_CO16-5" href="#callout_windows_and_time_CO16-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a></pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO16-1" href="#co_windows_and_time_CO16-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Returns an iterator that can be used for iterating through each key in the selected time range.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO16-2" href="#co_windows_and_time_CO16-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Get the next element in the iteration.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO16-3" href="#co_windows_and_time_CO16-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The timestamp of the record is available in the <code>key</code> property.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO16-4" href="#co_windows_and_time_CO16-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>The value can be extracted using the <code>value</code> property.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_windows_and_time_CO16-5" href="#co_windows_and_time_CO16-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Close the iterator to avoid memory leaks.</p>
                    </dd>
                  </dl>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Window range scans">
                <div class="sect3" id="idm46281561129128">
                  <h3>Window range scans</h3>
                  <p>The second type of range scan that can be performed on windowed key-value stores searches for <em>all keys</em> within a given time range.<a data-type="indexterm" data-primary="key-value stores" data-secondary="windowed, querying" data-tertiary="window range scans" id="idm46281561113416" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="range scans" data-secondary="window range" id="idm46281561112168" target="_blank" rel="noopener noreferrer"></a> This type of query requires two parameters:</p>
                  <ul>
                    <li>
                      <p>The lower boundary of the window range, represented as milliseconds from the epoch<sup><a data-type="noteref" id="idm46281561110072-marker" href="ch05.html#idm46281561110072" target="_blank" rel="noopener noreferrer">14</a></sup> (e.g., <code>1605171720000</code>, which translates to <code>2020-11-12T09:02:00.00Z</code>)</p></li>
                    <li>
                      <p>The upper boundary of the window range, represented as milliseconds from the epoch (e.g., <code>1605171780000</code>, which translates to <code>2020-11-12T09:03:00Z</code>)</p></li>
                  </ul>
                  <p>The following code block shows how to execute this type of range scan, and how to extract the relevant properties from the result:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">Instant fromTime = Instant.parse("2020-11-12T09:02:00.00Z");
Instant toTime = Instant.parse("2020-11-12T09:03:00Z");

KeyValueIterator&lt;Windowed&lt;String&gt;, Long&gt; range =
  getBpmStore().fetchAll(fromTime, toTime);

while (range.hasNext()) {
  KeyValue&lt;Windowed&lt;String&gt;, Long&gt; next = range.next();
  String key = next.key.key();
  Window window = next.key.window();
  Long start = window.start();
  Long end = window.end();
  Long count = next.value;
    // do something with the extracted values
}

range.close();</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="All entries">
                <div class="sect3" id="idm46281561104360">
                  <h3>All entries</h3>
                  <p>Similar to range scan queries, the <code>all()</code> query returns <a data-type="indexterm" data-primary="key-value stores" data-secondary="windowed, querying" data-tertiary="all entries scans" id="idm46281561102376" target="_blank" rel="noopener noreferrer"></a>an iterator for all windowed key-value pairs that are available in the local state store.<sup><a data-type="noteref" id="idm46281561100856-marker" href="ch05.html#idm46281561100856" target="_blank" rel="noopener noreferrer">15</a></sup> The following code snippet shows how to execute an <code>all()</code> query against a local windowed key-value store. Iterating through the results is the same as the range scan query, so we have omitted that logic for brevity:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">KeyValueIterator&lt;Windowed&lt;String&gt;, Long&gt; range = getBpmStore().all();</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <div data-type="warning" epub:type="warning">
                    <h6>Warning</h6>
                    <p>It’s very important to close the iterator once you are finished with it to avoid memory leaks. For example, looking at the preceding code snippet, we would call <code>range.close()</code> when we are finished with the iterator.</p>
                  </div>
                  <p>Using these query types, you can build an interactive query service in the same way that we discussed in the previous chapter.<a data-type="indexterm" data-primary="remote queries" id="idm46281561096584" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="interactive queries" id="idm46281561095880" target="_blank" rel="noopener noreferrer"></a> We just need to add an RPC service and client to our application, leverage the Kafka Streams instance discovery logic for discovering remote application instances, and wire up the preceding windowed key-value store queries to the RPC or RESTful endpoints (see <a data-type="xref" href="ch04.html#C4_REMOTE_QUERIES" target="_blank" rel="noopener noreferrer">“Remote Queries”</a> for more information).</p>
                </div>
              </section>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Summary">
        <div class="sect1" id="idm46281561093880">
          <h1>Summary</h1>
          <p>In this chapter, we learned how time can be used for more advanced stream processing use cases.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-secondary="time-driven data flow" data-startref="ix_PMAtddf" id="idm46281561092200" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="time" data-secondary="time-driven data flow" data-startref="ix_timedafl" id="idm46281561090984" target="_blank" rel="noopener noreferrer"></a> By being deliberate with the time semantics we use in our topology definitions, we can achieve more deterministic processing in Kafka Streams. Time not only drives the behavior of windowed aggregations, windowed joins, and other time-based operations, but since Kafka Streams tries to synchronize input streams based on time, it even controls how and when data flows through our application.</p>
          <p>Windowing data allows us to derive temporal relationships between events. Whether we want to leverage these relationships for aggregating data (as we did by converting raw pulse events into a time-based heart rate aggregation) or joining data (e.g., when we joined the heart rate stream with the body temperature stream), time opens the door for more meaningful data enrichment.</p>
          <p>Finally, learning how to query windowed key-value stores taught us something important about windowed state stores: keys are multidimensional (containing both the original key and the time range of the window), and therefore support a different set of queries, including windowed range scans.<a data-type="indexterm" data-primary="patient monitoring application tutorial" data-startref="ix_PMA" id="idm46281561087720" target="_blank" rel="noopener noreferrer"></a></p>
          <p>In the next chapter, we will wrap up our discussion of stateful Kafka Streams applications by looking at advanced state management tasks.</p>
        </div>
      </section>
      <div data-type="footnotes">
        <p data-type="footnote" id="idm46281561748520"><sup><a href="ch05.html#idm46281561748520-marker" target="_blank" rel="noopener noreferrer">1</a></sup> This is accomplished by measuring intracranial pressure, aggregating the measurements, and sending the aggregations to a predictive model. When the model predicts that the pressure will reach dangerous levels in the next 30 minutes, then healthcare professionals will be notified so they can take the appropriate action. All of this is made possible through time-aware stream processing. Check out Tim Berglund’s <a href="https://oreil.ly/GHbVd" target="_blank" rel="noopener noreferrer">interview with Ramesh Sringeri</a> for an in-depth look at this use case.</p>
        <p data-type="footnote" id="idm46281561744872"><sup><a href="ch05.html#idm46281561744872-marker" target="_blank" rel="noopener noreferrer">2</a></sup> We won’t actually implement the monitoring application in this tutorial since it isn’t necessary to demonstrate the time-centric features of Kafka Streams.</p>
        <p data-type="footnote" id="idm46281561659464"><sup><a href="ch05.html#idm46281561659464-marker" target="_blank" rel="noopener noreferrer">3</a></sup> Either directly via the topic-level config or indirectly using the broker-level config and no override for the topic-level config.</p>
        <p data-type="footnote" id="idm46281561652520"><sup><a href="ch05.html#idm46281561652520-marker" target="_blank" rel="noopener noreferrer">4</a></sup> Matthias J. Sax has a <a href="https://oreil.ly/MRiCu" target="_blank" rel="noopener noreferrer">great presentation</a> that talks about this, as well as some of the other topics discussed in this chapter.</p>
        <p data-type="footnote" id="idm46281561500104"><sup><a href="ch05.html#idm46281561500104-marker" target="_blank" rel="noopener noreferrer">5</a></sup> Taken from the <a href="https://oreil.ly/osqwA" target="_blank" rel="noopener noreferrer">Java docs</a>: “Aligned to the epoch means that the first window starts at timestamp zero.” In other words, a window size of 5,000 would have boundaries of 0–5,000, 5,000–10,000, etc. Note that the start time is inclusive but the end time is exclusive.</p>
        <p data-type="footnote" id="idm46281561485640"><sup><a href="ch05.html#idm46281561485640-marker" target="_blank" rel="noopener noreferrer">6</a></sup> In some systems, the term “sliding window” is used to refer to hopping windows. However, in Kafka Streams, hopping windows are distinct from sliding windows, which we will discuss shortly.</p>
        <p data-type="footnote" id="idm46281561434904"><sup><a href="ch05.html#idm46281561434904-marker" target="_blank" rel="noopener noreferrer">7</a></sup> Sliding aggregation windows are the only window type that require us to explicitly set a grace period. We’ll discuss grace periods later in this chapter.</p>
        <p data-type="footnote" id="idm46281561362696"><sup><a href="ch05.html#idm46281561362696-marker" target="_blank" rel="noopener noreferrer">8</a></sup> Of course, if you use ingestion-time semantics by setting <code>message.timestamp.type</code> to <code>LogAppendTime</code>, then the record timestamps will always appear in order. This is because the timestamp is overwritten at topic append time. However, the ingestion time is somewhat arbitrary in relation to the event itself, and in the best-case scenario is just a close approximation of the event time (assuming the event is written right after it’s <span class="keep-together">created</span>).</p>
        <p data-type="footnote" id="idm46281561323688"><sup><a href="ch05.html#idm46281561323688-marker" target="_blank" rel="noopener noreferrer">9</a></sup> This is different than how many other streaming systems work, in which processing occurs only when a window is closed.</p>
        <p data-type="footnote" id="idm46281561300536"><sup><a href="ch05.html#idm46281561300536-marker" target="_blank" rel="noopener noreferrer">10</a></sup> Since Kafka Streams holds the latest record for each key when using the <code>suppress</code> operator, the required amount of memory is a function of your keyspace, with smaller keyspaces for a given input stream requiring less memory than large keyspaces. The grace period can also impact memory usage.</p>
        <p data-type="footnote" id="idm46281561172184"><sup><a href="ch05.html#idm46281561172184-marker" target="_blank" rel="noopener noreferrer">11</a></sup> It’s important to note that this is best-effort when one of the input streams becomes empty. However, you can use the <code>max.task.idle.ms</code> configuration, which controls how long you are willing to wait for new records to arrive in the drained input stream, to help avoid this. The default value is <code>0</code>, but increasing this will allow you to improve time synchronization by waiting longer.</p>
        <p data-type="footnote" id="idm46281561160232"><sup><a href="ch05.html#idm46281561160232-marker" target="_blank" rel="noopener noreferrer">12</a></sup> Prior to version 2.3, the timestamp on the output record was set to the timestamp of whatever record triggered the join. Using the maximum value in newer versions of Kafka Streams is an improvement over the old behavior, since it ensures the result timestamp is the same whether data is out of order or not.</p>
        <p data-type="footnote" id="idm46281561144200"><sup><a href="ch05.html#idm46281561144200-marker" target="_blank" rel="noopener noreferrer">13</a></sup> 1970-01-01T00:00:00Z (UTC)</p>
        <p data-type="footnote" id="idm46281561110072"><sup><a href="ch05.html#idm46281561110072-marker" target="_blank" rel="noopener noreferrer">14</a></sup> 1970-01-01T00:00:00Z (UTC)</p>
        <p data-type="footnote" id="idm46281561100856"><sup><a href="ch05.html#idm46281561100856-marker" target="_blank" rel="noopener noreferrer">15</a></sup> Depending on the number of keys in your state store, this could be a heavyweight call.</p>
      </div>
    </div>
    <div class="chapter" id="ch6">
      <h1><span class="label">Chapter 6. </span>Advanced State Management</h1>
      <p>In the past two chapters, we discussed <a data-type="indexterm" data-primary="state" data-secondary="advanced management" id="ix_stadv" target="_blank" rel="noopener noreferrer"></a>stateful processing in Kafka Streams. As we learned how to perform aggregations, joins, and windowed operations, it became apparent that stateful processing is pretty easy to get started with.</p>
      <p>However, as I alluded to previously, state stores come with additional operational complexity. As you scale your application, experience failures, and perform routine maintenance, you will learn that stateful processing requires a deeper understanding of the underlying mechanics to ensure your application continues to operate smoothly over time.</p>
      <p>The goal of this chapter is to dig deeper into state stores so that you can achieve a higher level of reliability when building stateful stream processing applications. <a data-type="indexterm" data-primary="rebalancing" id="idm46281561081464" target="_blank" rel="noopener noreferrer"></a>A large portion of this chapter is dedicated to the topic of <em>rebalancing</em>, which occurs when work needs to be redistributed across your consumer group. Rebalancing can be especially impactful for stateful applications, so we’ll develop our understanding so that you are equipped to deal with this in your own applications.</p>
      <p>Some of the questions we will answer include:</p>
      <ul>
        <li>
          <p>How are persistent state stores represented on disk?</p></li>
        <li>
          <p>How do stateful applications achieve fault tolerance?</p></li>
        <li>
          <p>How can we configure built-in state stores?</p></li>
        <li>
          <p>What kinds of events are the most impactful for stateful applications?</p></li>
        <li>
          <p>What measures can be taken to minimize recovery time of stateful tasks?</p></li>
        <li>
          <p>How do you ensure that state stores don’t grow indefinitely?</p></li>
        <li>
          <p>How can the DSL cache be used to rate limit downstream updates?</p></li>
        <li class="pagebreak-before">
          <p>How do you track progress of state restoration using a State Restore Listener?</p></li>
        <li>
          <p>How can State Listeners be used to detect rebalances?</p></li>
      </ul>
      <p>Let’s start by looking at the on-disk layout of persistent state stores.</p>
      <section data-type="sect1" data-pdf-bookmark="Persistent Store Disk Layout">
        <div class="sect1" id="C6_STORE_LAYOUT">
          <h1>Persistent Store Disk Layout</h1>
          <p>Kafka Streams includes both in-memory and persistent state stores. <a data-type="indexterm" data-primary="persistent state stores" data-secondary="disk layout" id="ix_perSS" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="persistent, disk layout" id="ix_ststper" target="_blank" rel="noopener noreferrer"></a>The latter category of state stores are generally preferred because they can help reduce the recovery time of an application whenever state needs to be reinitialized (e.g., in the event of failure or task migration).</p>
          <p>By default, persistent state stores live in the <em>/tmp/kafka-streams</em> directory. You can override this by setting the <code>StreamsConfig.STATE_DIR_CONFIG</code> property, and given the ephemeral nature of a <em>/tmp</em> directory (the contents of this directory are deleted during system reboots/crashes), you should choose another location for persisting your application state.</p>
          <p>Since persistent state stores live on disk, we can inspect the files very easily.<sup><a data-type="noteref" id="idm46281561065496-marker" href="ch06.html#idm46281561065496" target="_blank" rel="noopener noreferrer">1</a></sup> Doing so allows us to glean a surprising amount of information from the directory and filenames alone. The file tree in <a data-type="xref" href="#C6_STATE_DISK_LAYOUT" target="_blank" rel="noopener noreferrer">Example&nbsp;6-1</a> was taken from the patient monitoring application that we created in the previous chapter. The annotations provide additional detail about the important directories and files.</p>
          <div id="C6_STATE_DISK_LAYOUT" data-type="example">
            <h5><span class="label">Example 6-1. </span>An example of how a persistent state store is represented on disk</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">.
└── dev-consumer <a class="co" id="co_advanced_state_management_CO1-1" href="#callout_advanced_state_management_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    ├── 0_0
    │   ├── .lock
    │   └── pulse-counts
    ├── 0_1
    │   ├── .lock
    │   └── pulse-counts
    ├── 0_2
    │   ├── .lock
    │   └── pulse-counts
    ├── 0_3 <a class="co" id="co_advanced_state_management_CO1-2" href="#callout_advanced_state_management_CO1-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    │   ├── .checkpoint <a class="co" id="co_advanced_state_management_CO1-3" href="#callout_advanced_state_management_CO1-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    │   ├── .lock <a class="co" id="co_advanced_state_management_CO1-4" href="#callout_advanced_state_management_CO1-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
    │   └── pulse-counts <a class="co" id="co_advanced_state_management_CO1-5" href="#callout_advanced_state_management_CO1-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
    │       └── ...
    ├── 1_0
    │   ├── ...</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_advanced_state_management_CO1-1" href="#co_advanced_state_management_CO1-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The top-level directory contains the application ID. This is helpful to understand which applications are running on the server, especially in a shared environment where workloads can be scheduled on any number of nodes (e.g., in a Kubernetes cluster).</p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO1-2" href="#co_advanced_state_management_CO1-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Each of the second-level directories corresponds to a single Kafka Streams task. The directory name is formatted as a task ID. Task IDs are composed of two parts: <code>&lt;sub-topology-id&gt;_&lt;partition&gt;</code>. Note that, as we discussed in <a data-type="xref" href="ch02.html#C2_SUBTOPOLOGIES" target="_blank" rel="noopener noreferrer">“Sub-Topologies”</a>, a sub-topology might process data from one or multiple topics, depending on the logic of your program.<a data-type="indexterm" data-primary="sub-topologies" id="idm46281561044056" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO1-3" href="#co_advanced_state_management_CO1-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Checkpoint files store offsets from changelog topics (see <a data-type="xref" href="#C6_CHANGELOG" target="_blank" rel="noopener noreferrer">“Changelog Topics”</a>). <a data-type="indexterm" data-primary="changelog topics" id="idm46281561039976" target="_blank" rel="noopener noreferrer"></a>They indicate to Kafka Streams what data has been read into the local state store and, as you’ll see shortly, play an important role in state store recovery.</p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO1-4" href="#co_advanced_state_management_CO1-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The lock file is used by Kafka Streams to acquire a lock on the state directory. This helps prevent concurrency issues.<a data-type="indexterm" data-primary="state directory" id="idm46281561036392" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO1-5" href="#co_advanced_state_management_CO1-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The actual data is stored in named state directories. Here, <code>pulse-counts</code> corresponds to an explicit name that we set when materializing the state store.</p>
            </dd>
          </dl>
          <p>The main benefit of knowing what state stores look like on disk is to simply remove some of the mystery around how they work. Furthermore, the lock file and checkpoint files are especially important, and are referenced in certain error logs (for example, <span class="keep-together">permissions</span> issues could surface as a failure to write to a checkpoint file, while a concurrency issue could lead to an error about Kafka Streams failing to acquire a lock), so understanding their location and utility is helpful.</p>
          <p>The checkpoint file plays an important role in state store recovery. Let’s dig into this further by first looking at the fault-tolerant features of stateful applications, and then seeing how offset checkpoints are used to reduce recovery time.<a data-type="indexterm" data-primary="state stores" data-secondary="persistent, disk layout" data-startref="ix_ststper" id="idm46281561030616" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="persistent state stores" data-secondary="disk layout" data-startref="ix_perSS" id="idm46281561029368" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Fault Tolerance">
        <div class="sect1" id="idm46281561028024">
          <h1>Fault Tolerance</h1>
          <p>Kafka Streams owes much of its fault-tolerant characteristics to Kafka’s storage layer and group management protocol.<a data-type="indexterm" data-primary="fault tolerance" data-secondary="stateful applications" id="ix_fault" target="_blank" rel="noopener noreferrer"></a> For example, data replication at the partition level means if a broker goes offline, data can still be consumed from one of the replicated partitions on another broker. Furthermore, by using consumer groups, if a single instance of your application goes down, work will be redistributed to one of the healthy instances.</p>
          <p>However, when it comes to stateful applications, Kafka Streams takes additional measures to ensure applications are resilient to failure. This includes using changelog topics to back state stores, and standby replicas to minimize reinitialization time in the event that state is lost. We’ll discuss these Kafka Streams–specific fault-tolerant features in further detail in the following sections.</p>
          <section data-type="sect2" data-pdf-bookmark="Changelog Topics">
            <div class="sect2" id="C6_CHANGELOG">
              <h2>Changelog Topics</h2>
              <p>Unless explicitly disabled, state stores are backed by changelog topics, which are created and managed by Kafka Streams.<a data-type="indexterm" data-primary="fault tolerance" data-secondary="stateful applications" data-tertiary="changelog topics" id="idm46281561021576" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="changelog topics" data-secondary="backing state stores" id="idm46281561020328" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="backed by changelog topics" id="idm46281561019384" target="_blank" rel="noopener noreferrer"></a> These topics capture state updates for every key in the store, and can be replayed in the event of failure to rebuild the application <span class="keep-together">state.<sup><a data-type="noteref" id="idm46281561017672-marker" href="ch06.html#idm46281561017672" target="_blank" rel="noopener noreferrer">2</a></sup></span> In the event of a total state loss (or when spinning up a new instance), the changelog topic is replayed from the beginning. However, if a checkpoint file exists (see <a data-type="xref" href="#C6_STATE_DISK_LAYOUT" target="_blank" rel="noopener noreferrer">Example&nbsp;6-1</a>), then the state can be replayed from the checkpointed offset found in that file, since this offset indicates what data has already been read into the state store. The latter is much quicker because recovering only part of the state takes less time than recovering the full state.</p>
              <p>Changelog topics are configurable <a data-type="indexterm" data-primary="changelog topics" data-secondary="configuring" id="idm46281561014376" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Materialized class" id="idm46281561013400" target="_blank" rel="noopener noreferrer"></a>using the <code>Materialized</code> class in the DSL. For example, in the previous chapter, we materialized a state store named <code>pulse-counts</code> using the following code:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">pulseEvents
  .groupByKey()
  .windowedBy(tumblingWindow)
  .count(Materialized.as("pulse-counts"));</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>There are some additional methods on the <code>Materialized</code> class that allow us to customize the changelog topics even further.<a data-type="indexterm" data-primary="ephemeral stores" id="idm46281561009976" target="_blank" rel="noopener noreferrer"></a> For example, to disable change logging completely, we could use the following code to create what is sometimes called an <em>ephemeral store</em> (i.e., state stores that cannot be restored on failure):</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">Materialized.as("pulse-counts").withLoggingDisabled();</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>However, disabling change logging isn’t usually a good idea since it means your state store will no longer be fault tolerant, and it prevents you from using standby replicas.<a data-type="indexterm" data-primary="windows" data-secondary="overriding retention of windowed or session stores" id="idm46281561007240" target="_blank" rel="noopener noreferrer"></a> When it comes to configuring changelog topics, you will more commonly either override the retention of windowed or session stores using the <code>withRetention</code> method (this is covered a little later in <a data-type="xref" href="#c6_WINDOW_RETENTION" target="_blank" rel="noopener noreferrer">“Window retention”</a>) or pass in certain topic configs for the changelog topic. For example, if we wanted to bump the number of <code>insync</code> replicas to two, we could use the following code:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">Map&lt;String, String&gt; topicConfigs =
  Collections.singletonMap("min.insync.replicas", "2"); <a class="co" id="co_advanced_state_management_CO2-1" href="#callout_advanced_state_management_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

KTable&lt;Windowed&lt;String&gt;, Long&gt; pulseCounts =
    pulseEvents
      .groupByKey()
      .windowedBy(tumblingWindow)
      .count(
          Materialized.&lt;String, Long, WindowStore&lt;Bytes, byte[]&gt;&gt;
              as("pulse-counts")
              .withValueSerde(Serdes.Long())
              .withLoggingEnabled(topicConfigs)); <a class="co" id="co_advanced_state_management_CO2-2" href="#callout_advanced_state_management_CO2-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO2-1" href="#co_advanced_state_management_CO2-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Create a map for saving our topic configs. The entries can include any valid <a href="https://oreil.ly/L_WOj" target="_blank" rel="noopener noreferrer">topic config</a> and value.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO2-2" href="#co_advanced_state_management_CO2-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Configure the changelog topic by passing the topic configs to the <code>Materialized.withLoggingEnabled</code> method.<a data-type="indexterm" data-primary="Materialized.withLoggingEnabled method" id="idm46281550693256" target="_blank" rel="noopener noreferrer"></a></p>
                </dd>
              </dl>
              <p>Now, if you describe the topic, you’ll see the topic was configured accordingly:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">$ kafka-topics \
  --bootstrap-server localhost:9092 \
  --topic dev-consumer-pulse-counts-changelog \ <a class="co" id="co_advanced_state_management_CO3-1" href="#callout_advanced_state_management_CO3-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  --describe

# output
Topic: dev-consumer-pulse-counts-changelog
PartitionCount: 4
ReplicationFactor:1
Configs: min.insync.replicas=2
...</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO3-1" href="#co_advanced_state_management_CO3-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Note that changelog topics have the following naming scheme: <br><code>&lt;⁠a⁠p⁠p⁠l⁠i​c⁠a⁠t⁠i⁠o⁠n⁠_⁠i⁠d⁠&gt;⁠-⁠&lt;⁠i⁠n⁠t⁠e⁠r⁠n⁠a⁠l⁠_⁠s⁠t⁠o⁠r⁠e⁠_⁠n⁠a⁠m⁠e⁠&gt;⁠-⁠c⁠h⁠a⁠n⁠g⁠e⁠l⁠o⁠g</code>.</p>
                </dd>
              </dl>
              <p>One thing to note is that, at the time of this writing, you cannot reconfigure a changelog topic using this method after it has been created.<sup><a data-type="noteref" id="idm46281550684552-marker" href="ch06.html#idm46281550684552" target="_blank" rel="noopener noreferrer">3</a></sup> If you need to update a topic configuration on an existing changelog topic, then you’ll need to do so using the Kafka console scripts. An example of how to manually update a changelog topic after it has been created is shown here:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">$ kafka-configs \ <a class="co" id="co_advanced_state_management_CO4-1" href="#callout_advanced_state_management_CO4-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  --bootstrap-server localhost:9092 \
  --entity-type topics \
  --entity-name dev-consumer-pulse-counts-changelog \
  --alter \
  --add-config min.insync.replicas=1 <a class="co" id="co_advanced_state_management_CO4-2" href="#callout_advanced_state_management_CO4-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

# output
Completed updating config for topic dev-consumer-pulse-counts-changelog</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO4-1" href="#co_advanced_state_management_CO4-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>You can also use the <code>kafka-topics</code> console script. Remember to append the file extension (<em>.sh</em>) if you are running vanilla Kafka outside of Confluent Platform.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO4-2" href="#co_advanced_state_management_CO4-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Update the topic configurations.</p>
                </dd>
              </dl>
              <p>Now that we understand the purpose of the state store–backing changelog topics, as well as how to override the default topic configurations, let’s take a look at a feature that makes Kafka Streams highly available: standby replicas.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Standby Replicas">
            <div class="sect2" id="C6_STANDBY_REPLICAS">
              <h2>Standby Replicas</h2>
              <p>One method for reducing the downtime of stateful application failure is to create and maintain copies of task state<a data-type="indexterm" data-primary="fault tolerance" data-secondary="stateful applications" data-tertiary="standby replicas" id="idm46281550668616" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="standby replicas" id="idm46281550667368" target="_blank" rel="noopener noreferrer"></a> across multiple application instances.</p>
              <p>Kafka Streams handles this automatically, as long as we set a positive value for the <span class="keep-together"><code>NUM_STANDBY_REPLICAS_CONFIG</code></span> property.<a data-type="indexterm" data-primary="NUM_STANDBY_REPLICAS_CONFIG property" id="idm46281550665240" target="_blank" rel="noopener noreferrer"></a> For example, to create two standby replicas, we can configure our application like so:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>When standby replicas are configured, Kafka Streams will attempt to reassign any failed stateful tasks to an instance with a hot standby. This will drastically reduce downtime for applications with large state by removing the need to reinitialize the underlying state store from scratch.</p>
              <p>Furthermore, as we’ll see toward the end of the chapter, newer versions of Kafka Streams allow us to fall back to standby replicas when querying state stores during a rebalance. But before we get to that, let’s discuss what rebalancing is, and why it is the biggest enemy of stateful Kafka Streams applications.<a data-type="indexterm" data-primary="fault tolerance" data-secondary="stateful applications" data-startref="ix_fault" id="idm46281550662168" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Rebalancing: Enemy of the State (Store)">
        <div class="sect1" id="idm46281550660792">
          <h1>Rebalancing: Enemy of the State (Store)</h1>
          <p>We’ve learned that changelog topics and standby replicas help reduce the impact of stateful application failure.<a data-type="indexterm" data-primary="state stores" data-secondary="rebalancing and" id="idm46281550659256" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="rebalancing" data-secondary="enemy of the state store" id="idm46281550658280" target="_blank" rel="noopener noreferrer"></a> The former allows Kafka Streams to reconstruct state whenever it’s lost, and the latter allows us to minimize the time it takes to reinitialize state stores.</p>
          <p>However, while Kafka Streams handles failure transparently, it doesn’t change the fact that losing a state store, even temporarily, can be incredibly disruptive (especially for heavily stateful applications). Why? Because the change-logging technique for backing up state still requires us to replay each message in the underlying topic, and if that topic is huge, then rereading each record could take several minutes or, in extreme cases, even hours.</p>
          <p class="pagebreak-before">The biggest culprit that leads to reinitializing state is <em>rebalancing</em>. We first encountered this term when we discussed consumer groups in <a data-type="xref" href="ch01.html#C1_CG" target="_blank" rel="noopener noreferrer">“Consumer Groups”</a>. A simplified explanation is that Kafka automatically distributes work across the active members of a consumer group, but occasionally the work needs to be redistributed in response to certain events—most notably group membership changes.<sup><a data-type="noteref" id="idm46281550653816-marker" href="ch06.html#idm46281550653816" target="_blank" rel="noopener noreferrer">4</a></sup> We won’t cover the entire rebalance protocol in depth, but the level of detail we will cover requires a quick vocabulary review:</p>
          <ul>
            <li>
              <p>The <em>group coordinator</em> is a designated broker that is responsible for maintaining the membership<a data-type="indexterm" data-primary="group coordinator" id="idm46281550651256" target="_blank" rel="noopener noreferrer"></a> of a consumer group (e.g., by receiving heartbeats and triggering a rebalance when a membership change is detected).</p></li>
            <li>
              <p>The <em>group leader</em> is a designated consumer in each consumer group that is responsible for determining the partition assignments.<a data-type="indexterm" data-primary="group leader" id="idm46281550648792" target="_blank" rel="noopener noreferrer"></a></p></li>
          </ul>
          <p>We will encounter these terms as we discuss rebalancing more in the following sections. But for now, the important takeaway is that rebalances are expensive when they cause a stateful task to be migrated to another instance that does not have a standby replica. There are a couple of strategies for dealing with the issues that rebalancing can introduce:</p>
          <ul>
            <li>
              <p>Prevent state from being moved when possible</p></li>
            <li>
              <p>If state does need to be moved or replayed, make recovery time as quick as <span class="keep-together">possible</span></p></li>
          </ul>
          <p>With both strategies, there are measures that Kafka Streams takes automatically, as well as actions we can take ourselves, to minimize the impact of rebalances. We’ll explore this in more detail, starting with the first strategy: preventing state migration.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Preventing State Migration">
        <div class="sect1" id="idm46281550643432">
          <h1>Preventing State Migration</h1>
          <p>When stateful tasks are reassigned to another running instance, the underlying state is migrated as well.<a data-type="indexterm" data-primary="state" data-secondary="advanced management" data-tertiary="preventing state migration" id="ix_stadvmig" target="_blank" rel="noopener noreferrer"></a> For applications with large state, it could take a long time to rebuild the state store on the destination node, and therefore should be avoided if possible.</p>
          <p>Since the group leader (one of the consumers) is responsible for determining how work is distributed among the active consumers, there is some onus on the Kafka Streams library (which implements the load balancing logic) to prevent unnecessary state store migration. One way it achieves this is through something called a <em>sticky assignor</em>, and it’s something we get for free when we use Kafka Streams. We’ll explore this in the next section.</p>
          <section data-type="sect2" data-pdf-bookmark="Sticky Assignment">
            <div class="sect2" id="idm46281550638600">
              <h2>Sticky Assignment</h2>
              <p>To help prevent stateful tasks from being reassigned, Kafka Streams uses a custom partition assignment strategy<sup><a data-type="noteref" id="idm46281550636968-marker" href="ch06.html#idm46281550636968" target="_blank" rel="noopener noreferrer">5</a></sup> that attempts to reassign tasks to instances that previously owned the task (and therefore, should still have a copy of the underlying state store).<a data-type="indexterm" data-primary="partitions" data-secondary="sticky partition assignment for tasks" id="idm46281550635704" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="tasks" data-secondary="sticky assignment" id="idm46281550634696" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="sticky partition assignment" id="idm46281550633752" target="_blank" rel="noopener noreferrer"></a> This strategy is called <em>sticky assignment</em>.</p>
              <p>To understand the problem Kafka Streams is solving with its sticky assignor, consider the default rebalancing strategy for other types of Kafka clients. <a data-type="xref" href="#c6_REBALANCE_WITHOUT_STICKY" target="_blank" rel="noopener noreferrer">Figure&nbsp;6-1</a> shows that when a rebalance occurs, a stateful task could potentially be migrated to another application instance, which would be extremely expensive.</p>
              <p>The sticky partition assignor that is included in Kafka Streams addresses this issue by keeping track of which task owned each partition (and the associated state stores) and reassigning the stateful task to its previous owner. As you can see in <a data-type="xref" href="#c6_REBALANCE_WITH_STICKY_EAGER" target="_blank" rel="noopener noreferrer">Figure&nbsp;6-2</a>, this drastically improves the availability of our application since it helps us avoid unnecessary reinitialization of potentially large state stores.</p>
              <p>While the sticky assignor helps reassign tasks to their previous owners, state stores can still be migrated if Kafka Streams clients are temporarily offline. Now, let’s discuss something we, as Kafka Streams developers, can enable to help avoid rebalances during transient downtime.</p>
              <figure>
                <div id="c6_REBALANCE_WITHOUT_STICKY" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0601.png" style="width: 25rem">
                  <h6><span class="label">Figure 6-1. </span>Nonsticky partition assignment</h6>
                </div>
              </figure>
              <figure>
                <div id="c6_REBALANCE_WITH_STICKY_EAGER" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0602.png" style="width: 25rem">
                  <h6><span class="label">Figure 6-2. </span>Sticky partition assignment using Kafka Streams’ built-in partition assignor</h6>
                </div>
              </figure>
            </div>
          </section>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Static Membership">
            <div class="sect2" id="idm46281550624392">
              <h2>Static Membership</h2>
              <p>One of the issues that can cause state to be moved around is <em>unnecessary rebalances</em>.<a data-type="indexterm" data-primary="rebalancing" data-secondary="preventing unnecessary rebalances" id="idm46281550621992" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="static membership" id="idm46281550620968" target="_blank" rel="noopener noreferrer"></a> Sometimes, even a healthy event, like a rolling bounce, can lead to several rebalances. If the group coordinator detects membership changes during these brief periods of unavailability, it will trigger a rebalance and immediately reassign the work to the remaining application instances.</p>
              <p>When an instance comes back online after a brief period of being down, the coordinator won’t recognize it because its member ID (a unique identifier assigned by the coordinator whenever a consumer is registered) is erased, so the instance is treated as a new member and work may be reassigned.</p>
              <p>To prevent this, you can use <a href="https://oreil.ly/Psghk" target="_blank" rel="noopener noreferrer"><em>static membership</em></a>. Static membership aims to reduce the number of rebalances due to transient downtime. It achieves this by using a hardcoded instance ID for identifying each unique application instance. The following configuration property allows you to set the ID:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">group.instance.id = app-1 <a class="co" id="co_advanced_state_management_CO5-1" href="#callout_advanced_state_management_CO5-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO5-1" href="#co_advanced_state_management_CO5-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>In this case, we set the ID to <code>app-1</code>. If we spin up another instance, we would assign it a unique ID as well (e.g., <code>app-2</code>). This ID must be unique across the entire cluster (even among different Kafka Streams applications, independent of <code>application.id</code>, and other consumers, independent of <code>group.id</code>).</p>
                </dd>
              </dl>
              <p>The hardcoded instance ID is typically used in conjunction with higher session timeouts,<sup><a data-type="noteref" id="idm46281550609272-marker" href="ch06.html#idm46281550609272" target="_blank" rel="noopener noreferrer">6</a></sup> which buys the application even more time for restarting so that the coordinator doesn’t think the consumer instance is dead when it goes offline for a short period of time.</p>
              <p>Static membership is only available for Kafka versions &gt;= 2.3, so if your client or brokers are on an older version, you’ll need to upgrade first. Be aware that increasing the session timeout is a double-edged sword. While it can prevent the coordinator from assuming an instance is dead during a brief period of downtime, the trade-off is that it can lead to slower detection of actual failure.</p>
              <p>Now that we’ve learned how we can use static membership to help avoid unnecessary rebalances, let’s look at how to reduce the impact of rebalances when they do happen. Once again, there are measures that Kafka Streams takes for us and actions we can take ourselves to reduce this impact. We will discuss both in the upcoming sections.<a data-type="indexterm" data-primary="state" data-secondary="advanced management" data-tertiary="preventing state migration" data-startref="ix_stadvmig" id="idm46281550606360" target="_blank" rel="noopener noreferrer"></a></p>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Reducing the Impact of Rebalances">
        <div class="sect1" id="idm46281550623960">
          <h1>Reducing the Impact of Rebalances</h1>
          <p>While static membership can be used to avoid unnecessary rebalances, sometimes a rebalance cannot be avoided.<a data-type="indexterm" data-primary="state" data-secondary="advanced management" data-tertiary="reducing impact of rebalances" id="ix_stadvreb" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="rebalancing" data-secondary="reducing impact of" id="ix_rebred" target="_blank" rel="noopener noreferrer"></a> After all, failure is expected in distributed systems. <span class="keep-together">Historically</span>, rebalancing has been very costly. This is because at the start of each rebalancing round, each client gives up all of its resources. We saw this in <a data-type="xref" href="#c6_REBALANCE_WITH_STICKY_EAGER" target="_blank" rel="noopener noreferrer">Figure&nbsp;6-2</a>, and a detailed view of this rebalancing step is shown in <a data-type="xref" href="#c6_REVOKED" target="_blank" rel="noopener noreferrer">Figure&nbsp;6-3</a>.</p>
          <figure>
            <div id="c6_REVOKED" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0603.png" style="width: 25rem">
              <h6><span class="label">Figure 6-3. </span>Revoking all resources during an eager rebalance is extremely inefficient</h6>
            </div>
          </figure>
          <p>This rebalancing strategy <a data-type="indexterm" data-primary="eager rebalancing" id="idm46281550595592" target="_blank" rel="noopener noreferrer"></a>is called <em>eager rebalancing</em>, and is impactful for two reasons:</p>
          <ul>
            <li>
              <p>A so-called stop-the-world effect occurs when all clients give up their resources, which means an application can fall behind on its work very quickly since processing is halted.</p></li>
            <li>
              <p>If a stateful task gets reassigned to a new instance, then the state will need to be replayed/rebuilt before processing starts. This leads to additional downtime.</p></li>
          </ul>
          <p>Remember, Kafka Streams tries to mitigate the second issue (stateful task migration) for us by using a custom sticky partition assignor. However, it has historically relied on its own implementation to achieve task stickiness instead of the rebalance protocol itself. As of version 2.4, however, an update to the rebalance protocol introduces additional measures that help reduce the impact of rebalances. We will discuss this new protocol, called <em>incremental cooperative rebalancing</em>, in the next section.</p>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Incremental Cooperative Rebalancing">
            <div class="sect2" id="idm46281550590088">
              <h2>Incremental Cooperative Rebalancing</h2>
              <p>Incremental cooperative rebalancing is a more efficient rebalancing protocol than eager rebalancing, and is enabled by default in versions &gt;= 2.4.<a data-type="indexterm" data-primary="incremental cooperative rebalancing" id="idm46281550588120" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="rebalancing" data-secondary="reducing impact of" data-tertiary="incremental cooperative rebalancing" id="idm46281550587352" target="_blank" rel="noopener noreferrer"></a> If you are using an older Kafka Streams version, you will want to upgrade in order to take advantage of this feature, since it provides the following advantages over the older eager rebalancing protocol:</p>
              <ul>
                <li>
                  <p>One global round of rebalancing is replaced with several smaller rounds (<em>incremental</em>).</p></li>
                <li>
                  <p>Clients hold on to resources (tasks) that do not need to change ownership, and they only stop processing the tasks that are being migrated (<em>cooperative</em>).</p></li>
              </ul>
              <p><a data-type="xref" href="#c6_ICR2" target="_blank" rel="noopener noreferrer">Figure&nbsp;6-4</a> shows how incremental cooperative rebalancing works at a high level when an application instance goes offline for a long period of time (i.e., any period of time <a data-type="indexterm" data-primary="session.timeout.ms config" id="idm46281550581080" target="_blank" rel="noopener noreferrer"></a>that exceeds the <code>session.timeout.ms</code> config, which is the timeout that the coordinator uses to detect consumer failures).</p>
              <p>As you can see, the healthy application instances (including the one with the stateful task) did not need to give up their resources when a rebalance was initiated. This is a huge improvement over the eager rebalancing strategy since it allows the application to continue processing during a rebalance.</p>
              <p>There’s a lot of detailed information about incremental cooperative rebalancing that we won’t cover here, but it’s important to know that newer versions of Kafka Streams implement this protocol under the hood, so we just need to make sure we’re running with a supported version of Kafka Streams.<sup><a data-type="noteref" id="idm46281550578376-marker" href="ch06.html#idm46281550578376" target="_blank" rel="noopener noreferrer">7</a></sup></p>
              <p>We’ve seen how incremental cooperative rebalancing can help reduce the impact of rebalances, so let’s look at a more active role application developers can take to ensure rebalances are less painful.</p>
              <figure>
                <div id="c6_ICR2" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0604.png" style="width: 25rem">
                  <h6><span class="label">Figure 6-4. </span>The stop-the-world effect is avoided in incremental cooperative rebalancing</h6>
                </div>
              </figure>
            </div>
          </section>
          <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Controlling State Size">
            <div class="sect2" id="idm46281550573752">
              <h2>Controlling State Size</h2>
              <p>If you’re not careful, your state stores could grow unbounded and cause operational issues.<a data-type="indexterm" data-primary="rebalancing" data-secondary="reducing impact of" data-tertiary="controlling state size" id="idm46281550571896" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="controlling size of" id="idm46281550570648" target="_blank" rel="noopener noreferrer"></a> For example, imagine the keyspace in your compacted changelog topic is very large (let’s say one billion keys), and the application state is evenly distributed across 10 physical nodes.<sup><a data-type="noteref" id="idm46281550569368-marker" href="ch06.html#idm46281550569368" target="_blank" rel="noopener noreferrer">8</a></sup> If one of the nodes goes offline, then you’ll need to replay a <span class="keep-together">minimum</span> of 100 million records to rebuild the state. This takes a lot of time and could lead to availability issues. It’s also not a great use of computational or storage <span class="keep-together">resources</span> to retain more data than you need.</p>
              <p>Depending on your use case, you may not need to retain the entire application state indefinitely. Perhaps the value of each record has a fixed lifetime. For example, at Mailchimp, we track the number of active email messages that haven’t yet been delivered, and perform per-key aggregations to expose various stats about these messages. Eventually, however, these email messages become inactive (i.e., they are delivered, or they bounce), and we no longer need to track them. Use cases like these necessitate active cleanup of our state stores. Removing unneeded data to keep your state store small greatly reduces the impact of rebalancing. If a stateful task needs to be migrated, it’s a lot easier to rebuild a small state store than an unnecessarily large one. So how do we remove unneeded state in Kafka Streams? We use <em>tombstones</em>.</p>
              <section data-type="sect3" data-pdf-bookmark="Tombstones">
                <div class="sect3" id="C6_TOMBSTONES">
                  <h3>Tombstones</h3>
                  <p>Tombstones are special records that indicate that some state needs to be deleted. <a data-type="indexterm" data-primary="tombstones" id="idm46281550563320" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="controlling size of" data-tertiary="using tombstones" id="idm46281550562616" target="_blank" rel="noopener noreferrer"></a>They are sometimes referred to as delete markers, and they always have a key and a <code>null</code> value. As mentioned previously, state stores are key-based, so the key in a tombstone record indicates which record from the state store needs to be deleted.</p>
                  <p>An example of how to generate a tombstone is shown in the following code block. In this hypothetical scenario, we are performing some aggregation for a hospital patient, but once we see a patient checkout event, we no longer expect any additional events for this patient, so we remove their data from the underlying state store:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">StreamsBuilder builder = new StreamsBuilder();
KStream&lt;byte[], String&gt; stream = builder.stream("patient-events");

stream
    .groupByKey()
    .reduce(
        (value1, value2) -&gt; {
          if (value2.equals(PATIENT_CHECKED_OUT)) {
            // create a tombstone
            return null; <a class="co" id="co_advanced_state_management_CO6-1" href="#callout_advanced_state_management_CO6-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
          }
          return doSomething(value1, value2); <a class="co" id="co_advanced_state_management_CO6-2" href="#callout_advanced_state_management_CO6-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
        });</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO6-1" href="#co_advanced_state_management_CO6-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Return <code>null</code> to generate a tombstone (i.e., a delete marker) whenever a patient checks out. This will cause the related key to be removed from the underlying state store.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO6-2" href="#co_advanced_state_management_CO6-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>If a patient has not checked out, then perform the aggregation logic.</p>
                    </dd>
                  </dl>
                  <p>While tombstones are useful for keeping key-value stores small, there is another method we can use to remove unneeded data from windowed key-value stores. We’ll discuss this in the next section.</p>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Window retention">
                <div class="sect3" id="c6_WINDOW_RETENTION">
                  <h3>Window retention</h3>
                  <p>Windowed stores have a configurable retention period to keep the state stores small.<a data-type="indexterm" data-primary="state stores" data-secondary="controlling size of" data-tertiary="window retention" id="idm46281550545784" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="retention of windowed state stores" id="idm46281550544536" target="_blank" rel="noopener noreferrer"></a> For example, in the previous chapter, we created a windowed store in our patient monitoring application to convert raw pulse events into a heart rate. The relevant code is shown here:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">TimeWindows tumblingWindow =
  TimeWindows.of(Duration.ofSeconds(60)).grace(Duration.ofSeconds(5));

KTable&lt;Windowed&lt;String&gt;, Long&gt; pulseCounts =
  pulseEvents
    .groupByKey()
    .windowedBy(tumblingWindow)
    .count(Materialized.&lt;String, Long, WindowStore&lt;Bytes, byte[]&gt;&gt;
      as("pulse-counts")) <a class="co" id="co_advanced_state_management_CO7-1" href="#callout_advanced_state_management_CO7-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    .suppress(
      Suppressed.untilWindowCloses(BufferConfig.unbounded().shutDownWhenFull()));</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO7-1" href="#co_advanced_state_management_CO7-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Materialize a windowed store with the default retention period (one day) since this value is not explicitly overridden.</p>
                    </dd>
                  </dl>
                  <p>However, the <code>Materialized</code> class has an additional method, called <code>withRetention</code>, that we can use to specify how long Kafka Streams should keep records in a <span class="keep-together">windowed</span> store.<a data-type="indexterm" data-primary="Materialized.withRetention method" id="idm46281550535112" target="_blank" rel="noopener noreferrer"></a> The following code demonstrates how to specify the retention of a windowed store:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">TimeWindows tumblingWindow =
  TimeWindows.of(Duration.ofSeconds(60)).grace(Duration.ofSeconds(5));

KTable&lt;Windowed&lt;String&gt;, Long&gt; pulseCounts =
  pulseEvents
    .groupByKey()
    .windowedBy(tumblingWindow)
    .count(
      Materialized.&lt;String, Long, WindowStore&lt;Bytes, byte[]&gt;&gt;
          as("pulse-counts")
          .withRetention(Duration.ofHours(6))) <a class="co" id="co_advanced_state_management_CO8-1" href="#callout_advanced_state_management_CO8-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    .suppress(
      Suppressed.untilWindowCloses(BufferConfig.unbounded().shutDownWhenFull()));</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO8-1" href="#co_advanced_state_management_CO8-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Materialize a windowed store with a retention period of six hours.</p>
                    </dd>
                  </dl>
                  <p>Note that the retention period should always be larger than the window size <em>and</em> the grace period combined. In the preceding example, the retention period must be larger than 65 seconds (60 seconds for the tumbling window size + 5 seconds for the grace period). The default window retention period is one day, so lowering this value can reduce the size of your windowed state stores (and their underlying changelog topics) and therefore speed up recovery time.</p>
                  <p>So far, we’ve discussed two methods we can employ in our application code for keeping our state stores small (namely, generating tombstones and setting the retention period for windowed stores). Let’s look at another method for keeping the underlying changelog topics small: aggressive topic compaction.</p>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Aggressive topic compaction">
                <div class="sect3" id="AGGRESSIVE_COMPACTION">
                  <h3>Aggressive topic compaction</h3>
                  <p>By default, changelog topics are compacted.<a data-type="indexterm" data-primary="state stores" data-secondary="controlling size of" data-tertiary="using aggressive topic compaction" id="idm46281550523944" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="topics" data-secondary="aggressive compaction of" id="idm46281550522632" target="_blank" rel="noopener noreferrer"></a> This means that only the latest value for each key is retained and, when tombstones are used, the value for the related key is deleted entirely. However, while the state store will reflect the compacted or deleted value immediately, the underlying topic may remain larger than it needs to be, by retaining the uncompacted/deleted values for a longer period of time.</p>
                  <p>The reason for this behavior is related to how Kafka represents topics on disk. We’ve already talked about how topics are split into partitions, and these partitions translate to a single unit of work in Kafka Streams. However, while partitions are the lowest-level topic abstraction we typically deal with on the application side (we generally think about partitions when we consider how many threads we need, and also when figuring out how data should be routed or co-partitioned with related data), there is an even lower-level abstraction on the Kafka broker side: <em>segments</em>.<a data-type="indexterm" data-primary="segments (topic)" id="idm46281550519768" target="_blank" rel="noopener noreferrer"></a></p>
                  <p>Segments are files that contain a subset of messages for a given topic partition. At any given point in time, there is always an <em>active segment</em>, which is the file that is currently being written to for the underlying partition.<a data-type="indexterm" data-primary="active segments" id="idm46281550518088" target="_blank" rel="noopener noreferrer"></a> Over time, the active segments will reach their size threshold and become inactive. Only once a segment is inactive will it be eligible for cleaning.</p>
                  <div data-type="note" epub:type="note">
                    <h6>Note</h6>
                    <p>Uncompacted records are sometimes referred to as <em>dirty</em>.<a data-type="indexterm" data-primary="dirty records" id="idm46281550515448" target="_blank" rel="noopener noreferrer"></a> The log cleaner is a process that performs compaction on dirty logs, which benefits both the brokers, by increasing available disk space, and the Kafka Streams clients, by reducing the number of records that need to be replayed in order to rebuild a state store.</p>
                  </div>
                  <p>Since the active segment isn’t eligible for cleaning, and could therefore include a large number of uncompacted records and tombstones that would need to be replayed when initializing a state store, it is sometimes beneficial to reduce the segment size in order to enable more aggressive topic compaction.<sup><a data-type="noteref" id="idm46281550513480-marker" href="ch06.html#idm46281550513480" target="_blank" rel="noopener noreferrer">9</a></sup> Furthermore, the log cleaner will also avoid cleaning a log if more than 50% of the log has already been cleaned/compacted. This is also configurable and can be adjusted to increase the frequency at which log cleaning occurs.</p>
                  <p>The topic<a data-type="indexterm" data-primary="topics" data-secondary="aggressive compaction of" data-tertiary="configurations for more frequent log cleaning/compaction" id="idm46281550510696" target="_blank" rel="noopener noreferrer"></a> configurations listed in <a data-type="xref" href="#c6_TOPIC_CONFIGS_1" target="_blank" rel="noopener noreferrer">Table&nbsp;6-1</a> are useful for enabling more aggressive compaction, leading to fewer records that need to be replayed in the event of a state store needing to be reinitialized.<sup><a data-type="noteref" id="idm46281550508248-marker" href="ch06.html#idm46281550508248" target="_blank" rel="noopener noreferrer">10</a></sup></p>
                  <table id="c6_TOPIC_CONFIGS_1">
                    <caption>
                      <span class="label">Table 6-1. </span>Topic configurations that can be used for triggering more frequent log cleaning/compaction
                    </caption>
                    <thead>
                      <tr>
                        <th>Configuration</th>
                        <th>Default</th>
                        <th>Definition</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>
                          <div>
                            <p><code>segment.bytes</code></p>
                          </div></td>
                        <td>
                          <div>
                            <p>1073741824 (1 GB)</p>
                          </div></td>
                        <td>
                          <div>
                            <p>This configuration controls the segment file size for the log. Cleaning is always done a file at a time, so a larger segment size means fewer files but less granular control over retention.</p>
                          </div></td>
                      </tr>
                      <tr>
                        <td>
                          <div>
                            <p><code>segment.ms</code></p>
                          </div></td>
                        <td>
                          <div>
                            <p>604800000 (7 days)</p>
                          </div></td>
                        <td>
                          <div>
                            <p>This configuration controls the period of time after which Kafka will force the log to roll, even if the segment file isn’t full, to ensure older data can be compacted or deleted.</p>
                          </div></td>
                      </tr>
                      <tr>
                        <td>
                          <div>
                            <p><code>min.cleanable.dirty.ratio</code></p>
                          </div></td>
                        <td>
                          <div>
                            <p>0.5</p>
                          </div></td>
                        <td>
                          <div>
                            <p>This configuration controls how frequently the log compactor will attempt to clean the log (assuming log compaction is enabled). By default we will avoid cleaning a log where more than 50% of the log has been compacted. This ratio bounds the maximum space wasted in the log by duplicates (with the default setting, at most, 50% of the log could contain duplicates). A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log. If the <code>max.compaction.lag.ms</code> or the <code>min.compaction.lag.ms</code> configurations are also specified, then the log compactor considers the log to be eligible for compaction as soon as either: (i) the dirty ratio threshold has been met and the log has had dirty (uncompacted) records for at least the <code>min.compaction.lag.ms</code> duration, or (ii) the log has had dirty (uncompacted) records for at most the <code>max.compaction.lag.ms</code> period.</p>
                          </div></td>
                      </tr>
                      <tr>
                        <td>
                          <div>
                            <p><code>max.compaction.lag.ms</code></p>
                          </div></td>
                        <td>
                          <div>
                            <p><code>Long.MAX_VALUE - 1</code></p>
                          </div></td>
                        <td>
                          <div>
                            <p>The maximum time a message will remain ineligible for compaction in the log. Only applicable for logs that are being compacted.</p>
                          </div></td>
                      </tr>
                      <tr>
                        <td>
                          <div>
                            <p><code>min.compaction.lag.ms</code></p>
                          </div></td>
                        <td>
                          <div>
                            <p>0</p>
                          </div></td>
                        <td>
                          <div>
                            <p>The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.</p>
                          </div></td>
                      </tr>
                    </tbody>
                  </table>
                  <p>An example of how to change two of these configs in a materialized store is shown in the following code. These topic configurations could help trigger more frequent log cleaning by reducing the segment size and also the minimum cleanable dirty ratio:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">Map&lt;String, String&gt; topicConfigs = new HashMap&lt;&gt;();
topicConfigs.put("segment.bytes", "536870912"); <a class="co" id="co_advanced_state_management_CO9-1" href="#callout_advanced_state_management_CO9-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
topicConfigs.put("min.cleanable.dirty.ratio", "0.3"); <a class="co" id="co_advanced_state_management_CO9-2" href="#callout_advanced_state_management_CO9-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

StreamsBuilder builder = new StreamsBuilder();
KStream&lt;byte[], String&gt; stream = builder.stream("patient-events");

KTable&lt;byte[], Long&gt; counts =
    stream
        .groupByKey()
        .count(
            Materialized.&lt;byte[], Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as("counts")
                .withKeySerde(Serdes.ByteArray())
                .withValueSerde(Serdes.Long())
                .withLoggingEnabled(topicConfigs));</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO9-1" href="#co_advanced_state_management_CO9-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Reduce the segment size to 512 MB.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO9-2" href="#co_advanced_state_management_CO9-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Reduce the minimum cleanable dirty ratio to 30%.</p>
                    </dd>
                  </dl>
                  <p>Now, topic compaction is needed because the underlying storage medium is theoretically unbounded. Another approach to the problem of minimizing state store size is to instead use a fixed-size data structure. While there are some drawbacks to this approach, Kafka Streams does include a state store that tackles the problem from this angle. We’ll discuss this next.</p>
                </div>
              </section>
              <section data-type="sect3" data-pdf-bookmark="Fixed-size LRU cache">
                <div class="sect3" id="idm46281550525192">
                  <h3>Fixed-size LRU cache</h3>
                  <p>A less common method for ensuring state stores don’t grow indefinitely is to use an in-memory LRU cache.<a data-type="indexterm" data-primary="state stores" data-secondary="controlling size of" data-tertiary="using fixed-size LRU cache" id="idm46281550470456" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="LRU cache, fixed-size" id="idm46281550469368" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="key-value stores" data-secondary="fixed-size LRU cache" id="idm46281550468760" target="_blank" rel="noopener noreferrer"></a> This is a simple key-value store that has a configurable, fixed capacity (specified by the max number of entries) that automatically deletes the least-recently used entry when the state exceeds the configured size. Furthermore, whenever an entry is removed from the in-memory store, a tombstone is automatically sent to the underlying changelog topic as well.</p>
                  <p class="pagebreak-before">An example of how to use an in-memory LRU map is shown here:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">KeyValueBytesStoreSupplier storeSupplier = Stores.lruMap("counts", 10); <a class="co" id="co_advanced_state_management_CO10-1" href="#callout_advanced_state_management_CO10-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

StreamsBuilder builder = new StreamsBuilder();
KStream&lt;String, String&gt; stream = builder.stream("patient-events");

stream
    .groupByKey()
    .count(
        Materialized.&lt;String, Long&gt;as(storeSupplier) <a class="co" id="co_advanced_state_management_CO10-2" href="#callout_advanced_state_management_CO10-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            .withKeySerde(Serdes.String())
            .withValueSerde(Serdes.Long()));

return builder.build();</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <dl class="calloutlist">
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO10-1" href="#co_advanced_state_management_CO10-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Create an in-memory LRU store named <em>counts</em> with a max size of 10 entries.</p>
                    </dd>
                    <dt>
                      <a class="co" id="callout_advanced_state_management_CO10-2" href="#co_advanced_state_management_CO10-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                    </dt>
                    <dd>
                      <p>Materialize the in-memory LRU store using a store supplier.</p>
                    </dd>
                  </dl>
                  <p>To achieve this in the Processor API, you could use a store builder as follows:</p>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">StreamsBuilder builder = new StreamsBuilder();

KeyValueBytesStoreSupplier storeSupplier = Stores.lruMap("counts", 10);

StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; lruStoreBuilder =
  Stores.keyValueStoreBuilder(storeSupplier, Serdes.String(), Serdes.Long());

builder.addStateStore(lruStoreBuilder);</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div>
                  <p>As mentioned in <a data-type="xref" href="#AGGRESSIVE_COMPACTION" target="_blank" rel="noopener noreferrer">“Aggressive topic compaction”</a>, compaction and deletion will not happen immediately in the underlying changelog topic that backs the LRU cache, so in the event of failure, the restore time could be higher than a persistent state store since the entire topic will need to be replayed in order to reinitialize the state store (even if it resolves to only 10 records in the LRU map!). This is a major downside to using in-memory stores that we initially discussed in <a data-type="xref" href="ch04.html#C4_PVM" target="_blank" rel="noopener noreferrer">“Persistent Versus In-Memory Stores”</a>, so this option should be used with a full understanding of the trade-offs.</p>
                  <p>This wraps up our discussion for keeping state stores and their underlying changelog topics free of unneeded records. Now, let’s look at a strategy you can pursue if it appears that your state stores are bottlenecked by either read latency or write volume.<a data-type="indexterm" data-primary="state" data-secondary="advanced management" data-tertiary="reducing impact of rebalances" data-startref="ix_stadvreb" id="idm46281550450744" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="rebalancing" data-secondary="reducing impact of" data-startref="ix_rebred" id="idm46281550449208" target="_blank" rel="noopener noreferrer"></a></p>
                </div>
              </section>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Deduplicating Writes with Record Caches">
        <div class="sect1" id="idm46281550447864">
          <h1>Deduplicating Writes with Record Caches</h1>
          <p>As we discussed in <a data-type="xref" href="ch05.html#C5_SUPPRESS_SECTION" target="_blank" rel="noopener noreferrer">“Suppression”</a>, there are some DSL methods (namely, <code>suppress</code>, in combination with a buffer config; see <a data-type="xref" href="ch05.html#C5_BUFFER_CONFIGS" target="_blank" rel="noopener noreferrer">Table&nbsp;5-2</a>) for rate-limiting updates in a windowed store.<a data-type="indexterm" data-primary="state stores" data-secondary="deduplicating writes with record caches" id="idm46281550443656" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="deduplicating writes with record caches" id="idm46281550442744" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="record caches" data-secondary="deduplicating writes to state stores with" id="idm46281550442056" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="topics" data-secondary="configurations reducing writes to state stores and downstream processors" id="idm46281550441016" target="_blank" rel="noopener noreferrer"></a> We also have an operational parameter<sup><a data-type="noteref" id="idm46281550439976-marker" href="ch06.html#idm46281550439976" target="_blank" rel="noopener noreferrer">11</a></sup> for controlling the frequency with which state updates are written to both the underlying state stores and downstream processors. These parameters are shown in <a data-type="xref" href="#c6_TOPIC_CONFIGS_2" target="_blank" rel="noopener noreferrer">Table&nbsp;6-2</a>.</p>
          <table id="c6_TOPIC_CONFIGS_2">
            <caption>
              <span class="label">Table 6-2. </span>Topic configurations that can be used to reduce writes to state stores and downstream processors
            </caption>
            <thead>
              <tr>
                <th>Raw config</th>
                <th>StreamsConfig property</th>
                <th>Default</th>
                <th>Definition</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div>
                    <p><code>cache.max.bytes.buffering</code></p>
                  </div></td>
                <td>
                  <div>
                    <p><code>C⁠A⁠C⁠H⁠E⁠_⁠M⁠A⁠X⁠_⁠B⁠Y⁠T⁠E⁠S⁠_⁠B⁠U⁠F⁠F⁠E⁠R⁠I⁠N⁠G​_⁠C⁠O⁠N⁠F⁠I⁠G</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>1048576 (10 MB)</p>
                  </div></td>
                <td>
                  <div>
                    <p>The maximum amount of memory, in bytes, to be used for buffering across all threads</p>
                  </div></td>
              </tr>
              <tr>
                <td>
                  <div>
                    <p><code>commit.interval.ms</code></p>
                  </div></td>
                <td>
                  <div>
                    <p><code>COMMIT_INTERVAL_MS_CONFIG</code></p>
                  </div></td>
                <td>
                  <div>
                    <p>30000 (30 seconds)</p>
                  </div></td>
                <td>
                  <div>
                    <p>The frequency with which to save the position of the processor</p>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>A larger cache size and higher commit interval can help deduplicate consecutive updates to the same key. <a data-type="indexterm" data-primary="commit.interval.ms parameter (StreamsConfig)" id="idm46281550422760" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="buffering, larger cache size and higher commit interval" id="idm46281550422056" target="_blank" rel="noopener noreferrer"></a>This has a few benefits, including:</p>
          <ul>
            <li>
              <p>Reducing read latency</p></li>
            <li>
              <p>Reducing write volume to:</p>
              <ul>
                <li>
                  <p>State stores</p></li>
                <li>
                  <p>Their underlying changelog topics (if enabled)</p></li>
                <li>
                  <p>Downstream stream processors</p></li>
              </ul></li>
          </ul>
          <p>Therefore, if your bottleneck appears to be with reading/writing to the state store, or with network I/O (which can be a byproduct of frequent updates to the changelog topic), you should consider adjusting these parameters. Of course, larger record caches do come with a couple of trade-offs:</p>
          <ul class="less-space-list">
            <li>
              <p>Higher memory usage</p></li>
            <li>
              <p>Higher latency (records are emitted less frequently)</p></li>
          </ul>
          <p>Regarding the first point, the total memory that is allocated for the record caches (controlled by the <code>cache.max.bytes.buffering</code> parameter) is shared across all of the streaming threads.<a data-type="indexterm" data-primary="cache.max.bytes.buffering parameter (StreamsConfig)" id="idm46281550411576" target="_blank" rel="noopener noreferrer"></a> The memory pool will be subdivided evenly, so threads that are processing “hot partitions” (i.e., partitions with relatively high data volume compared to others) will flush their cache more frequently. Regardless of the cache size or commit interval, the final stateful computation will be the same.</p>
          <p>There is also a trade-off with using larger commit intervals. Namely, the amount of work that needs to be redone after a failure will increase as you increase the value of this configuration.</p>
          <p>Finally, sometimes it may be desirable to see each intermediate state change without any caching at all. In fact, sometimes people who are new to Kafka Streams will observe the deduplication or the delay in flushing the cache and think something is wrong with their topology, because they produced a certain number of records to their source topic and only saw a subset of the state changes get flushed (possibly after a delay of several seconds, instead of immediately). Therefore, sometimes people will disable the cache entirely and have a smaller commit interval in their development environments. Just be careful of doing this in production because it could impact performance.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="State Store Monitoring">
        <div class="sect1" id="idm46281550447272">
          <h1>State Store Monitoring</h1>
          <p>Before deploying your application to production, it’s important to ensure you have sufficient visibility into your application to properly support it.<a data-type="indexterm" data-primary="state stores" data-secondary="monitoring" id="ix_ststmon" target="_blank" rel="noopener noreferrer"></a> In this section, we will discuss the common approaches for monitoring stateful applications so that you can reduce operational toil and have enough info to debug it when an error occurs.</p>
          <section data-type="sect2" data-pdf-bookmark="Adding State Listeners">
            <div class="sect2" id="c6_ADD_STATE_LISTENERS">
              <h2>Adding State Listeners</h2>
              <p>A Kafka Streams application can be in one of many states (not to be confused with state stores). <a data-type="xref" href="#c6_APP_STATES" target="_blank" rel="noopener noreferrer">Figure&nbsp;6-5</a> shows each of these states and their valid transitions.<a data-type="indexterm" data-primary="state stores" data-secondary="monitoring" data-tertiary="adding State Listeners" id="idm46281550402568" target="_blank" rel="noopener noreferrer"></a></p>
              <p>As mentioned before, the rebalancing state can be especially impactful for stateful Kafka Streams applications, so being able to track when your applications transition to a rebalancing state, and how often this happens, can be useful for monitoring purposes. Luckily for us, Kafka Streams makes it extremely easy to monitor when the application state changes, using something called a <em>State Listener</em>. <a data-type="indexterm" data-primary="State Listeners" id="idm46281550400072" target="_blank" rel="noopener noreferrer"></a>A State Listener is simply a callback method that is invoked whenever the application state changes.</p>
              <p>Depending on your application, you may want to take a certain action when a rebalance occurs. For example, at Mailchimp, we create a special metric in our Kafka Streams applications that gets incremented whenever a rebalance is triggered. The metric is sent to our monitoring system (Prometheus), where it can be queried or even used for creating alerts.</p>
              <figure>
                <div id="c6_APP_STATES" class="figure">
                  <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0605.png" style="width: 13rem">
                  <h6><span class="label">Figure 6-5. </span>Application states and their valid transitions in Kafka Streams</h6>
                </div>
              </figure>
              <p>The following code shows an example of how to add a State Listener to a Kafka Streams topology, which listens specifically for transitions to the rebalancing state:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KafkaStreams streams = new KafkaStreams(...);

streams.setStateListener( <a class="co" id="co_advanced_state_management_CO11-1" href="#callout_advanced_state_management_CO11-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    (oldState, newState) -&gt; { <a class="co" id="co_advanced_state_management_CO11-2" href="#callout_advanced_state_management_CO11-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
      if (newState.equals(State.REBALANCING)) {<a class="co" id="co_advanced_state_management_CO11-3" href="#callout_advanced_state_management_CO11-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
        // do something
      }
    });</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO11-1" href="#co_advanced_state_management_CO11-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Use the <code>KafkaStreams.setStateListener</code> method to invoke a method whenever the application state changes.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO11-2" href="#co_advanced_state_management_CO11-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The method signature for the <code>StateListener</code> class includes both the old state and new state.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO11-3" href="#co_advanced_state_management_CO11-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>Conditionally perform some action whenever the application enters a rebalancing state.</p>
                </dd>
              </dl>
              <p>While State Listeners are very useful, they are not the only listeners we can leverage in our Kafka Streams applications. The next section will discuss another method that can be used for improving visibility of our stateful applications.</p>
            </div>
          </section>
          <section data-type="sect2" data-pdf-bookmark="Adding State Restore Listeners">
            <div class="sect2" id="idm46281550404984">
              <h2>Adding State Restore Listeners</h2>
              <p>In the previous section, we learned how to listen to rebalance triggers in our Kafka Streams application.<a data-type="indexterm" data-primary="state stores" data-secondary="monitoring" data-tertiary="adding State Restore Listeners" id="idm46281550377880" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="State Restore Listeners" id="idm46281550376616" target="_blank" rel="noopener noreferrer"></a> However, rebalances are primarily of interest when they cause a state store to be reinitialized. Kafka Streams includes another listener, called a <em>State Restore Listener</em>, that can be invoked whenever a state store is reinitialized. The <span class="keep-together">following</span> code shows how to add a State Restore Listener to your Kafka Streams <span class="keep-together">application</span>:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">KafkaStreams streams = new KafkaStreams(...);

streams.setGlobalStateRestoreListener(new MyRestoreListener());</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <p>The class <code>MyRestoreListener</code> is an instance of a <code>StateRestoreListener</code>, which we have implemented in the following code block. Unlike the State Listener we built in the previous section, a State Restore Listener requires us to implement three methods, each of which is hooked into part of the life cycle of the state restoration process. The annotations of the following code describe what each method is used for:</p>
              <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                <pre data-type="programlisting">class MyRestoreListener implements StateRestoreListener {

  private static final Logger log =
      LoggerFactory.getLogger(MyRestoreListener.class);

  @Override
  public void onRestoreStart(  <a class="co" id="co_advanced_state_management_CO12-1" href="#callout_advanced_state_management_CO12-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
      TopicPartition topicPartition,
      String storeName,
      long startingOffset,
      long endingOffset) {

    log.info("The following state store is being restored: {}", storeName);

  }

  @Override
  public void onRestoreEnd(  <a class="co" id="co_advanced_state_management_CO12-2" href="#callout_advanced_state_management_CO12-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
      TopicPartition topicPartition,
      String storeName,
      long totalRestored) {

    log.info("Restore complete for the following state store: {}", storeName);

  }

  @Override
  public void onBatchRestored(  <a class="co" id="co_advanced_state_management_CO12-3" href="#callout_advanced_state_management_CO12-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
      TopicPartition topicPartition,
      String storeName,
      long batchEndOffset,
      long numRestored) {

    // this is very noisy. don't log anything

  }
}</pre>
                <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
              </div>
              <dl class="calloutlist">
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO12-1" href="#co_advanced_state_management_CO12-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>onRestoreStart</code> method is invoked at the beginning of a state reinitialization. The <code>startingOffset</code> parameter is of particular interest, since it indicates whether or not the entire state needs to be replayed (this is the most impactful type of reinitialization, and occurs when using in-memory stores, or when using persistent stores and the previous state is lost). If the <code>startingOffset</code> is set to 0, a full reinitialization is required. If it is set to a value greater than 0, then only a partial restore is necessary.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO12-2" href="#co_advanced_state_management_CO12-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>onRestoreEnd</code> method is invoked whenever a restore is completed.</p>
                </dd>
                <dt>
                  <a class="co" id="callout_advanced_state_management_CO12-3" href="#co_advanced_state_management_CO12-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
                </dt>
                <dd>
                  <p>The <code>onBatchRestored</code> method is invoked whenever a single batch of records is restored. The maximum size of a batch is the same as the <code>MAX_POLL_RECORDS</code> config.<a data-type="indexterm" data-primary="MAX_POLL_RECORDS config" id="idm46281550352664" target="_blank" rel="noopener noreferrer"></a> This method could potentially be called a lot of times, so be careful when doing any synchronous processing in this method because it can slow down the restore process. I typically don’t do anything in this method (even logging can be extremely noisy).<a data-type="indexterm" data-primary="state stores" data-secondary="monitoring" data-startref="ix_ststmon" id="idm46281550351528" target="_blank" rel="noopener noreferrer"></a></p>
                </dd>
              </dl>
            </div>
          </section>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Built-in Metrics">
        <div class="sect1" id="idm46281550350056">
          <h1>Built-in Metrics</h1>
          <p>We’ll defer most of our discussion of monitoring Kafka Streams applications to <a data-type="xref" href="ch12.html#ch12" target="_blank" rel="noopener noreferrer">Chapter&nbsp;12</a> (see <a data-type="xref" href="ch12.html#C12_MONITORING" target="_blank" rel="noopener noreferrer">“Monitoring”</a>). However, it’s important to note that Kafka Streams includes a set of built-in JMX metrics, many of which relate to state stores.<a data-type="indexterm" data-primary="JMX metrics" data-secondary="built in, relating to state stores" id="idm46281550346984" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="built-in JMX metrics relating to" id="idm46281550346072" target="_blank" rel="noopener noreferrer"></a></p>
          <p>For example, you can access the rate of certain state store operations and queries (e.g., <code>get</code>, <code>put</code>, <code>delete</code>, <code>all</code>, <code>range</code>), the average and maximum execution time for these operations, and the size of the suppression buffer. There are a number of metrics for RocksDB-backed stores as well, with <code>bytes-written-rate</code> and <code>bytes-read-rate</code> being especially useful when looking at I/O traffic at the byte level.</p>
          <p>A detailed breakdown of these metrics can be found in Confluent’s <a href="https://oreil.ly/vpBn0" target="_blank" rel="noopener noreferrer">monitoring documentation</a>. In practice, I typically use higher-level measures of the application’s health (e.g., consumer lag) for alerting purposes, but it’s nice to have these detailed state store metrics for certain troubleshooting scenarios.</p>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Interactive Queries">
        <div class="sect1" id="idm46281550339544">
          <h1>Interactive Queries</h1>
          <p>Prior to Kafka Streams 2.5, rebalances were especially painful for applications that exposed their state using interactive queries.<a data-type="indexterm" data-primary="rebalancing" data-secondary="and interactive queries to state stores" data-secondary-sortas="interactive" id="idm46281550337528" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state stores" data-secondary="interactive queries" id="idm46281550336440" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="rebalancing" data-secondary="effects on interactive queries to state stores" id="idm46281550335592" target="_blank" rel="noopener noreferrer"></a> In these older versions of the library, offline or rebalancing partitions would cause interactive queries to your state stores to fail. Since even healthy rebalances (e.g., a rolling update) could introduce an availability issue, it was a deal breaker for microservices that require high availability.</p>
          <p>However, starting in <a href="https://oreil.ly/1nY9t" target="_blank" rel="noopener noreferrer">Kafka Streams 2.5</a>, standby replicas can be used to serve stale results while the newly migrated state store is being initialized.<a data-type="indexterm" data-primary="standby replicas" data-secondary="using during rebalancing" id="idm46281550332744" target="_blank" rel="noopener noreferrer"></a> This keeps your API highly available even when your application enters a rebalancing state. Recall that in <a data-type="xref" href="ch04.html#C4_REMOTE_EG" target="_blank" rel="noopener noreferrer">Example&nbsp;4-11</a>, we learned how to retrieve the metadata for a given key in our state store.<a data-type="indexterm" data-primary="metadata" data-secondary="retrieving for key in state store" id="idm46281550330712" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="active Kafka Streams instances" id="idm46281550329752" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="KeyQueryMetadata" id="idm46281550329064" target="_blank" rel="noopener noreferrer"></a> In our initial example, we extracted the <em>active Kafka Streams</em> instance:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KeyQueryMetadata metadata =
  streams.queryMetadataForKey(storeName, key, Serdes.String().serializer());  <a class="co" id="co_advanced_state_management_CO13-1" href="#callout_advanced_state_management_CO13-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

String remoteHost = metadata.activeHost().host(); <a class="co" id="co_advanced_state_management_CO13-2" href="#callout_advanced_state_management_CO13-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
int remotePort = metadata.activeHost().port(); <a class="co" id="co_advanced_state_management_CO13-3" href="#callout_advanced_state_management_CO13-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_advanced_state_management_CO13-1" href="#co_advanced_state_management_CO13-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Get the metadata for the specified key, which includes the host and port pair that a specific key should live on if it exists.<a data-type="indexterm" data-primary="host and port pairs, retrieving for a key" id="idm46281550318136" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO13-2" href="#co_advanced_state_management_CO13-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Extract the hostname of the active Kafka Streams instance.</p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO13-3" href="#co_advanced_state_management_CO13-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Extract the port of the active Kafka Streams instance.</p>
            </dd>
          </dl>
          <p>As of version 2.5, we can retrieve the standby hosts using the following code:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KeyQueryMetadata metadata =
    streams.queryMetadataForKey(storeName, key, Serdes.String().serializer()); <a class="co" id="co_advanced_state_management_CO14-1" href="#callout_advanced_state_management_CO14-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

if (isAlive(metadata.activeHost())) { <a class="co" id="co_advanced_state_management_CO14-2" href="#callout_advanced_state_management_CO14-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  // route the query to the active host
} else {
  // route the query to the standby hosts
  Set&lt;HostInfo&gt; standbys = metadata.standbyHosts(); <a class="co" id="co_advanced_state_management_CO14-3" href="#callout_advanced_state_management_CO14-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_advanced_state_management_CO14-1" href="#co_advanced_state_management_CO14-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Use the <code>KafkaStreams.queryMetadataForKey</code> method to get both the active and standby hosts for a given key.<a data-type="indexterm" data-primary="KafkaStreams.queryMetadataForKey method" id="idm46281550301576" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO14-2" href="#co_advanced_state_management_CO14-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Check to see if the active host is alive. You will need to implement this yourself, but you could potentially add a State Listener (see <a data-type="xref" href="#c6_ADD_STATE_LISTENERS" target="_blank" rel="noopener noreferrer">“Adding State Listeners”</a>) and a corresponding API endpoint in your RPC server to surface the current state of your application.<a data-type="indexterm" data-primary="State Listeners" id="idm46281550297192" target="_blank" rel="noopener noreferrer"></a> <code>isAlive</code> should resolve to true whenever your application is in the <em>Running</em> state.<a data-type="indexterm" data-primary="Running state, applications" id="idm46281550295480" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_advanced_state_management_CO14-3" href="#co_advanced_state_management_CO14-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>If the active host is not alive, retrieve the standby hosts so you can query one of the replicated state stores. Note: if no standbys are configured, then this method will return an empty set.</p>
            </dd>
          </dl>
          <p>As you can see, this ability to query standby replicas ensures our application is highly available, even when the active instance is down or unable to serve queries. This wraps up our discussion of how to mitigate the impact of rebalances. Next, we’ll discuss custom state stores.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Custom State Stores">
        <div class="sect1" id="idm46281550339112">
          <h1>Custom State Stores</h1>
          <p>It’s also possible to implement your own state store.<a data-type="indexterm" data-primary="state stores" data-secondary="custom" id="idm46281550289880" target="_blank" rel="noopener noreferrer"></a> In order to do this, you need to implement the <code>StateStore</code> interface.<a data-type="indexterm" data-primary="StateStore interface, implementing" id="idm46281550288328" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="KeyValueStore" id="idm46281550287624" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="windows" data-secondary="WindowStore interface" id="idm46281550286952" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="SessionStore interface" id="idm46281550286008" target="_blank" rel="noopener noreferrer"></a> You can either implement this directly or, more likely, use one of the higher-level interfaces like <code>KeyValueStore</code>, <code>WindowStore</code>, or <code>SessionStore</code>, which add additional interface methods specific to how the store is intended to be used.<sup><a data-type="noteref" id="idm46281550283704-marker" href="ch06.html#idm46281550283704" target="_blank" rel="noopener noreferrer">12</a></sup></p>
          <p>In addition to the <code>StateStore</code> interface, you will also want to implement the <code>StoreSupplier</code> interface, which will contain the logic for creating new instances of your custom state store.<a data-type="indexterm" data-primary="StoreSupplier interface" id="idm46281550280840" target="_blank" rel="noopener noreferrer"></a> Since the performance characteristics of the built-in RocksDB-based state stores are hard to match, it’s typically not necessary to go through the very tedious and error-prone task of implementing your own custom store. For this reason, and given the sheer amount of code that would be needed just to implement a very basic custom store, we will instead point you to one of the few examples of a custom store <a href="https://oreil.ly/pZf9g" target="_blank" rel="noopener noreferrer">on GitHub</a>.</p>
          <p>Finally, if you do decide to implement a custom store, be aware that any storage solution that requires a network call could potentially greatly impact performance. One of the reasons RocksDB or a local, in-memory store is a good choice is because they are colocated with the streams task. Of course, your mileage will vary based on your project’s requirements, so ultimately, just be sure to define your performance targets up front and choose your state store accordingly.</p>
        </div>
      </section>
      <section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Summary">
        <div class="sect1" id="idm46281550277720">
          <h1>Summary</h1>
          <p>You should now have a deeper understanding of how state stores are internally managed by Kafka Streams, and what options are available to you, as the developer, for ensuring your stateful applications run smoothly over time. This includes using tombstones, aggressive topic compaction, and other techniques for removing old data from state stores (and therefore reducing state reinitialization time). Also, by using standby replicas, you can reduce failover time for stateful tasks and also keep your application highly available when a rebalance occurs. Finally, rebalancing, while impactful, can be avoided to some extent using static membership, and the impact can be minimized by using a version of Kafka Streams that supports an improved rebalance protocol called incremental cooperative rebalancing.<a data-type="indexterm" data-primary="state" data-secondary="advanced management" data-startref="ix_stadv" id="idm46281550275336" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <div data-type="footnotes">
        <p data-type="footnote" id="idm46281561065496"><sup><a href="ch06.html#idm46281561065496-marker" target="_blank" rel="noopener noreferrer">1</a></sup> Note: you should never attempt to modify the files.</p>
        <p data-type="footnote" id="idm46281561017672"><sup><a href="ch06.html#idm46281561017672-marker" target="_blank" rel="noopener noreferrer">2</a></sup> A dedicated consumer called the <em>restore consumer</em> is used to replay the changelog topic when a state store needs to be reinitialized.</p>
        <p data-type="footnote" id="idm46281550684552"><sup><a href="ch06.html#idm46281550684552-marker" target="_blank" rel="noopener noreferrer">3</a></sup> There is a ticket for allowing internal topics to be reconfigured, which you can track at <a href="https://oreil.ly/OoKBV" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://oreil.ly/OoKBV</em></a>.</p>
        <p data-type="footnote" id="idm46281550653816"><sup><a href="ch06.html#idm46281550653816-marker" target="_blank" rel="noopener noreferrer">4</a></sup> Adding or removing partitions from the source topics could also trigger a rebalance.</p>
        <p data-type="footnote" id="idm46281550636968"><sup><a href="ch06.html#idm46281550636968-marker" target="_blank" rel="noopener noreferrer">5</a></sup> The internal class is called <code>StickyTaskAssignor</code>. Note that it is not possible to override the default partitioner in Kafka Streams.</p>
        <p data-type="footnote" id="idm46281550609272"><sup><a href="ch06.html#idm46281550609272-marker" target="_blank" rel="noopener noreferrer">6</a></sup> See the <code>session.timeout.ms</code> consumer configuration.</p>
        <p data-type="footnote" id="idm46281550578376"><sup><a href="ch06.html#idm46281550578376-marker" target="_blank" rel="noopener noreferrer">7</a></sup> Versions &gt;= 2.4 use the improved rebalancing strategy. More information about incremental cooperative rebalancing can be found at <a href="https://oreil.ly/P3iVG" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://oreil.ly/P3iVG</em></a>.</p>
        <p data-type="footnote" id="idm46281550569368"><sup><a href="ch06.html#idm46281550569368-marker" target="_blank" rel="noopener noreferrer">8</a></sup> It’s unlikely the keyspace would be exactly evenly split in a real-world scenario, but it’s convenient for this discussion and the point remains the same.</p>
        <p data-type="footnote" id="idm46281550513480"><sup><a href="ch06.html#idm46281550513480-marker" target="_blank" rel="noopener noreferrer">9</a></sup> For more on this, see <a href="https://oreil.ly/ZR4A6" target="_blank" rel="noopener noreferrer">“Achieving High Availability with Stateful Kafka Streams Applications” by Levani <span class="keep-together">Kokhreidze</span></a>.</p>
        <p data-type="footnote" id="idm46281550508248"><sup><a href="ch06.html#idm46281550508248-marker" target="_blank" rel="noopener noreferrer">10</a></sup> Configuration definitions come from the official Kafka documentation.</p>
        <p data-type="footnote" id="idm46281550439976"><sup><a href="ch06.html#idm46281550439976-marker" target="_blank" rel="noopener noreferrer">11</a></sup> The distinction between this operational parameter and a business logic type of approach that <code>suppress</code> offers is discussed in “Watermarks, Tables, Event Time, and the Dataflow Model” by Eno Thereska et al. on the <a href="https://oreil.ly/MTN-R" target="_blank" rel="noopener noreferrer">Confluent blog</a>.</p>
        <p data-type="footnote" id="idm46281550283704"><sup><a href="ch06.html#idm46281550283704-marker" target="_blank" rel="noopener noreferrer">12</a></sup> For example, the <code>KeyValueStore</code> interface adds the <code>void put(K key, V value)</code> method, among others, since it knows the underlying store will need to write key-value pairs to the underlying storage engine.</p>
      </div>
    </div>
    <div class="chapter" id="ch7">
      <h1><span class="label">Chapter 7. </span>Processor API</h1>
      <p>Just a few chapters ago, we embarked on our journey to learn about Kafka Streams.<a data-type="indexterm" data-primary="Processor API" id="ix_PrAPI" target="_blank" rel="noopener noreferrer"></a> We started with Kafka Streams’ high-level DSL, which allows us to build stream processing applications using a functional and fluent interface.<a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="Kafka Streams high-level DSL" id="idm46281550270264" target="_blank" rel="noopener noreferrer"></a> This involves composing and chaining stream processing functions using the library’s built-in operators (e.g., <code>filter</code>, <code>flatMap</code>, <code>groupBy</code>, etc.) and abstractions (<code>KStream</code>, <code>KTable</code>, <code>GlobalKTable</code>).</p>
      <p>In this chapter, we will explore a lower-level API that is available in Kafka Streams: the Processor API (sometimes called PAPI).<a data-type="indexterm" data-primary="PAPI" data-see="Processor API" id="idm46281550266072" target="_blank" rel="noopener noreferrer"></a> The Processor API has fewer abstractions than the high-level DSL and uses an imperative style of programming. While the code is generally more verbose, it is also more powerful, giving us fine-grained control over the following: how data flows through our topologies, how stream processors relate to each other, how state is created and maintained, and even the timing of certain operations.</p>
      <p>Some of the questions we will answer in this chapter include:</p>
      <ul>
        <li>
          <p>When should you use the Processor API?</p></li>
        <li>
          <p>How do you add source, sink, and stream processors using the Processor API?</p></li>
        <li>
          <p>How can you schedule periodic functions?</p></li>
        <li>
          <p>Is it possible to mix the Processor API with the higher-level DSL?</p></li>
        <li>
          <p>What is the difference between processors and transformers?</p></li>
      </ul>
      <p>As usual, we will demonstrate the fundamentals of the API through a tutorial, answering the preceding questions along the way. However, before we show you <em>how</em> to use the Processor API, let’s first discuss <em>when</em> to use it.</p>
      <section data-type="sect1" data-pdf-bookmark="When to Use the Processor API">
        <div class="sect1" id="idm46281550257256">
          <h1>When to Use the Processor API</h1>
          <p>Deciding which abstraction level to use for your stream processing application is important.<a data-type="indexterm" data-primary="Processor API" data-secondary="deciding when to use" id="idm46281550255640" target="_blank" rel="noopener noreferrer"></a> In general, whenever you introduce complexity into a project, you should have a good reason for doing so. While the Processor API isn’t unnecessarily complex, its low-level nature (compared to the DSL and ksqlDB) and fewer abstractions can lead to more code and, if you’re not careful, more mistakes.</p>
          <p>In general, you may want to utilize the Processor API if you need to take advantage of any of the following:</p>
          <ul>
            <li>
              <p>Access to record metadata (topic, partition, offset information, record headers, and so on)</p></li>
            <li>
              <p>Ability to schedule periodic functions</p></li>
            <li>
              <p>More fine-grained control over when records get forwarded to downstream <span class="keep-together">processors</span></p></li>
            <li>
              <p>More granular access to state stores</p></li>
            <li>
              <p>Ability to circumvent any limitations you come across in the DSL (we’ll see an example of this later)</p></li>
          </ul>
          <p>On the other hand, using the Processor API can<a data-type="indexterm" data-primary="Processor API" data-secondary="disadvantages of" id="idm46281550247416" target="_blank" rel="noopener noreferrer"></a> come with some disadvantages, including:</p>
          <ul>
            <li>
              <p>More verbose code, which can lead to higher maintenance costs and impair <span class="keep-together">readability</span></p></li>
            <li>
              <p>A higher barrier to entry for other project maintainers</p></li>
            <li>
              <p>More footguns, including accidental reinvention of DSL features or abstractions, exotic problem-framing,<sup><a data-type="noteref" id="idm46281550242520-marker" href="ch07.html#idm46281550242520" target="_blank" rel="noopener noreferrer">1</a></sup> and performance traps<sup><a data-type="noteref" id="idm46281550241560-marker" href="ch07.html#idm46281550241560" target="_blank" rel="noopener noreferrer">2</a></sup></p></li>
          </ul>
          <p>Fortunately, Kafka Streams allows us to mix both the DSL and Processor API in an application, so you don’t need to go all in on either choice. You can use the DSL for simpler and more standard operations, and the Processor API for more complex or unique functions that require lower-level access to the processing context, state, or record metadata. We will discuss how to combine the DSL and Processor API at the end of this chapter, but initially, we will see how to implement an application using <em>only</em> the Processor API. So without further ado, let’s take a look at the application we’ll be building in this chapter.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Introducing Our Tutorial: IoT Digital Twin Service">
        <div class="sect1" id="idm46281550239048">
          <h1>Introducing Our Tutorial: IoT Digital Twin Service</h1>
          <p>In this tutorial, we will use the Processor API to build a <em>digital twin</em> service for an offshore wind farm.<a data-type="indexterm" data-primary="digital twins" data-seealso="IoT digital twin service tutorial" id="idm46281550236632" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="introduction to" id="ix_PrAPIIoTdts" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="introduction to" id="ix_IoTDT" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IIoT (industrial IoT)" id="idm46281550233000" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT (Internet of Things)" id="idm46281550232328" target="_blank" rel="noopener noreferrer"></a> Digital twins (sometimes called device shadows) are popular in both IoT (Internet of Things) and IIoT (industrial IoT) use cases,<sup><a data-type="noteref" id="idm46281550231368-marker" href="ch07.html#idm46281550231368" target="_blank" rel="noopener noreferrer">3</a></sup> in which the state of a physical object is mirrored in a digital copy. This is a great use case for Kafka Streams, which can easily ingest and process high-volume sensor data, capture the state of a physical object using state stores, and subsequently expose this state using interactive queries.</p>
          <p>To give you a quick example of what a digital twin is (this will make our tutorial a little clearer), consider the following. We have a wind farm with 40 wind turbines. Whenever one of the turbines reports its current state (wind speed, temperature, power status, etc.), we save that information in a key-value store.<a data-type="indexterm" data-primary="reported and desired state on digital twins" id="idm46281550228248" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="state" data-secondary="reported and desired state on digital twin" id="idm46281550227448" target="_blank" rel="noopener noreferrer"></a> An example of a <em>reported state</em> record value is shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">{
  "timestamp": "2020-11-23T09:02:00.000Z",
  "wind_speed_mph": 40,
  "temperature_fahrenheit": 60,
  "power": "ON"
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Note that a device ID is communicated via the record key (e.g., the preceding value may correspond to a device with an ID of <code>abc123</code>). This will allow us to distinguish the reported/desired state events of one device from another.</p>
          <p>Now, if we want to interact with a particular wind turbine,<sup><a data-type="noteref" id="idm46281550223608-marker" href="ch07.html#idm46281550223608" target="_blank" rel="noopener noreferrer">4</a></sup> we don’t do so directly. IoT devices can and do frequently go offline, so we can achieve higher availability and reduce errors if we instead only interact with the digital copy (twin) of a physical device.</p>
          <p class="pagebreak-before">For example, if we want to set the power state from <code>ON</code> to <code>OFF</code>, instead of sending that signal to the turbine directly, we would set the so-called <em>desired state</em> on the digital copy.<a data-type="indexterm" data-primary="desired state (on digital twin)" id="idm46281550220312" target="_blank" rel="noopener noreferrer"></a> The physical turbine would subsequently synchronize its state (i.e., disable power to the blades) whenever it comes online, and usually at set intervals, thereafter. Therefore, a digital twin record will include both a <em>reported</em> and <em>desired</em> state, and we will create and expose digital twin records like the following using Kafka Streams’ Processor API:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">{
  "desired": {
    "timestamp": "2020-11-23T09:02:01.000Z",
    "power": "OFF"
  },
  "reported": {
    "timestamp": "2020-11-23T09:00:01.000Z",
    "windSpeedMph": 68,
    "power": "ON"
  }
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>With this in mind, our application needs to ingest a stream of sensor data from a set of wind turbines,<sup><a data-type="noteref" id="idm46281550216936-marker" href="ch07.html#idm46281550216936" target="_blank" rel="noopener noreferrer">5</a></sup> perform some minor processing on the data, and maintain the latest state of each wind turbine in a persistent key-value state store. We will then expose the data via Kafka Streams’ interactive queries feature.</p>
          <p>Though we will avoid much of the technical detail around interactive queries since this has already been covered in previous chapters, we will present some additional value statements of interactive queries that IoT use cases afford.</p>
          <p><a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a> shows the topology we will be building in this chapter. Each step is detailed after the diagram.</p>
          <figure>
            <div id="C7_TOPOLOGY" class="figure">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/mksk_0701.png" style="width: 30rem">
              <h6><span class="label">Figure 7-1. </span>The topology that we will be implementing for our IoT digital twin service</h6>
            </div>
          </figure>
          <dl class="calloutlist">
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12">
            </dt>
            <dd>
              <p>Our Kafka cluster contains two topics, and therefore we need to learn how to add source processors using the Processor API. Here is a description of these topics:</p>
              <ul>
                <li>
                  <p>Each wind turbine (edge node) is outfitted with a set of environmental sensors, and this data (e.g., wind speed), along with some metadata about the turbine itself (e.g., power state), is sent to the <code>reported-state-events</code> topic periodically.</p></li>
                <li>
                  <p>The <code>desired-state-events</code> topic is written to whenever a user or process wants to change the power state of a turbine (i.e., turn it off or on).</p></li>
              </ul>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12">
            </dt>
            <dd>
              <p>Since the environmental sensor data is reported in the <code>reported-state-events</code> topic, we will add a stream processor that determines whether or not the reported wind speed for a given turbine exceeds safe operating levels,<sup><a data-type="noteref" id="idm46281550203368-marker" href="ch07.html#idm46281550203368" target="_blank" rel="noopener noreferrer">6</a></sup> and if it does, we will automatically generate a shutdown signal. This will teach you how to add a stateless stream processor using the Processor API.</p>
            </dd>
            <dt class="pagebreak-before">
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12">
            </dt>
            <dd>
              <p>The third step is broken into two parts:</p>
              <ul>
                <li>
                  <p>First, both types of events (reported and desired) will be combined into a so-called digital twin record. These records will be processed and then written to a persistent key-value store called <code>digital-twin-store</code>. In this step, you will learn how to connect to and interact with state stores using the Processor API, and also how to access certain record metadata that isn’t accessible via the DSL.</p></li>
                <li>
                  <p>The second part of this step involves scheduling a periodic function, called a <em>punctuator</em>, to clean out old digital twin records that haven’t seen an update in more than seven days. This will introduce you to the Processor API’s punctuation interface, and also demonstrate an alternative method for removing keys from state stores.<sup><a data-type="noteref" id="idm46281550197448-marker" href="ch07.html#idm46281550197448" target="_blank" rel="noopener noreferrer">7</a></sup></p></li>
              </ul>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12">
            </dt>
            <dd>
              <p>Each digital twin record will be written to an output topic called <code>digital-twins</code> for analytical purposes. In this step, you will learn how to add sink processors using the Processor API.</p>
            </dd>
            <dt>
              <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12">
            </dt>
            <dd>
              <p>We will expose the digital twin records via Kafka Streams’ interactive queries feature. Every few seconds, the microcontroller on the wind turbine will attempt to synchronize its own state with the desired state exposed by Kafka Streams. For example, if we generate a shutdown signal in step 2 (which would set the desired power state to <code>OFF</code>), then the turbine would see this desired state when it queries our Kafka Streams app, and kill power to the blades.</p>
            </dd>
          </dl>
          <p>Now that we understand what we’ll be building (and what we’ll learn at each step), let’s set up our project.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-startref="ix_PrAPIIoTdts" id="idm46281550190584" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="introduction to" data-startref="ix_IoTDT" id="idm46281550189384" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Project Setup">
        <div class="sect1" id="idm46281550238456">
          <h1>Project Setup</h1>
          <p>The code for this chapter is<a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="project setup" id="idm46281550186744" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="project setup" id="idm46281550185704" target="_blank" rel="noopener noreferrer"></a> located at <a href="https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb.git</em></a>.</p>
          <p>If you would like to reference the code as we work our way through each topology step, clone the repo and change to the directory containing this chapter’s tutorial. The following command will do the trick:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ git clone git@github.com:mitch-seymour/mastering-kafka-streams-and-ksqldb.git
$ cd mastering-kafka-streams-and-ksqldb/chapter-07/digital-twin</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p class="pagebreak-before">You can build the project anytime by running the following command:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ ./gradlew build --info</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>With our project set up, let’s start implementing our digital twin application.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Data Models">
        <div class="sect1" id="idm46281550179608">
          <h1>Data Models</h1>
          <p>As usual, before we start creating our topology, we’ll first define our data models.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="defining data models" id="ix_PrAPIIoTdtsDM" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="defining data models" id="ix_IoTDTSDM" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="data models" data-secondary="defining for IoT digital twin service tutorial" id="ix_DMIoTDS" target="_blank" rel="noopener noreferrer"></a> The example records and class definition shown in <a data-type="xref" href="#C7_DATA_CLASSES" target="_blank" rel="noopener noreferrer">Table&nbsp;7-1</a> correspond to the data in our input topics (see step 1 in our processor topology: <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>).<a data-type="indexterm" data-primary="JSON" data-secondary="in IoT digital twin service data models" id="idm46281550172184" target="_blank" rel="noopener noreferrer"></a></p>
          <p>Note that the data coming through both of the input topics is formatted as JSON <span class="keep-together">for simplicity,</span> and both types of records are represented using a common class: <span class="keep-together"><code>TurbineState</code></span>. We have omitted the accessor functions in the <code>TurbineState</code> class for brevity’s sake.</p>
          <table class="border" id="C7_DATA_CLASSES">
            <caption>
              <span class="label">Table 7-1. </span>Example records and data classes for each source topic
            </caption>
            <thead>
              <tr>
                <th>Kafka topic</th>
                <th>Example record</th>
                <th>Data class</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><span class="keep-together"><code>reported-state-events</code></span></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
{
  "timestamp": "...",
  "wind_speed_mph": 40,
  "power": "ON"
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
public class TurbineState {
  private String timestamp;
  private Double windSpeedMph;

  public enum Power { ON, OFF }

  public enum Type { DESIRED, REPORTED }

  private Power power;
  private Type type;
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
              </tr>
              <tr>
                <td><span class="keep-together"><code>desired-state-events</code></span></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
{
  "timestamp": "...",
  "power": "OFF"
}</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
                <td>
                  <p>Same as the data class for <code>reported-state</code></p></td>
              </tr>
            </tbody>
          </table>
          <p>As mentioned in the tutorial overview, we need to combine the reported and desired state records to create a digital twin record. Therefore, we also need a data class for the combined record. The following table shows the JSON structure of the combined digital twin record, as well as the corresponding data class:</p>
          <table id="C7_DATA_CLASSES2">
            <thead>
              <tr>
                <th>Example record</th>
                <th>Data class</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">{
  "desired": {
    "timestamp": "2020-11-23T09:02:01.000Z",
    "power": "OFF"
  },
  "reported": {
    "timestamp": "2020-11-23T09:00:01.000Z",
    "windSpeedMph": 68,
    "power": "ON"
  }
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
                <td>
                  <div>
                    <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                      <pre data-type="programlisting">public class DigitalTwin {
  private TurbineState desired;
  private TurbineState reported;

  // getters and setters omitted for
  // brevity
}</pre>
                      <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                    </div>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>As you can see from the example record, the desired state shows the turbine powered off, but the last reported state shows that the power is on. The turbine will eventually synchronize its state with the digital twin and power the blades off.</p>
          <p>Now, at this point, you may be wondering how record serialization and deserialization work in the Processor API compared to the high-level DSL.<a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="record serialization in Processor API" id="idm46281550149176" target="_blank" rel="noopener noreferrer"></a> In other words, how do we actually convert the raw record bytes in our Kafka topics into the data classes shown in <a data-type="xref" href="#C7_DATA_CLASSES" target="_blank" rel="noopener noreferrer">Table&nbsp;7-1</a>? In <a data-type="xref" href="ch03.html#C3_SERDES_SECTION" target="_blank" rel="noopener noreferrer">“Serialization/Deserialization”</a>, we talked about using Serdes classes in the DSL, which are wrapper classes that contain both a serializer and deserializer.<a data-type="indexterm" data-primary="operators" data-secondary="DSL, using to specify a Serdes" id="idm46281550146136" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="operators specifying Serdes instance" id="idm46281550145176" target="_blank" rel="noopener noreferrer"></a> Many DSL operators, like <code>stream</code>, <code>table</code>, <code>join</code>, etc., allow you to specify a Serdes instance, so they are commonplace in applications that use the DSL.</p>
          <p>In the Processor API, the various API methods only require the underlying serializer or deserializer <a data-type="indexterm" data-primary="Serdes classes" data-secondary="for digital twin and turbine state records" data-secondary-sortas="digital" id="idm46281550142296" target="_blank" rel="noopener noreferrer"></a>that a Serdes instance would typically contain. However, it is still often convenient to define a Serdes for your data classes, since 1) you can always extract the underlying serializer/deserializer to satisfy a Processor API method signature, and 2) Serdes are often useful for testing purposes.</p>
          <p>With that said, for this tutorial, we will leverage<a data-type="indexterm" data-primary="JsonSerdes class (example)" id="idm46281550140088" target="_blank" rel="noopener noreferrer"></a> the Serdes classes shown in <a data-type="xref" href="#C7_SERDES" target="_blank" rel="noopener noreferrer">Example&nbsp;7-1</a>.</p>
          <div id="C7_SERDES" data-type="example">
            <h5><span class="label">Example 7-1. </span>Serdes for our digital twin and turbine state records</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">public class JsonSerdes {

  public static Serde&lt;DigitalTwin&gt; DigitalTwin() { <a class="co" id="co_processor_api_CO1-1" href="#callout_processor_api_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    JsonSerializer&lt;DigitalTwin&gt; serializer = new JsonSerializer&lt;&gt;();
    JsonDeserializer&lt;DigitalTwin&gt; deserializer =
      new JsonDeserializer&lt;&gt;(DigitalTwin.class);

    return Serdes.serdeFrom(serializer, deserializer);
  }

  public static Serde&lt;TurbineState&gt; TurbineState() { <a class="co" id="co_processor_api_CO1-2" href="#callout_processor_api_CO1-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    JsonSerializer&lt;TurbineState&gt; serializer = new JsonSerializer&lt;&gt;();
    JsonDeserializer&lt;TurbineState&gt; deserializer =
      new JsonDeserializer&lt;&gt;(TurbineState.class);

    return Serdes.serdeFrom(serializer, deserializer); <a class="co" id="co_processor_api_CO1-3" href="#callout_processor_api_CO1-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
  }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO1-1" href="#co_processor_api_CO1-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>A method for retrieving a <code>DigitalTwin</code> Serdes.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO1-2" href="#co_processor_api_CO1-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>A method for retrieving a <code>TurbineState</code> Serdes.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO1-3" href="#co_processor_api_CO1-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>In previous tutorials, we implemented the <code>Serde</code> interface directly (see <a data-type="xref" href="ch03.html#C3_TWEET_SERDES" target="_blank" rel="noopener noreferrer">“Building the Tweet Serdes”</a> for an example). This shows an alternative approach, which is to use the <code>Serdes.serdeFrom</code> method in Kafka Streams to construct a Serdes from a serializer and deserializer instance.<a data-type="indexterm" data-primary="Serdes.serdeFrom method" id="idm46281550119544" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
          </dl>
          <p>In the next section, we’ll learn how to add source processors and deserialize input records using the Processor API.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="defining data models" data-startref="ix_PrAPIIoTdtsDM" id="idm46281550118328" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="defining data models" data-startref="ix_IoTDTSDM" id="idm46281550116808" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="data models" data-secondary="defining for IoT digital twin service tutorial" data-startref="ix_DMIoTDS" id="idm46281550115576" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Adding Source Processors">
        <div class="sect1" id="idm46281550179016">
          <h1>Adding Source Processors</h1>
          <p>Now that we have defined our data classes, we are ready to tackle step 1 of our processor topology (see <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>).<a data-type="indexterm" data-primary="source processors" data-secondary="adding to IoT digital twin service tutorial" id="idm46281550111896" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="adding source processors" id="idm46281550110888" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="adding source processors" id="idm46281550109912" target="_blank" rel="noopener noreferrer"></a> This involves adding two source processors, which will allow us to stream data from our input topics into our Kafka Streams application. <a data-type="xref" href="#C7_ADD_SOURCE" target="_blank" rel="noopener noreferrer">Example&nbsp;7-2</a> shows how we can accomplish this using the Processor API.</p>
          <div id="C7_ADD_SOURCE" data-type="example">
            <h5><span class="label">Example 7-2. </span>The initial topology with both of our source processors added</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">Topology builder = new Topology(); <a class="co" id="co_processor_api_CO2-1" href="#callout_processor_api_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

builder.addSource(<a class="co" id="co_processor_api_CO2-2" href="#callout_processor_api_CO2-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    "Desired State Events", <a class="co" id="co_processor_api_CO2-3" href="#callout_processor_api_CO2-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    Serdes.String().deserializer(), <a class="co" id="co_processor_api_CO2-4" href="#callout_processor_api_CO2-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
    JsonSerdes.TurbineState().deserializer(), <a class="co" id="co_processor_api_CO2-5" href="#callout_processor_api_CO2-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
    "desired-state-events"); <a class="co" id="co_processor_api_CO2-6" href="#callout_processor_api_CO2-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>

builder.addSource(<a class="co" id="co_processor_api_CO2-7" href="#callout_processor_api_CO2-7"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
    "Reported State Events",
    Serdes.String().deserializer(),
    JsonSerdes.TurbineState().deserializer(),
    "reported-state-events");</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO2-1" href="#co_processor_api_CO2-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Instantiate a <code>Topology</code> instance directly.<a data-type="indexterm" data-primary="Topology instance" data-secondary="instantiating directly" id="idm46281550088520" target="_blank" rel="noopener noreferrer"></a> This is what we will use to add and connect source, sink, and stream processors. Note: instantiating a <code>Topology</code> directly is different than how<a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="building a Topology instance" id="idm46281550086872" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="StreamsBuilder.build method" id="idm46281550085864" target="_blank" rel="noopener noreferrer"></a> we work with the DSL, which requires us to instantiate a <code>StreamsBuilder</code> object, add our DSL operators (e.g., <code>map</code>, <code>flatMap</code>, <code>merge</code>, <code>branch</code>, etc.) to the <code>StreamsBuilder</code> instance, and ultimately build a <code>Topology</code> instance using the <code>StreamsBuilder#build</code> method.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO2-2" href="#co_processor_api_CO2-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Use the <code>addSource</code> method to create a source processor. There are many overloaded versions of this method, including variations that support offset reset strategies, topic patterns, and more. So check out the Kafka Streams Javadocs or navigate to the Topology class using your IDE and choose the <code>addSource</code> variation that best fits your needs.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO2-3" href="#co_processor_api_CO2-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of the source processor. Each processor must have a unique name, since under the hood, Kafka Streams stores these names in a topologically sorted map (and therefore, each key must be unique). As we’ll see shortly, names are important in the Processor API since they are used to connect child processors. Again, this is very different from how the DSL makes connections, which doesn’t need an explicit name to define the relationship between processors (by default, the DSL generates an internal name for you). It’s advisable to use a descriptive name here to improve the readability of your code.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO2-4" href="#co_processor_api_CO2-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The key deserializer.<a data-type="indexterm" data-primary="deserialization" data-secondary="key and value deserializers for IoT digital twin service" id="idm46281550072504" target="_blank" rel="noopener noreferrer"></a> Here, we use the built-in <code>String</code> deserializer since our keys are formatted as strings. This is another difference between the Processor API and the DSL. The latter requires us to pass in a Serdes (an object that contains both a record serializer and deserializer), while the Processor API only requires the underlying deserializer (which can be extracted directly from the Serdes, as we have done here).</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO2-5" href="#co_processor_api_CO2-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The value deserializer. Here, we use a custom Serdes (found in the source code of this tutorial) to convert the record values into a <code>TurbineState</code> object. The additional notes about the key deserializer also apply here.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO2-6" href="#co_processor_api_CO2-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of the topic this source processor consumes from.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO2-7" href="#co_processor_api_CO2-7" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Add a second source processor for the <code>reported-state-events</code> topic. We won’t go through each parameter again since the parameter types match the previous source processor.</p>
            </dd>
          </dl>
          <p>One thing to note about the preceding example is that you will see no mention of a stream or table.<a data-type="indexterm" data-primary="streams" data-secondary="nonexistent in Processor API" id="idm46281550061560" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="tables" data-secondary="nonexistent in Processor API" id="idm46281550060584" target="_blank" rel="noopener noreferrer"></a> These abstractions do not exist in the Processor API. However, conceptually speaking, both source processors we have added in the preceding code represent a stream. This is because the processors are not stateful (i.e., they are not connected to a state store) and therefore have no way of remembering the latest state/representation of a given key.</p>
          <p class="pagebreak-before">We will see a table-like representation of our stream when we get to step 3 of our processor topology, but this section completes step 1 of our topology. Now, let’s see how to add a stream processor for generating shutdown signals when our wind turbine is reporting dangerous wind speeds.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Adding Stateless Stream Processors">
        <div class="sect1" id="C7_STATELESS">
          <h1>Adding Stateless Stream Processors</h1>
          <p>The next step in our processor topology requires us to automatically generate a shutdown signal whenever the wind speed recorded by a given turbine exceeds safe operating levels (65 mph).<a data-type="indexterm" data-primary="stateless processors" data-secondary="adding to IoT digital twin service" id="idm46281550056088" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="adding stateless stream processors" id="idm46281550055096" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="adding stateless stream processors" id="idm46281550053848" target="_blank" rel="noopener noreferrer"></a> In order to do this, we need to learn how to add a stream processor using the Processor API. The API method we can use for this purpose is called <code>addProcessor</code>, and an example of how to use this method is shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">builder.addProcessor(
    "High Winds Flatmap Processor", <a class="co" id="co_processor_api_CO3-1" href="#callout_processor_api_CO3-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    HighWindsFlatmapProcessor::new, <a class="co" id="co_processor_api_CO3-2" href="#callout_processor_api_CO3-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    "Reported State Events"); <a class="co" id="co_processor_api_CO3-3" href="#callout_processor_api_CO3-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO3-1" href="#co_processor_api_CO3-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of this stream processor.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO3-2" href="#co_processor_api_CO3-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The second argument expects us to provide a <code>ProcessSupplier</code>, which is a functional interface that returns a <code>Processor</code> instance.<a data-type="indexterm" data-primary="ProcessSupplier interface" id="idm46281550039144" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor interface" id="idm46281550038440" target="_blank" rel="noopener noreferrer"></a> <code>Processor</code> instances contain all of the data processing/transformation logic for a given stream processor. In the next section, we will define a class called <code>HighWindsFlatmapProcessor</code>, which will implement the <code>Processor</code> interface. Therefore, we can simply use a method reference for that class’s constructor.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO3-3" href="#co_processor_api_CO3-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Names of the parent processors. In this case, we only have one parent processor, which is the <code>Reported State Events</code> processor that we created in <a data-type="xref" href="#C7_ADD_SOURCE" target="_blank" rel="noopener noreferrer">Example&nbsp;7-2</a>. Stream processors can be connected to one or more parent nodes.</p>
            </dd>
          </dl>
          <p>Whenever you add a stream processor, you need to implement the <code>Processor</code> interface. This isn’t a functional interface (unlike <code>ProcessSupplier</code>, which we just discussed), so you can’t just pass in a lambda expression to the <code>addProcessor</code> method, like we often do with DSL operators. It’s a little more involved and requires more code than we may be accustomed to, but we’ll walk through how to do this in the next section.</p>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Creating Stateless Processors">
        <div class="sect1" id="C7_STATELESS_PROCESSORS">
          <h1>Creating Stateless Processors</h1>
          <p>Whenever we use the <code>addProcessor</code> method in the Processor API, we need to implement the <code>Processor</code> interface, which will contain the logic for processing and transforming records in a stream.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="creating stateless processors" id="ix_PrAPIIoTdtsstl" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="creating stateless processors" id="ix_IoTDTScrstlpr" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processors" data-secondary="creating" id="ix_stlprccr" target="_blank" rel="noopener noreferrer"></a> The interface has three methods, as shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">public interface Processor&lt;K, V&gt; { <a class="co" id="co_processor_api_CO4-1" href="#callout_processor_api_CO4-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

    void init(ProcessorContext context); <a class="co" id="co_processor_api_CO4-2" href="#callout_processor_api_CO4-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

    void process(K key, V value); <a class="co" id="co_processor_api_CO4-3" href="#callout_processor_api_CO4-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>

    void close(); <a class="co" id="co_processor_api_CO4-4" href="#callout_processor_api_CO4-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO4-1" href="#co_processor_api_CO4-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Notice that the <code>Processor</code> interface specifies two generics: one for the key type (<code>K</code>) and one for the value type (<code>V</code>). We will see how to leverage these generics when we implement our own processor later in this section.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO4-2" href="#co_processor_api_CO4-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>init</code> method is called when the <code>Processor</code> is first instantiated. If your processor needs to perform any initialization tasks, you can specify the initialization logic in this method.<a data-type="indexterm" data-primary="ProcessorContext class" id="idm46281550006264" target="_blank" rel="noopener noreferrer"></a> The <code>ProcessorContext</code> that is passed to the <code>init</code> method is extremely useful, and contains many methods that we will explore in this chapter.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO4-3" href="#co_processor_api_CO4-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>process</code> method is called whenever this processor receives a new record. It contains the per-record data transformation/processing logic. In our example, this is where we will add the logic for detecting whether or not wind speeds exceed safe operating levels for our turbine.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO4-4" href="#co_processor_api_CO4-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>close</code> method is invoked by Kafka Streams whenever it is finished with this operator (e.g., during shutdown). This method typically encapsulates any clean up logic you need for your processor and its local resources. However, you should not attempt to cleanup any Kafka Streams–managed resources, like state stores, in this method, since that is handled by the library itself.</p>
            </dd>
          </dl>
          <p>With this interface in mind, let’s implement a <code>Processor</code> that will generate a shutdown signal when wind speeds reach dangerous levels. <a data-type="indexterm" data-primary="Processor interface" data-secondary="implementation that detects dangerous wind speeds" id="idm46281549997272" target="_blank" rel="noopener noreferrer"></a>The code in <a data-type="xref" href="#PROCESSOR_HIGH_WINDS_IMPL" target="_blank" rel="noopener noreferrer">Example&nbsp;7-3</a> shows what our high winds processor looks like.</p>
          <div id="PROCESSOR_HIGH_WINDS_IMPL" data-type="example">
            <h5><span class="label">Example 7-3. </span>A <code>Processor</code> implementation that detects dangerous wind speeds</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting" class="code-small">public class HighWindsFlatmapProcessor
    implements Processor&lt;String, TurbineState, String, TurbineState&gt; { <a class="co" id="co_processor_api_CO5-1" href="#callout_processor_api_CO5-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  private ProcessorContext&lt;String, TurbineState&gt; context;

  @Override
  public void init(ProcessorContext&lt;String, TurbineState&gt; context) { <a class="co" id="co_processor_api_CO5-2" href="#callout_processor_api_CO5-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    this.context = context; <a class="co" id="co_processor_api_CO5-3" href="#callout_processor_api_CO5-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
  }

  @Override
  public void process(Record&lt;String, TurbineState&gt; record) {
    TurbineState reported = record.value();
    context.forward(record); <a class="co" id="co_processor_api_CO5-4" href="#callout_processor_api_CO5-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>

    if (reported.getWindSpeedMph() &gt; 65 &amp;&amp; reported.getPower() == Power.ON) { <a class="co" id="co_processor_api_CO5-5" href="#callout_processor_api_CO5-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
      TurbineState desired = TurbineState.clone(reported); <a class="co" id="co_processor_api_CO5-6" href="#callout_processor_api_CO5-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
      desired.setPower(Power.OFF);
      desired.setType(Type.DESIRED);

      Record&lt;String, TurbineState&gt; newRecord = <a class="co" id="co_processor_api_CO5-7" href="#callout_processor_api_CO5-7"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
        new Record&lt;&gt;(record.key(), desired, record.timestamp());
      context.forward(newRecord); <a class="co" id="co_processor_api_CO5-8" href="#callout_processor_api_CO5-8"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/8.png" alt="8" width="12" height="12"></a>
    }
  }

  @Override
  public void close() {
    // nothing to do <a class="co" id="co_processor_api_CO5-9" href="#callout_processor_api_CO5-9"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/9.png" alt="9" width="12" height="12"></a>
  }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO5-1" href="#co_processor_api_CO5-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Recall that the <code>Processor</code> interface is parameterized with four generics. The first two generics (in this case, <span class="keep-together"><code>Processor&lt;String, TurbineState, ..., ...&gt;</code>)</span> refer to the <span class="keep-together"><em>input</em> key</span> and value types. The last two generics (in this case, <span class="keep-together"><code>Processor&lt;..., ..., String, TurbineState&gt;</code>)</span> refer to the <em>output</em> key and value types.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-2" href="#co_processor_api_CO5-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The generics in the <code>ProcessorContext</code> interface refer to the output key and value types (in this case, <code>ProcessorContext&lt;String, TurbineState&gt;</code>).</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-3" href="#co_processor_api_CO5-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>It is typical to save the processor context as an instance property, as we do here, so that we can access it later (e.g., from the <code>process</code> and/or <code>close</code> methods).</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-4" href="#co_processor_api_CO5-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Whenever you want to send a record to downstream processors, you can call the <code>forward</code> method on the <code>ProcessorContext</code> instance (we have saved this in the <code>context</code> property).<a data-type="indexterm" data-primary="ProcessorContext#forward method" id="idm46281549957096" target="_blank" rel="noopener noreferrer"></a> This method accepts the record that you would like to forward. In our processor implementation, we always want to forward the reported state records, which is why we call <code>context.forward</code> using an unmodified record in this line.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-5" href="#co_processor_api_CO5-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Check to see if our turbine meets the conditions for sending a shutdown signal. In this case, we check that wind speeds exceed a safe threshold (65 mph), and that our turbine is currently powered on.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-6" href="#co_processor_api_CO5-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
            </dt>
            <dd>
              <p>If the previous conditions are met, generate a new record containing a desired power state of <code>OFF</code>. Since we have already sent the original reported state record downstream, and we are now generating a desired state record, this is effectively a type of <code>flatMap</code> operation (our processor has created two output records from one input record).<a data-type="indexterm" data-primary="flatMap operator" id="idm46281549949400" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-7" href="#co_processor_api_CO5-7" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Create an output record that includes the desired state that we saved to the state store. The record key and timestamp are inherited from the input record.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-8" href="#co_processor_api_CO5-8" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/8.png" alt="8" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Call the <code>context.forward</code> method in order to send the new record (the shutdown signal) to downstream processors.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO5-9" href="#co_processor_api_CO5-9" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/9.png" alt="9" width="12" height="12"></a>
            </dt>
            <dd>
              <p>In this processor, there’s no special logic that we need to execute when our processor is closed.</p>
            </dd>
          </dl>
          <p>As you can see, implementing a <code>Processor</code> is pretty straightforward. One interesting thing to call out is that when you build processors like this, you usually don’t need <span class="keep-together">to concern</span> yourself with <em>where</em> the output records will be forwarded to from the <span class="keep-together"><code>Processor</code></span> implementation itself (you can define the dataflow by setting the parent <span class="keep-together">name of</span> a downstream processor). The exception to this is if you use a variation of <span class="keep-together"><code>ProcessorContext#forward</code></span> that accepts a list of downstream processor names, which tells Kafka Streams which <em>child processors</em> to forward the output to.<a data-type="indexterm" data-primary="child processor, forwarding output to" id="idm46281549935368" target="_blank" rel="noopener noreferrer"></a> An example of this is shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">context.forward(newRecord, "some-child-node");</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Whether or not you use this variation of the <code>forward</code> method depends on if you want to broadcast the output to all of the downstream processors or to a specific downstream processor. For example, the DSL’s branch method uses the above variation since it only needs to broadcast its output to a subset of the available downstream processors.</p>
          <p>This completes step 2 of our processor topology (see <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>). Next, we need to implement a stateful stream processor that creates and saves digital twin records in a key-value store.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="creating stateless processors" data-startref="ix_PrAPIIoTdtsstl" id="idm46281549931256" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="creating stateless processors" data-startref="ix_IoTDTScrstlpr" id="idm46281549929688" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateless processors" data-secondary="creating" data-startref="ix_stlprccr" id="idm46281549928360" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Creating Stateful Processors">
        <div class="sect1" id="idm46281550029640">
          <h1>Creating Stateful Processors</h1>
          <p>In <a data-type="xref" href="ch04.html#C4_STATE_STORES" target="_blank" rel="noopener noreferrer">“State Stores”</a>, we learned that stateful operations in Kafka Streams require so-called state stores for maintaining<a data-type="indexterm" data-primary="stateful processors, creating" id="ix_stflprcr" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="creating stateful processors" id="ix_IoTDTScrstfl" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="creating stateful processors" id="ix_PrAPIIoTdtsstfl" target="_blank" rel="noopener noreferrer"></a> some memory of previously seen data. In order to create our digital twin records, we need to combine desired state and recorded state events into a single record. Since these records will arrive at different times for a given wind turbine, we have a stateful requirement of remembering the last recorded and desired state record for each turbine.</p>
          <p>So far in this book, we have mostly focused on using state stores in the DSL. Furthermore, the<a data-type="indexterm" data-primary="state stores" data-secondary="in Processor API" data-secondary-sortas="Processor API" id="ix_ststPrAPI" target="_blank" rel="noopener noreferrer"></a> DSL gives us a couple of different options for using state stores. We can use a default internal state store by simply using a stateful operator without specifying a state store, like so:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">grouped.aggregate(initializer, adder);</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Or we can use the <code>Stores</code> factory class to create a <em>store supplier</em>, and materialize the state store using the <code>Materialized</code> class in conjunction with a stateful operator, as shown <a data-type="indexterm" data-primary="Materialized class" id="idm46281549914952" target="_blank" rel="noopener noreferrer"></a>in the following code:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">KeyValueBytesStoreSupplier storeSupplier =
    Stores.persistentTimestampedKeyValueStore("my-store");

grouped.aggregate(
    initializer,
    adder,
    Materialized.&lt;String, String&gt;as(storeSupplier));</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Using state stores in the Processor API is a little different. Unlike the DSL, the Processor API won’t create an internal state store for you. Therefore, you must always create <em>and</em> connect state stores to the appropriate stream processors yourself when you need to perform stateful operations. Furthermore, while we can still use the <code>Stores</code> factory class, we will use a different set of methods that are available in this class for creating state stores. <a data-type="indexterm" data-primary="StoreSupplier interface" id="idm46281549911160" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Stores factory class" id="idm46281549910456" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="store builders" id="idm46281549909784" target="_blank" rel="noopener noreferrer"></a>Instead of using one of the methods that returns a <em>store supplier</em>, we will use the methods for creating <em>store builders</em>.</p>
          <p>For example, to store the digital twin records, we need a simple key-value store. <a data-type="indexterm" data-primary="key-value stores" data-secondary="store builders" id="idm46281549907832" target="_blank" rel="noopener noreferrer"></a>The factory method for retrieving a key-value store <em>builder</em> is called <code>keyValueStoreBuilder</code>, and the following code demonstrates how we can use this method for creating our digital twin store:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">StoreBuilder&lt;KeyValueStore&lt;String, DigitalTwin&gt;&gt; storeBuilder =
    Stores.keyValueStoreBuilder(
        Stores.persistentKeyValueStore("digital-twin-store"),
        Serdes.String(), <a class="co" id="co_processor_api_CO6-1" href="#callout_processor_api_CO6-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
        JsonSerdes.DigitalTwin()); <a class="co" id="co_processor_api_CO6-2" href="#callout_processor_api_CO6-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist pagebreak-before">
            <dt>
              <a class="co" id="callout_processor_api_CO6-1" href="#co_processor_api_CO6-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>We’ll use the built-in String Serdes for serializing/deserializing keys.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO6-2" href="#co_processor_api_CO6-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Use the Serdes we defined in <a data-type="xref" href="#C7_SERDES" target="_blank" rel="noopener noreferrer">Example&nbsp;7-1</a> for serializing/deserializing values.</p>
            </dd>
          </dl>
          <p>Once you’ve created a store builder for your stateful processor, it’s time to implement the <code>Processor</code> interface. This process is very similar to what we saw when we added a stateless processor in <a data-type="xref" href="#C7_STATELESS_PROCESSORS" target="_blank" rel="noopener noreferrer">“Creating Stateless Processors”</a>. We simply need to use the <code>addProcessor</code> method in the Processor API, as shown in the following code:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">builder.addProcessor(
  "Digital Twin Processor", <a class="co" id="co_processor_api_CO7-1" href="#callout_processor_api_CO7-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  DigitalTwinProcessor::new, <a class="co" id="co_processor_api_CO7-2" href="#callout_processor_api_CO7-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  "High Winds Flatmap Processor", "Desired State Events"); <a class="co" id="co_processor_api_CO7-3" href="#callout_processor_api_CO7-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO7-1" href="#co_processor_api_CO7-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of this stream processor.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO7-2" href="#co_processor_api_CO7-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>A <code>ProcessSupplier</code>, which is a method that can be used to retrieve a <code>Processor</code> instance. We will implement the <code>DigitalTwinProcessor</code> referenced in this line shortly.<a data-type="indexterm" data-primary="ProcessSupplier interface" id="idm46281549878888" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO7-3" href="#co_processor_api_CO7-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Names of the parent processors. By specifying multiple parents, we are effectively performing what would be a <code>merge</code> operation in the DSL.</p>
            </dd>
          </dl>
          <p>Before we implement the <code>DigitalTwinProcessor</code>, let’s go ahead and add our new state store to the topology. <a data-type="indexterm" data-primary="Topology#addStateStore method" id="idm46281549874312" target="_blank" rel="noopener noreferrer"></a>We can do this using the <code>Topology#addStateStore</code> method, and a demonstration of its usage is shown in <a data-type="xref" href="#C7_ADD_SS" target="_blank" rel="noopener noreferrer">Example&nbsp;7-4</a>.</p>
          <div id="C7_ADD_SS" data-type="example">
            <h5><span class="label">Example 7-4. </span>Example usage of the <code>addStateStore</code> method</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">builder.addStateStore(
  storeBuilder, <a class="co" id="co_processor_api_CO8-1" href="#callout_processor_api_CO8-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  "Digital Twin Processor" <a class="co" id="co_processor_api_CO8-2" href="#callout_processor_api_CO8-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
);</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO8-1" href="#co_processor_api_CO8-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>A store builder that can be used to obtain the state store.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO8-2" href="#co_processor_api_CO8-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>We can <em>optionally</em> pass in the name of the processors that should have access to this store. In this case, the <code>DigitalTwinProcessor</code> that we created in the previous code block should have access. We could also have passed in more processor names here if we had multiple processors with shared state. Finally, if we omit this optional argument, we could instead use the <code>Topology#connectProcessorAndState</code> to connect a state store to a processor <em>after</em> the store was added to the topology (instead of at the same time, which is what we’re doing here).<a data-type="indexterm" data-primary="Topology#connectProcessorAndState method" id="idm46281549858056" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
          </dl>
          <p>The last step is to implement our new stateful processor: <code>DigitalTwinProcessor</code>. Like stateless stream processors, we will need to implement the <code>Processor</code> interface. However, this time, our implementation will be slightly more involved since this <span class="keep-together">processor</span> needs to interact with a state store. The code in <a data-type="xref" href="#C7_DT_PROCESSOR" target="_blank" rel="noopener noreferrer">Example&nbsp;7-5</a>, and the annotations that follow it, will describe how to implement a stateful processor.</p>
          <div id="C7_DT_PROCESSOR" data-type="example">
            <h5><span class="label">Example 7-5. </span>A stateful processor for creating digital twin records</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">public class DigitalTwinProcessor
    implements Processor&lt;String, TurbineState, String, DigitalTwin&gt; { <a class="co" id="co_processor_api_CO9-1" href="#callout_processor_api_CO9-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  private ProcessorContext&lt;String, DigitalTwin&gt; context;
  private KeyValueStore&lt;String, DigitalTwin&gt; kvStore;

  @Override
  public void init(ProcessorContext&lt;String, DigitalTwin&gt; context) { <a class="co" id="co_processor_api_CO9-2" href="#callout_processor_api_CO9-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    this.context = context; <a class="co" id="co_processor_api_CO9-3" href="#callout_processor_api_CO9-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    this.kvStore = (KeyValueStore) context.getStateStore("digital-twin-store"); <a class="co" id="co_processor_api_CO9-4" href="#callout_processor_api_CO9-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
  }

  @Override
  public void process(Record&lt;String, TurbineState&gt; record) {
    String key = record.key(); <a class="co" id="co_processor_api_CO9-5" href="#callout_processor_api_CO9-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
    TurbineState value = record.value();
    DigitalTwin digitalTwin = kvStore.get(key); <a class="co" id="co_processor_api_CO9-6" href="#callout_processor_api_CO9-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
    if (digitalTwin == null) { <a class="co" id="co_processor_api_CO9-7" href="#callout_processor_api_CO9-7"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
      digitalTwin = new DigitalTwin();
    }

    if (value.getType() == Type.DESIRED) { <a class="co" id="co_processor_api_CO9-8" href="#callout_processor_api_CO9-8"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/8.png" alt="8" width="12" height="12"></a>
      digitalTwin.setDesired(value);
    } else if (value.getType() == Type.REPORTED) {
      digitalTwin.setReported(value);
    }

    kvStore.put(key, digitalTwin); <a class="co" id="co_processor_api_CO9-9" href="#callout_processor_api_CO9-9"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/9.png" alt="9" width="12" height="12"></a>

    Record&lt;String, DigitalTwin&gt; newRecord =
      new Record&lt;&gt;(record.key(), digitalTwin, record.timestamp()); <a class="co" id="co_processor_api_CO9-10" href="#callout_processor_api_CO9-10"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/10.png" alt="10" width="12" height="12"></a>
    context.forward(newRecord); <a class="co" id="co_processor_api_CO9-11" href="#callout_processor_api_CO9-11"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/11.png" alt="11" width="12" height="12"></a>
  }

  @Override
  public void close() {
    // nothing to do
  }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <dl class="calloutlist pagebreak-before">
            <dt>
              <a class="co" id="callout_processor_api_CO9-1" href="#co_processor_api_CO9-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The first two generics in the <code>Processor</code> interface (in this case, <span class="keep-together"><code>Processor&lt;String, TurbineState, ..., ...&gt;</code>)</span> refer to the <em>input</em> key and value types, and the last two generics <span class="keep-together">(<code>Processor&lt;..., ..., String, DigitalTwin&gt;</code>)</span> refer to the <em>output</em> key and value types.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-2" href="#co_processor_api_CO9-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The generics in the <code>ProcessorContext</code> interface (<code>ProcessorContext&lt;String, DigitalTwin&gt;</code>) refer to the output key and value types</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-3" href="#co_processor_api_CO9-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>We’ll save the <code>ProcessorContext</code> (referenced by the <code>context</code> property) so that we can access it later on.<a data-type="indexterm" data-primary="ProcessorContext.getStateStore method" id="idm46281549817128" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-4" href="#co_processor_api_CO9-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The <code>getStateStore</code> method on the <code>ProcessorContext</code> allows us to retrieve the state store we previously attached to our stream processor. We will interact with this state store directly whenever a record is processed, so we will save it to an instance property named <code>kvStore</code>.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-5" href="#co_processor_api_CO9-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>This line and the one that follows it show how to extract the key and value of the input record</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-6" href="#co_processor_api_CO9-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Use our key-value store to perform a point lookup for the current record’s key. If we have seen this key before, then this will return the previously saved digital twin record.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-7" href="#co_processor_api_CO9-7" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
            </dt>
            <dd>
              <p>If the point lookup didn’t return any results, then we will create a new digital twin record.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-8" href="#co_processor_api_CO9-8" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/8.png" alt="8" width="12" height="12"></a>
            </dt>
            <dd>
              <p>In this code block, we set the appropriate value in the digital twin record depending on the current record’s type (reported state or desired state).</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-9" href="#co_processor_api_CO9-9" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/9.png" alt="9" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Store the digital twin record in the state store directly, using the key-value store’s <code>put</code> method.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-10" href="#co_processor_api_CO9-10" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/10.png" alt="10" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Create an output record that includes the digital twin instance that we saved to the state store. The record key and timestamp are inherited from the input record.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO9-11" href="#co_processor_api_CO9-11" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/11.png" alt="11" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Forward the output record to downstream processors.</p>
            </dd>
          </dl>
          <p>We’ve now implemented the first part of step 3 in our processor topology. The next step (step 3.2 in <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>) will introduce you to a very important feature in the Processor API that has no DSL equivalent. Let’s take a look at how to schedule periodic functions in the DSL.<a data-type="indexterm" data-primary="state stores" data-secondary="in Processor API" data-secondary-sortas="Processor" data-startref="ix_ststPrAPI" id="idm46281549792840" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="creating stateful processors" data-startref="ix_IoTDTScrstfl" id="idm46281549791352" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="creating stateful processors" data-startref="ix_PrAPIIoTdtsstfl" id="idm46281549790104" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="stateful processors, creating" data-startref="ix_stflprcr" id="idm46281549788584" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Periodic Functions with Punctuate">
        <div class="sect1" id="idm46281549926232">
          <h1>Periodic Functions with Punctuate</h1>
          <p>Depending on your use case, you may need to perform some periodic task in your Kafka Streams application.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="periodic functions with punctuator" id="ix_PrAPIIoTdtsperfnc" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="periodic functions with punctuator" id="ix_IoTDTSperfnc" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="periodic functions with punctuator" id="ix_perfnc" target="_blank" rel="noopener noreferrer"></a> This is one area where the Processor API really shines, since it allows you to easily schedule a task using the <code>ProcessorContext#schedule</code> method. <a data-type="indexterm" data-primary="ProcessorContext#schedule method" id="idm46281549781608" target="_blank" rel="noopener noreferrer"></a>Recall that in <a data-type="xref" href="ch06.html#C6_TOMBSTONES" target="_blank" rel="noopener noreferrer">“Tombstones”</a>, we discussed how to keep our state store size to a minimum by removing unneeded records. In this tutorial, we will present another method for cleaning out state stores that leverages this task scheduling capability. Here, we will remove all digital twin records that haven’t seen any state updates in the last seven days. We will assume these turbines are no longer active or are under long-term maintenance, and therefore we’ll delete these records from our key-value store.</p>
          <p>In <a data-type="xref" href="ch05.html#ch5" target="_blank" rel="noopener noreferrer">Chapter&nbsp;5</a>, we showed that when it comes to stream processing, <em>time</em> is a complex subject. When we think about <em>when</em> a periodic function will execute in Kafka Streams, we are reminded of this complexity.<a data-type="indexterm" data-primary="time" data-secondary="periodic function execution and" id="idm46281549777176" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="punctuation types" id="idm46281549776232" target="_blank" rel="noopener noreferrer"></a> There are two <em>punctuation types</em> (i.e., timing strategies) that you can select from, as shown in <a data-type="xref" href="#C7_RECORD_METADATA_1" target="_blank" rel="noopener noreferrer">Table&nbsp;7-2</a>.</p>
          <table id="C7_RECORD_METADATA_1">
            <caption>
              <span class="label">Table 7-2. </span>The types of punctuations that are available in Kafka Streams
            </caption>
            <thead>
              <tr>
                <th>Punctuation type</th>
                <th>Enum</th>
                <th>Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <p>Stream time</p></td>
                <td>
                  <p><span class="keep-together"><code>PunctuationType.STREAM_TIME</code></span></p></td>
                <td>
                  <p>Stream time is the highest timestamp observed for a particular topic-partition.<a data-type="indexterm" data-primary="STREAM_TIME punctuation type" id="idm46281549767432" target="_blank" rel="noopener noreferrer"></a> It is initially unknown and can only increase or stay the same. It advances only when new data is seen, so if you use this punctuation type, then your function will not execute unless data arrives on a continuous basis.<a data-type="indexterm" data-primary="WALL_CLOCK_TIME punctuation type" id="idm46281549766424" target="_blank" rel="noopener noreferrer"></a></p></td>
              </tr>
              <tr>
                <td>
                  <p>Wall clock time</p></td>
                <td>
                  <p><span class="keep-together"><code>PunctuationType.WALL_CLOCK_TIME</code></span></p></td>
                <td>
                  <p>The local system time, which is advanced during each iteration of the consumer poll method.<a data-type="indexterm" data-primary="StreamsConfig#POLL_MS_CONFIG" id="idm46281549762888" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="POLL_MS_CONFIG setting" id="idm46281549762152" target="_blank" rel="noopener noreferrer"></a> The upper bound for how often this gets updated is defined by the <code>S⁠t⁠r⁠e⁠a⁠m⁠s​C⁠o⁠n⁠f⁠i⁠g⁠#⁠P⁠O⁠L⁠L⁠_⁠M⁠S⁠_⁠C⁠O⁠N⁠F⁠I⁠G</code> configuration, which is the maximum amount of time (in milliseconds) the underlying poll method will block as it waits for new data. This means periodic functions will continue to execute regardless of whether or not new messages arrive.</p></td>
              </tr>
            </tbody>
          </table>
          <p>Since we don’t want our periodic function to be tied to new data arriving (in fact, the very presence of this TTL (“time to live”) function is based on the assumption that data may stop arriving), then we will use wall clock time as our punctuation type. Now that we’ve decided which abstraction to use, the rest of the work is simply a <span class="keep-together">matter</span> of scheduling and implementing our TTL function.</p>
          <p class="pagebreak-before">The following code shows our implementation:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">public class DigitalTwinProcessor
    implements Processor&lt;String, TurbineState, String, DigitalTwin&gt; {

  private Cancellable punctuator; <a class="co" id="co_processor_api_CO10-1" href="#callout_processor_api_CO10-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

  // other omitted for brevity

  @Override
  public void init(ProcessorContext&lt;String, DigitalTwin&gt; context) {

    punctuator = this.context.schedule(
        Duration.ofMinutes(5),
        PunctuationType.WALL_CLOCK_TIME, this::enforceTtl); <a class="co" id="co_processor_api_CO10-2" href="#callout_processor_api_CO10-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>

    // ...
  }

  @Override
  public void close() {
    punctuator.cancel(); <a class="co" id="co_processor_api_CO10-3" href="#callout_processor_api_CO10-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
  }

  public void enforceTtl(Long timestamp) {
    try (KeyValueIterator&lt;String, DigitalTwin&gt; iter = kvStore.all()) { <a class="co" id="co_processor_api_CO10-4" href="#callout_processor_api_CO10-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>

      while (iter.hasNext()) {
        KeyValue&lt;String, DigitalTwin&gt; entry = iter.next();
        TurbineState lastReportedState = entry.value.getReported(); <a class="co" id="co_processor_api_CO10-5" href="#callout_processor_api_CO10-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
        if (lastReportedState == null) {
          continue;
        }

        Instant lastUpdated = Instant.parse(lastReportedState.getTimestamp());
        long daysSinceLastUpdate =
          Duration.between(lastUpdated, Instant.now()).toDays(); <a class="co" id="co_processor_api_CO10-6" href="#callout_processor_api_CO10-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
        if (daysSinceLastUpdate &gt;= 7) {
          kvStore.delete(entry.key); <a class="co" id="co_processor_api_CO10-7" href="#callout_processor_api_CO10-7"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
        }
      }
    }
  }

  // ...
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist pagebreak-before">
            <dt>
              <a class="co" id="callout_processor_api_CO10-1" href="#co_processor_api_CO10-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>When we schedule the punctuator function, it will return a <code>Cancellable</code> object that we can use to stop the scheduled function later on.<a data-type="indexterm" data-primary="Cancellable object" id="idm46281549739864" target="_blank" rel="noopener noreferrer"></a> We’ll use an object variable named <code>punctuator</code> to keep track of this object.<a data-type="indexterm" data-primary="punctuator property" id="idm46281549738520" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO10-2" href="#co_processor_api_CO10-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Schedule our periodic function to execute every five minutes based on wall clock time, and save the returned <code>Cancellable</code> under the <code>punctuator</code> property (see the preceding callout).</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO10-3" href="#co_processor_api_CO10-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Cancel the punctuator when our processor is closed (e.g., during a clean shutdown of our Kafka Streams application).</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO10-4" href="#co_processor_api_CO10-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>During each invocation of our function, retrieve each value in our state store. Note that we use a try-with-resources statement to ensure the iterator is closed properly, which will prevent resource leaks.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO10-5" href="#co_processor_api_CO10-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Extract the last reported state of the current record (which corresponds to a physical wind turbine).</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO10-6" href="#co_processor_api_CO10-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Determine how long it’s been (in days) since this turbine last reported its state.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO10-7" href="#co_processor_api_CO10-7" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Delete the record from the state store if it’s stale (hasn’t been updated in at least seven days).</p>
            </dd>
          </dl>
          <div data-type="note" epub:type="note">
            <h6>Note</h6>
            <p>The <code>process</code> function and any punctuations we schedule will be executed in the same thread (i.e., there’s no background thread for punctuations) so you don’t need to worry about concurrency issues.</p>
          </div>
          <p>As you can see, scheduling periodic functions is extremely easy. Next, let’s look at another area where the Processor API shines: accessing record metadata.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="periodic functions with punctuator" data-startref="ix_PrAPIIoTdtsperfnc" id="idm46281549719112" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="periodic functions with punctuator" data-startref="ix_IoTDTSperfnc" id="idm46281549717608" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="periodic functions with punctuator" data-startref="ix_perfnc" id="idm46281549716360" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Accessing Record Metadata">
        <div class="sect1" id="idm46281549787368">
          <h1>Accessing Record Metadata</h1>
          <p>When we use the DSL, we typically only have access to a record’s key and value.<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="accessing record metadata" id="idm46281549713608" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="accessing record metadata" id="idm46281549712376" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="metadata" data-secondary="accessing record metadata in Processor API" id="idm46281549711400" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="records" data-secondary="metadata, accessing in Processor API" id="idm46281549710360" target="_blank" rel="noopener noreferrer"></a> However, there’s a lot of additional information associated with a given record that is not exposed by the DSL, but that we can access using the Processor API. Some of the more prominent examples of record metadata that you might want to access are shown in <a data-type="xref" href="#C7_RECORD_METADATA_2" target="_blank" rel="noopener noreferrer">Table&nbsp;7-3</a>. Note that <a data-type="indexterm" data-primary="context variable (ProcessorContext)" id="idm46281549708200" target="_blank" rel="noopener noreferrer"></a>the <code>context</code> variable in the following table refers to an instance of <code>ProcessorContext</code>, which <a data-type="indexterm" data-primary="ProcessorContext class" id="idm46281549706440" target="_blank" rel="noopener noreferrer"></a>is made available in the <code>init</code> method, as we first showed in <a data-type="xref" href="#PROCESSOR_HIGH_WINDS_IMPL" target="_blank" rel="noopener noreferrer">Example&nbsp;7-3</a>.</p>
          <table id="C7_RECORD_METADATA_2" class="pagebreak-before">
            <caption>
              <span class="label">Table 7-3. </span>Methods for accessing additional record metadata
            </caption>
            <thead>
              <tr>
                <th>Metadata</th>
                <th>Example</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <p>Record headers</p></td>
                <td>
                  <p><code>context.headers()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Offset</p></td>
                <td>
                  <p><code>context.offset()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Partition</p></td>
                <td>
                  <p><code>context.partition()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Timestamp</p></td>
                <td>
                  <p><code>context.timestamp()</code></p></td>
              </tr>
              <tr>
                <td>
                  <p>Topic</p></td>
                <td>
                  <p><code>context.topic()</code></p></td>
              </tr>
            </tbody>
          </table>
          <div data-type="tip">
            <h6>Tip</h6>
            <p>The methods shown in <a data-type="xref" href="#C7_RECORD_METADATA_2" target="_blank" rel="noopener noreferrer">Table&nbsp;7-3</a> pull metadata about the current record, and can be used within the <code>process()</code> function. However, there isn’t a current record when the <code>init()</code> or <code>close()</code> functions are invoked <em>or</em> when a punctuation is executing, so there’s no metadata to extract.</p>
          </div>
          <p>So what could you do with this metadata? One use case is to decorate the record values with additional context before they are written to some downstream system. You can also decorate application logs with this information to help with debugging purposes. For example, if you encounter a malformed record, you could log an error containing the partition and offset of the record in question, and use that as a basis for further troubleshooting.</p>
          <p>Record headers are also interesting, because they <a data-type="indexterm" data-primary="headers (record)" id="idm46281549686440" target="_blank" rel="noopener noreferrer"></a>can be used to inject additional metadata (for example, tracing context that can be used for <a href="https://oreil.ly/ZshyG" target="_blank" rel="noopener noreferrer">distributed tracing</a>). Some examples <a data-type="indexterm" data-primary="distributed tracing" id="idm46281549684840" target="_blank" rel="noopener noreferrer"></a>of how to interact with record headers are shown here:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">Headers headers = context.headers();
headers.add("hello", "world".getBytes(StandardCharsets.UTF_8)); <a class="co" id="co_processor_api_CO11-1" href="#callout_processor_api_CO11-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
headers.remove("goodbye"); <a class="co" id="co_processor_api_CO11-2" href="#callout_processor_api_CO11-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
headers.toArray(); <a class="co" id="co_processor_api_CO11-3" href="#callout_processor_api_CO11-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO11-1" href="#co_processor_api_CO11-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Add a header named <code>hello</code>. This header will be propagated to downstream <span class="keep-together">processors.</span></p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO11-2" href="#co_processor_api_CO11-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Remove a header named <code>goodbye</code>.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO11-3" href="#co_processor_api_CO11-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Get an array of all of the available headers. You could iterate over this and do something with each.</p>
            </dd>
          </dl>
          <p>Finally, if you’d like to trace the origin of a record, the <code>topic()</code> method could be useful for this purpose. In this tutorial, we don’t need to do this, or really access any metadata at all, but you should now have a good understanding of how to access additional metadata in case you encounter a use case that requires it in the future.</p>
          <p>We’re ready to move to the next step of our processor topology and learn how to add a sink processor using the Processor API.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Adding Sink Processors">
        <div class="sect1" id="idm46281549714808">
          <h1>Adding Sink Processors</h1>
          <p>Tackling step 4 of our processor topology (see <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>).<a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="adding sink processors" id="idm46281549663912" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="adding sink processors" id="idm46281549662696" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="sink processors" data-secondary="adding to IoT digital twin service" id="idm46281549661816" target="_blank" rel="noopener noreferrer"></a> involves adding a sink processor that will write all digital twin records to an output topic called <code>digital-twins</code>. This is very simple with the Processor API, so this section will be short.<a data-type="indexterm" data-primary="serialization/deserialization" data-secondary="key and value serializers for sink processor in IoT digital twins service" id="idm46281549660344" target="_blank" rel="noopener noreferrer"></a> We simply need to use the <code>addSink</code> method and specify a few additional parameters, which are detailed in the following code:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">builder.addSink(
  "Digital Twin Sink", <a class="co" id="co_processor_api_CO12-1" href="#callout_processor_api_CO12-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  "digital-twins", <a class="co" id="co_processor_api_CO12-2" href="#callout_processor_api_CO12-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  Serdes.String().serializer(), <a class="co" id="co_processor_api_CO12-3" href="#callout_processor_api_CO12-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
  JsonSerdes.DigitalTwin().serializer(), <a class="co" id="co_processor_api_CO12-4" href="#callout_processor_api_CO12-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
  "Digital Twin Processor"); <a class="co" id="co_processor_api_CO12-5" href="#callout_processor_api_CO12-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO12-1" href="#co_processor_api_CO12-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of this sink node.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO12-2" href="#co_processor_api_CO12-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of the output topic.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO12-3" href="#co_processor_api_CO12-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The key serializer.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO12-4" href="#co_processor_api_CO12-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The value serializer.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO12-5" href="#co_processor_api_CO12-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of one or more parent nodes to connect to this sink.</p>
            </dd>
          </dl>
          <p>That’s all there is to adding a sink processor. Of course, as with most methods in Kafka Streams, there are some additional variations of this method you may want to utilize. <a data-type="indexterm" data-primary="StreamPartitioner class" id="idm46281549635000" target="_blank" rel="noopener noreferrer"></a>For example, one variation allows you to specify a custom <code>StreamPartitioner</code> to map the output record to a partition number.<a data-type="indexterm" data-primary="partitions" data-secondary="StreamPartitioner" id="idm46281549633640" target="_blank" rel="noopener noreferrer"></a> Another variation allows you to exclude the key and value serializers and, instead, use the default serializers that are derived from the <code>DEFAULT_KEY_SERDE_CLASS_CONFIG</code> property.<a data-type="indexterm" data-primary="DEFAULT_KEY_SERDE_CLASS_CONFIG property" id="idm46281549631992" target="_blank" rel="noopener noreferrer"></a> But no matter which overloaded method you use, adding a sink processor is a pretty simple operation.</p>
          <p>Let’s move on to the final step of exposing digital twin records to external services (including the wind turbines themselves, which will synchronize their state to the digital twin records in our state store).</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Interactive Queries">
        <div class="sect1" id="idm46281549630424">
          <h1>Interactive Queries</h1>
          <p>We’ve completed steps 1–4 of our topology (see <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>). The fifth step simply involves exposing the digital twin records using Kafka Streams’ interactive queries feature.<a data-type="indexterm" data-primary="interactive queries" data-secondary="exposing the digital twin records using" id="idm46281549627672" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="interactive queries" id="idm46281549626760" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="interactive queries" id="idm46281549625848" target="_blank" rel="noopener noreferrer"></a> We already covered this topic in detail in <a data-type="xref" href="ch04.html#C4_IQ" target="_blank" rel="noopener noreferrer">“Interactive Queries”</a>, so we won’t go into too much detail or show the entire implementation. However, <a data-type="xref" href="#C7_IQ" target="_blank" rel="noopener noreferrer">Example&nbsp;7-6</a> shows a very simple REST service that uses interactive queries to pull the latest digital twin record. Note that in this example, remote queries aren’t shown, but you can view this example’s source code for a more complete example.</p>
          <p>The important thing to note is that from an interactive query perspective, using the Processor API is exactly the same as using the DSL.<a data-type="indexterm" data-primary="REST APIs" data-secondary="service to expose digital twin records" id="idm46281549622200" target="_blank" rel="noopener noreferrer"></a></p>
          <div id="C7_IQ" data-type="example">
            <h5><span class="label">Example 7-6. </span>An example REST service to expose digital twin records</h5>
            <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
              <pre data-type="programlisting">class RestService {
  private final HostInfo hostInfo;
  private final KafkaStreams streams;

  RestService(HostInfo hostInfo, KafkaStreams streams) {
    this.hostInfo = hostInfo;
    this.streams = streams;
  }

  ReadOnlyKeyValueStore&lt;String, DigitalTwin&gt; getStore() {
    return streams.store(
        StoreQueryParameters.fromNameAndType(
            "digital-twin-store", QueryableStoreTypes.keyValueStore()));
  }

  void start() {
    Javalin app = Javalin.create().start(hostInfo.port());
    app.get("/devices/:id", this::getDevice);
  }

  void getDevice(Context ctx) {
    String deviceId = ctx.pathParam("id");
    DigitalTwin latestState = getStore().get(deviceId);
    ctx.json(latestState);
  }
}</pre>
              <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
            </div>
          </div>
          <p>This completes step 5 of our processor topology (see <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>). Let’s put the various pieces that we’ve constructed together.</p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Putting It All Together">
        <div class="sect1" id="idm46281549617016">
          <h1>Putting It All Together</h1>
          <p>The following code block shows what our full<a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="putting it all together" id="ix_IoTDTSall" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="putting it all together" id="ix_PrAPIIoTdtsall" target="_blank" rel="noopener noreferrer"></a> processor topology looks like at this point:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">Topology builder = new Topology();

builder.addSource( <a class="co" id="co_processor_api_CO13-1" href="#callout_processor_api_CO13-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
    "Desired State Events",
    Serdes.String().deserializer(),
    JsonSerdes.TurbineState().deserializer(),
    "desired-state-events");

builder.addSource( <a class="co" id="co_processor_api_CO13-2" href="#callout_processor_api_CO13-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
    "Reported State Events",
    Serdes.String().deserializer(),
    JsonSerdes.TurbineState().deserializer(),
    "reported-state-events");

builder.addProcessor( <a class="co" id="co_processor_api_CO13-3" href="#callout_processor_api_CO13-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
    "High Winds Flatmap Processor",
    HighWindsFlatmapProcessor::new,
    "Reported State Events");

builder.addProcessor( <a class="co" id="co_processor_api_CO13-4" href="#callout_processor_api_CO13-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
    "Digital Twin Processor",
    DigitalTwinProcessor::new,
    "High Winds Flatmap Processor",
    "Desired State Events");

StoreBuilder&lt;KeyValueStore&lt;String, DigitalTwin&gt;&gt; storeBuilder =
    Stores.keyValueStoreBuilder( <a class="co" id="co_processor_api_CO13-5" href="#callout_processor_api_CO13-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
        Stores.persistentKeyValueStore("digital-twin-store"),
        Serdes.String(),
        JsonSerdes.DigitalTwin());

builder.addStateStore(storeBuilder, "Digital Twin Processor"); <a class="co" id="co_processor_api_CO13-6" href="#callout_processor_api_CO13-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>

builder.addSink( <a class="co" id="co_processor_api_CO13-7" href="#callout_processor_api_CO13-7"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
    "Digital Twin Sink",
    "digital-twins",
    Serdes.String().serializer(),
    JsonSerdes.DigitalTwin().serializer(),
    "Digital Twin Processor");</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO13-1" href="#co_processor_api_CO13-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Create a <em>source processor</em> named <code>Desired State Events</code> that consumes data from the <code>desired-state-events</code> topic.<a data-type="indexterm" data-primary="source processors" data-secondary="creating for IoT digital twin service" id="idm46281549593112" target="_blank" rel="noopener noreferrer"></a> This is the equivalent of a <em>stream</em> in the DSL.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO13-2" href="#co_processor_api_CO13-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Create a <em>source processor</em> named <code>Reported State Events</code> that consumes data from the <code>reported-state-events</code> topic. This is also the equivalent of a <em>stream</em> in the DSL.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO13-3" href="#co_processor_api_CO13-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Add a <em>stream processor</em> named <code>High Winds Flatmap Processor</code> that generates a shutdown signal if high winds are detected.<a data-type="indexterm" data-primary="stream processors" data-secondary="for IoT digital twin service" id="idm46281549583672" target="_blank" rel="noopener noreferrer"></a> This processor receives events from the <code>Reported State Events</code> processor. This would be a <code>flatMap</code> operation in the DSL since there is a 1:N relationship between the number of input and output records for this stream processor. <a data-type="xref" href="#PROCESSOR_HIGH_WINDS_IMPL" target="_blank" rel="noopener noreferrer">Example&nbsp;7-3</a> shows the implementation of this processor.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO13-4" href="#co_processor_api_CO13-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Add a <em>stream processor</em> named <code>Digital Twin Processor</code> that creates digital twin records using data emitted from both the <code>High Winds Flatmap Processor</code> and <code>Desired State Events</code>. This would be a <em>merge</em> operation in the DSL since multiple sources are involved. Furthermore, since this is a stateful processor, this would be the equivalent of an aggregated <em>table</em> in the DSL. <a data-type="xref" href="#C7_DT_PROCESSOR" target="_blank" rel="noopener noreferrer">Example&nbsp;7-5</a> shows the implementation of this processor.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO13-5" href="#co_processor_api_CO13-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Use the <code>Stores</code> factory class to create a <em>store builder</em>, which can be used by Kafka Streams to build persistent key-value stores that are accessible from the <code>Digital Twin Processor</code> node.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO13-6" href="#co_processor_api_CO13-6" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/6.png" alt="6" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Add the state store to the topology and connect it to the <code>Digital Twin</code> <span class="keep-together"><code>Processor</code></span> node.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO13-7" href="#co_processor_api_CO13-7" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/7.png" alt="7" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Create a <em>sink processor</em> named <code>Digital Twin Sink</code> that writes all digital twin records that get emitted from the <code>Digital Twin Processor</code> node to an output topic named <code>digital-twins</code>.<a data-type="indexterm" data-primary="sink processors" data-secondary="creating for IoT digital twin service" id="idm46281549562248" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
          </dl>
          <p>We can now run our application, write some test data to our Kafka cluster, and query our digital twin service. Running our application is no different than what we’ve seen in previous chapters, as you can see from the following code block:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "dev-consumer"); <a class="co" id="co_processor_api_CO14-1" href="#callout_processor_api_CO14-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
// ...

KafkaStreams streams = new KafkaStreams(builder, props); <a class="co" id="co_processor_api_CO14-2" href="#callout_processor_api_CO14-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
streams.start(); <a class="co" id="co_processor_api_CO14-3" href="#callout_processor_api_CO14-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>

Runtime.getRuntime().addShutdownHook(new Thread(streams::close)); <a class="co" id="co_processor_api_CO14-4" href="#callout_processor_api_CO14-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>

RestService service = new RestService(hostInfo, streams); <a class="co" id="co_processor_api_CO14-5" href="#callout_processor_api_CO14-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
service.start();</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO14-1" href="#co_processor_api_CO14-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Configure the Kafka Streams application. This works the same way as we saw when building applications with the DSL. Most of the configs are omitted for brevity’s sake.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO14-2" href="#co_processor_api_CO14-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Instantiate a new <code>KafkaStreams</code> instance that can be used to execute our <span class="keep-together">topology</span>.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO14-3" href="#co_processor_api_CO14-3" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/3.png" alt="3" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Start the Kafka Streams application.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO14-4" href="#co_processor_api_CO14-4" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/4.png" alt="4" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Add a shutdown hook to gracefully stop the Kafka Streams application when a global shutdown signal is received.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO14-5" href="#co_processor_api_CO14-5" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/5.png" alt="5" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Instantiate and (on the following line) start the REST service, which we implemented in <a data-type="xref" href="#C7_IQ" target="_blank" rel="noopener noreferrer">Example&nbsp;7-6</a>.</p>
            </dd>
          </dl>
          <p>Our application now reads from multiple source topics, but we will only produce test data to the <code>reported-state-events</code> topic in this book (see the <a href="https://oreil.ly/LySHt" target="_blank" rel="noopener noreferrer">source code</a> for a more complete example). To test that our application generates a shutdown signal, we will include one record that contains a wind speed that exceeds our safe operating threshold of 65 mph. The following code shows the test data we will produce, with record keys and values separated by <code>|</code> and timestamps omitted for brevity:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting" class="code-quiet-shrink">1|{"timestamp": "...", "wind_speed_mph": 40, "power": "ON", "type": "REPORTED"}
1|{"timestamp": "...", "wind_speed_mph": 42, "power": "ON", "type": "REPORTED"}
1|{"timestamp": "...", "wind_speed_mph": 44, "power": "ON", "type": "REPORTED"}
1|{"timestamp": "...", "wind_speed_mph": 68, "power": "ON", "type": "REPORTED"} <a class="co" id="co_processor_api_CO15-1" href="#callout_processor_api_CO15-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO15-1" href="#co_processor_api_CO15-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>This sensor data shows a wind speed of 68 mph. When our application sees this record, it should generate a shutdown signal by creating a new <code>TurbineState</code> record, with a desired power state of OFF.</p>
            </dd>
          </dl>
          <p>If we produce this test data into the <code>reported-state-events</code> topic and then query our digital twin service, we will see that our Kafka Streams application not only processed the reported states of our windmill, but also produced a desired state record where the power is set to <code>OFF</code>. The following code block shows an example request and response to our REST service:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">$ curl localhost:7000/devices/1 | jq '.'

{
  "desired": {
    "timestamp": "2020-11-23T09:02:01.000Z",
    "windSpeedMph": 68,
    "power": "OFF",
    "type": "DESIRED"
  },
  "reported": {
    "timestamp": "2020-11-23T09:02:01.000Z",
    "windSpeedMph": 68,
    "power": "ON",
    "type": "REPORTED"
  }</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <p>Now, our wind turbines can query our REST service and synchronize their own state with the desired state that was either captured (via the <code>desired-state-events</code> topic) or enforced (using the high winds processor to send a shutdown signal) by Kafka Streams.<a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="putting it all together" data-startref="ix_IoTDTSall" id="idm46281549522376" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="putting it all together" data-startref="ix_PrAPIIoTdtsall" id="idm46281549521064" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Combining the Processor API with the DSL">
        <div class="sect1" id="idm46281549616344">
          <h1>Combining the Processor API with the DSL</h1>
          <p>We’ve verified that our application works. However, if you look closely at our code, you’ll see that only one of the topology steps requires the low-level access that the Processor API offers. <a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="combining Processor API with DSL" id="ix_IoTDTSDSL" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="combining with Processor API in IoT digital twin service" id="ix_DSLPrAPI" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="combining with DSL in IoT digital twin service" id="ix_PrAPIDSL" target="_blank" rel="noopener noreferrer"></a>The step I’m referring to is the <code>Digital Twin Processor</code> step (see step 3 in <a data-type="xref" href="#C7_TOPOLOGY" target="_blank" rel="noopener noreferrer">Figure&nbsp;7-1</a>), which leverages an important feature of the Processor API: the ability to schedule periodic functions.</p>
          <p>Since Kafka Streams allows us to combine the Processor API and the DSL, we can easily refactor our application to <em>only</em> use the Processor API for the <code>Digital Twin Processor</code> step, and to use the DSL for everything else. The biggest benefit of performing this kind of refactoring is that other stream processing steps can be simplified. In this tutorial, the <code>High Winds Flatmap Processor</code> offers the biggest opportunity of simplification, but in larger applications, this kind of refactoring reduces complexity on an even greater scale.</p>
          <p>The first two steps in our processor topology (registering the source processors <span class="keep-together">and generating</span> a shutdown signal using a <code>flatMap</code>-like operation) can be refactored <span class="keep-together">using operators</span> we’ve already discussed in this book. Specifically, we can make the <span class="keep-together">following</span> changes:</p>
          <table id="C7_REFACTOR_1" class="border code-table-fix">
            <thead>
              <tr>
                <th>Processor API</th>
                <th>DSL</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
Topology builder = new Topology();

builder.addSource(
  "Desired State Events",
  Serdes.String().deserializer(),
  JsonSerdes.TurbineState().deserializer(),
  "desired-state-events");

builder.addSource(
  "Reported State Events",
  Serdes.String().deserializer(),
  JsonSerdes.TurbineState().deserializer(),
  "reported-state-events");

builder.addProcessor(
  "High Winds Flatmap Processor",
  HighWindsFlatmapProcessor::new,
  "Reported State Events");
  </pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
StreamsBuilder builder = new StreamsBuilder();

KStream&lt;String, TurbineState&gt; desiredStateEvents =
  builder.stream("desired-state-events",
    Consumed.with(
      Serdes.String(),
      JsonSerdes.TurbineState()));

KStream&lt;String, TurbineState&gt; highWinds =
  builder.stream("reported-state-events",
    Consumed.with(
      Serdes.String(),
      JsonSerdes.TurbineState()))
  .flatMapValues((key, reported) -&gt; ... )
  .merge(desiredStateEvents);
</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>As you can see, these changes are pretty straightforward. However, step 3 in our topology actually does require the Processor API, so how do we go about combining the DSL and Processor API in this step? The answer lies in a special set of DSL operators, which we will explore next.<a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="combining Processor API with DSL" data-startref="ix_IoTDTSDSL" id="idm46281549500104" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="combining with Processor API in IoT digital twin service" data-startref="ix_DSLPrAPI" id="idm46281549498808" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="combining with DSL in IoT digital twin service" data-startref="ix_PrAPIDSL" id="idm46281549497528" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Processors and Transformers">
        <div class="sect1" id="C7_PROCESSOR_VS_TRANSFORMER">
          <h1>Processors and Transformers</h1>
          <p>The DSL includes a special set of operators that allow us to use the Processor API whenever we need <a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="using processors and transformers" id="ix_IoTDTSprctrns" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processors and transformers (operators)" id="ix_prctrns" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="processors and transformers" id="ix_DSLprctrns" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="using processors and transformers" id="ix_PrAPIprctrns" target="_blank" rel="noopener noreferrer"></a>lower-level access to state stores, record metadata, and processor context (which can be used for scheduling periodic functions, among other things). These special operators are broken into two categories: <em>processors</em> and <em>transformers</em>. The following outlines the distinction between these two groups:</p>
          <ul>
            <li>
              <p>A <em>processor</em> is a <em>terminal operation</em> (meaning it returns void and downstream operators cannot be chained), and the computational logic must be implemented using the <code>Processor</code> interface (which we first discussed in <a data-type="xref" href="#C7_STATELESS" target="_blank" rel="noopener noreferrer">“Adding Stateless Stream Processors”</a>).<a data-type="indexterm" data-primary="Processor interface" data-secondary="process operator and" id="idm46281549484600" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="terminal operations" id="idm46281549483624" target="_blank" rel="noopener noreferrer"></a> Processors should be used whenever you need to leverage the Processor API from the DSL, but don’t need to chain any downstream operators.<a data-type="indexterm" data-primary="process operator" id="idm46281549482664" target="_blank" rel="noopener noreferrer"></a> There is currently only one variation of this type of operator, as shown in the following table:</p>
              <table id="C7_P_OPS">
                <thead>
                  <tr>
                    <th>DSL operator</th>
                    <th>Interface to implement</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <div>
                        <p><code>process</code></p>
                      </div></td>
                    <td>
                      <div>
                        <p><code>Processor</code></p>
                      </div></td>
                    <td>
                      <div>
                        <p>Apply a <code>Processor</code> to each record at a time</p>
                      </div></td>
                  </tr>
                </tbody>
              </table></li>
            <li>
              <p><em>Transformers</em> are a more diverse<a data-type="indexterm" data-primary="transformers" id="ix_trnsf" target="_blank" rel="noopener noreferrer"></a> set of operators and can return one or more records (depending on which variation you use), and are therefore more optimal if you need to chain a downstream operator. <a data-type="indexterm" data-primary="transform operator" id="idm46281549471688" target="_blank" rel="noopener noreferrer"></a>The variations of the transform operator are shown in <a data-type="xref" href="#C7_T_OPS" target="_blank" rel="noopener noreferrer">Table&nbsp;7-4</a>.</p>
              <table id="C7_T_OPS">
                <caption>
                  <span class="label">Table 7-4. </span>Various transform operators that are available in Kafka Streams
                </caption>
                <thead>
                  <tr>
                    <th>DSL operator</th>
                    <th>Interface to implement</th>
                    <th>Description</th>
                    <th>Input/output ratio</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      <p><code>transform</code></p></td>
                    <td>
                      <p><code>Transformer</code></p></td>
                    <td>
                      <p>Apply a <code>Transformer</code> to each record, generating one or more output records. Single records can be returned from the <code>Transformer#transform</code> method, and multiple values can be emitted using <code>ProcessorContext#forward</code>.<sup><a data-type="noteref" id="idm46281549461976-marker" href="ch07.html#idm46281549461976" target="_blank" rel="noopener noreferrer">a</a></sup> The transformer has access to the record key, value, metadata, processor context (which can be used for scheduling periodic functions), and connected state stores.</p></td>
                    <td>
                      <p>1:N</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>transformValues</code></p></td>
                    <td>
                      <p><span class="keep-together"><code>ValueTransformer</code></span></p></td>
                    <td>
                      <p>Similar to <code>transform</code>, but <em>does not</em> have access to the record key and <em>cannot</em> forward multiple records using <code>ProcessorContext#forward</code> (if you try to forward multiple records, you will get a <code>StreamsException</code>).<a data-type="indexterm" data-primary="ProcessorContext#forward method" id="idm46281549453656" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="transformValues operator" id="idm46281549452856" target="_blank" rel="noopener noreferrer"></a> Since state store operations are key-based, this operator is not ideal if you need to perform lookups against a state store. Furthermore, output records will have the same key as the input records, and downstream auto-repartitioning will not be triggered since the key cannot be modified (which is advantageous since it can help avoid network trips).</p></td>
                    <td>
                      <p>1:1</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>transformValues</code></p></td>
                    <td>
                      <p><span class="keep-together"><code>ValueTransformerWithKey</code></span></p></td>
                    <td>
                      <p>Similar to <code>transform</code>, but the record key is <em>read-only</em> and <em>should not be modified</em>. <span class="keep-together">Also, you</span> cannot forward multiple records using <span class="keep-together"><code>ProcessorContext#forward</code></span> (if you try to forward multiple records, you will get a <code>StreamsException</code>).</p></td>
                    <td>
                      <p>1:1</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>flatTransform</code></p></td>
                    <td>
                      <p><code>Transformer</code> (with an iterable return value)</p></td>
                    <td>
                      <p>Similar to <code>transform</code>, but instead of relying on <code>ProcessorContext#forward</code> to return multiple records, you can simply return a collection of values.<a data-type="indexterm" data-primary="flatTransform operator" id="idm46281549440760" target="_blank" rel="noopener noreferrer"></a> For this reason, it’s recommended to use <code>flatTransform</code> over <code>transform</code> if you need to emit multiple records, since this method is type-safe while the latter is not (since it relies on <code>ProcessorContext#forward</code>).</p></td>
                    <td>
                      <p>1:N</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>flatTransformValues</code></p></td>
                    <td>
                      <p><span class="keep-together"><code>ValueTransformer</code></span> (with an iterable return value)</p></td>
                    <td>
                      <p>Apply a <code>Transformer</code> to each record, returning<a data-type="indexterm" data-primary="flatTransformValues operator" id="idm46281549434600" target="_blank" rel="noopener noreferrer"></a> one or more output records directly from the <code>ValueTransformer#transform</code> method.</p></td>
                    <td>
                      <p>1:N</p></td>
                  </tr>
                  <tr>
                    <td>
                      <p><code>flatTransformValues</code></p></td>
                    <td>
                      <p><span class="keep-together"><code>ValueTransformerWithKey</code></span> (with an iterable return value)</p></td>
                    <td>
                      <p>A stateful variation of <code>flatTransformValues</code> in which a read-only key is passed to the <code>transform</code> method, which can be used for state lookups. One or more output records are returned directly from the <code>ValueTransformerWithKey#transform</code> method.</p></td>
                    <td>
                      <p>1:N</p></td>
                  </tr>
                </tbody>
                <tbody>
                  <tr class="footnotes">
                    <td colspan="4">
                      <p data-type="footnote" id="idm46281549461976"><sup><a href="ch07.html#idm46281549461976-marker" target="_blank" rel="noopener noreferrer">a</a></sup> Though 1:N transformations are technically supported, <code>transform</code> is better for 1:1 or 1:0 transformations in which a single record is returned directly, since the <code>ProcessorContext#forward</code> approach is not type-safe. Therefore, if you need to forward multiple records from your transform, <code>flatTransform</code> is recommended instead, since it is type-safe.</p></td>
                  </tr>
                </tbody>
              </table></li>
          </ul>
          <p>No matter which variation you choose, if your operator is stateful, you will need to connect the state store to your topology builder before adding your new operator.<a data-type="indexterm" data-primary="stateful operators" data-secondary="transformers" id="idm46281549426552" target="_blank" rel="noopener noreferrer"></a> Since we are refactoring our stateful <code>Digital Twin Processor</code> step, let’s go ahead and do that:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">StoreBuilder&lt;KeyValueStore&lt;String, DigitalTwin&gt;&gt; storeBuilder =
  Stores.keyValueStoreBuilder(
      Stores.persistentKeyValueStore("digital-twin-store"),
      Serdes.String(),
      JsonSerdes.DigitalTwin());

builder.addStateStore(storeBuilder); <a class="co" id="co_processor_api_CO16-1" href="#callout_processor_api_CO16-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO16-1" href="#co_processor_api_CO16-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>In <a data-type="xref" href="#C7_ADD_SS" target="_blank" rel="noopener noreferrer">Example&nbsp;7-4</a>, we discussed an optional second parameter to the <code>Topology#addStateStore</code> method, which specifies the processor names that should be connected with the state store. Here, we omit the second parameter, so this state store is <em>dangling</em> (though we will connect it in the next code block).<a data-type="indexterm" data-primary="dangling state store" id="idm46281549417288" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Topology#addStateStore method" id="idm46281549416584" target="_blank" rel="noopener noreferrer"></a></p>
            </dd>
          </dl>
          <p>Now, we need to make a decision. Do we use a processor or transformer for refactoring the <code>Digital Twin Processor</code> step? Looking at the definitions in the preceding tables, you may be tempted to use the <code>process</code> operator since we already implemented the <code>Processor</code> interface in the pure Processor API version of our app (see <a data-type="xref" href="#C7_DT_PROCESSOR" target="_blank" rel="noopener noreferrer">Example&nbsp;7-5</a>). If we were to take this approach (which is problematic for reasons we’ll discuss shortly), we’d end up with the following implementation:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">highWinds.process(
  DigitalTwinProcessor::new, <a class="co" id="co_processor_api_CO17-1" href="#callout_processor_api_CO17-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
  "digital-twin-store"); <a class="co" id="co_processor_api_CO17-2" href="#callout_processor_api_CO17-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a></pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO17-1" href="#co_processor_api_CO17-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>A <code>ProcessSupplier</code>, which is used to retrieve an instance of our <code>DigitalTwinProcessor</code>.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO17-2" href="#co_processor_api_CO17-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>The name of the state store that our processor will interact with.</p>
            </dd>
          </dl>
          <p>Unfortunately, this isn’t ideal because we need to connect a sink processor to this node, and the <code>process</code> operator is a terminal operation. Instead, one of the transformer operators would work better here since it allows us to easily connect a sink processor (as we’ll see shortly). Now, looking at <a data-type="xref" href="#C7_T_OPS" target="_blank" rel="noopener noreferrer">Table&nbsp;7-4</a>, let’s find an operator that meets our requirements:</p>
          <ul>
            <li>
              <p>Each input record will always produce one output record (1:1 mapping)</p></li>
            <li>
              <p>We need read-only access to the record key since we’re performing point lookups in our state store, but do not need to modify the key in any way</p></li>
          </ul>
          <p>The operator that best fits these requirements is <code>transformValues</code> (the variation that uses a <code>ValueTransformerWithKey</code>). We’ve already implemented the computational logic for this step using a <code>Processor</code> (see <a data-type="xref" href="#C7_DT_PROCESSOR" target="_blank" rel="noopener noreferrer">Example&nbsp;7-5</a>), so we just need to implement the <code>ValueTransformerWithKey</code> interface and copy the <a data-type="indexterm" data-primary="ValueTransformerWithKey interface" id="idm46281549394248" target="_blank" rel="noopener noreferrer"></a>logic from the <code>process</code> method in <a data-type="xref" href="#C7_DT_PROCESSOR" target="_blank" rel="noopener noreferrer">Example&nbsp;7-5</a> to the <code>transform</code> method, shown in the following. Most of the code has been omitted because it’s the same as the processor implementation. The changes are highlighted in the annotations following this example:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">public class DigitalTwinValueTransformerWithKey
    implements ValueTransformerWithKey&lt;String, TurbineState, DigitalTwin&gt; { <a class="co" id="co_processor_api_CO18-1" href="#callout_processor_api_CO18-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>

  @Override
  public void init(ProcessorContext context) {
    // ...
  }

  @Override
  public DigitalTwin transform(String key, TurbineState value) {
    // ...
    return digitalTwin; <a class="co" id="co_processor_api_CO18-2" href="#callout_processor_api_CO18-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
  }

  @Override
  public void close() {
    // ...
  }

  public void enforceTtl(Long timestamp) {
    // ...
  }
}</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
          <dl class="calloutlist">
            <dt>
              <a class="co" id="callout_processor_api_CO18-1" href="#co_processor_api_CO18-1" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/1.png" alt="1" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Implement the <code>ValueTransformerWithKey</code> interface. <code>String</code> refers to the key type, <code>TurbineState</code> refers to the value type of the input record, and <code>DigitalTwin</code> refers to the value type of the output record.</p>
            </dd>
            <dt>
              <a class="co" id="callout_processor_api_CO18-2" href="#co_processor_api_CO18-2" target="_blank" rel="noopener noreferrer"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492062486/files/assets/2.png" alt="2" width="12" height="12"></a>
            </dt>
            <dd>
              <p>Instead of using <code>context.forward</code> to send records to downstream processors, we can return the record directly from the <code>transform</code> method. As you can see, this is already feeling much more DSL-like.</p>
            </dd>
          </dl>
          <p>With our transformer implementation in place, we can add the following line to our application:</p>
          <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
            <pre data-type="programlisting">highWinds
  .transformValues(DigitalTwinValueTransformerWithKey::new, "digital-twin-store")
  .to("digital-twins", Produced.with(Serdes.String(), JsonSerdes.DigitalTwin()));</pre>
            <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
          </div>
        </div>
      </section>
      <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Putting It All Together: Refactor">
        <div class="sect1" id="idm46281549495560">
          <h1>Putting It All Together: Refactor</h1>
          <p>Now that we’ve discussed the individual<a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="using processors and transformers" data-startref="ix_IoTDTSprctrns" id="idm46281549375192" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="processors and transformers (operators)" data-startref="ix_prctrns" id="idm46281549373912" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="transformers" data-startref="ix_trnsf" id="idm46281549372952" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="processors and transformers" data-startref="ix_DSLprctrns" id="idm46281549372008" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="using processors and transformers" data-startref="ix_PrAPIprctrns" id="idm46281549370760" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="DSL refactor, putting it all together" id="ix_IoTDTSDSLref" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="DSL refactor, putting it all together" id="ix_PrAPIDSLref" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="refactor of IoT digital twin service" id="ix_DSLrefac" target="_blank" rel="noopener noreferrer"></a> steps in our DSL refactor, let’s take a look at the two implementations of our application side by side, as seen in <a data-type="xref" href="#C7_REFACTOR_1_2" target="_blank" rel="noopener noreferrer">Table&nbsp;7-5</a>.</p>
          <table id="C7_REFACTOR_1_2" class="code-table-fix-2 border">
            <caption>
              <span class="label">Table 7-5. </span>Two different implementations of our digital twin topology
            </caption>
            <thead>
              <tr>
                <th>Processor API only</th>
                <th>DSL + Processor API</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
Topology builder = new Topology();

builder.addSource(
  "Desired State Events",
  Serdes.String().deserializer(),
  JsonSerdes.TurbineState().deserializer(),
  "desired-state-events");

builder.addSource(
  "Reported State Events",
  Serdes.String().deserializer(),
  JsonSerdes.TurbineState().deserializer(),
  "reported-state-events");

builder.addProcessor(
  "High Winds Flatmap Processor",
  HighWindsFlatmapProcessor::new,
  "Reported State Events");

builder.addProcessor(
  "Digital Twin Processor",
  DigitalTwinProcessor::new,
  "High Winds Flatmap Processor",
  "Desired State Events");

StoreBuilder&lt;KeyValueStore&lt;String, DigitalTwin&gt;&gt;
  storeBuilder =
    Stores.keyValueStoreBuilder(
        Stores.persistentKeyValueStore(
          "digital-twin-store"),
        Serdes.String(),
        JsonSerdes.DigitalTwin());

builder.addStateStore(storeBuilder,
  "Digital Twin Processor");

builder.addSink(
  "Digital Twin Sink",
  "digital-twins",
  Serdes.String().serializer(),
  JsonSerdes.DigitalTwin().serializer(),
  "Digital Twin Processor");
      </pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
                <td>
                  <div data-testid="custom pre block" class="orm-ChapterReader-codeSnippetContainer">
                    <pre data-type="programlisting">
StreamsBuilder builder = new StreamsBuilder();

KStream&lt;String, TurbineState&gt; desiredStateEvents =
  builder.stream("desired-state-events",
    Consumed.with(
      Serdes.String(),
      JsonSerdes.TurbineState()));

KStream&lt;String, TurbineState&gt; highWinds =
  builder.stream("reported-state-events",
    Consumed.with(
      Serdes.String(),
      JsonSerdes.TurbineState()))
  .flatMapValues((key, reported) -&gt; ... )
  .merge(desiredStateEvents);




  // empty space to align next topology step





StoreBuilder&lt;KeyValueStore&lt;String, DigitalTwin&gt;&gt;
  storeBuilder =
    Stores.keyValueStoreBuilder(
        Stores.persistentKeyValueStore(
          "digital-twin-store"),
        Serdes.String(),
        JsonSerdes.DigitalTwin());

builder.addStateStore(storeBuilder);

highWinds
  .transformValues(
    DigitalTwinValueTransformerWithKey::new,
    "digital-twin-store")
  .to("digital-twins",
    Produced.with(
      Serdes.String(),
      JsonSerdes.DigitalTwin()));
</pre>
                    <div class="orm-ChapterReader-snippetButtonContainer orm-ChapterReader-abTest"></div>
                  </div></td>
              </tr>
            </tbody>
          </table>
          <p>Either implementation is perfectly fine. But going back to something I mentioned earlier, you don’t want to introduce additional complexity unless you have a good reason for doing so.</p>
          <p class="pagebreak-before"><a data-type="indexterm" data-primary="operators" data-secondary="leveraging DSL operators for using Processor API" id="idm46281549355416" target="_blank" rel="noopener noreferrer"></a>The benefits of the hybrid DSL + Processor API implementation are:</p>
          <ul>
            <li>
              <p>It’s easier to construct a mental map of your dataflow by chaining operators instead of having to define the relationship between processors using node names and parent names.<a data-type="indexterm" data-primary="lambda support for DSL operators" id="idm46281549352728" target="_blank" rel="noopener noreferrer"></a></p></li>
            <li>
              <p>The DSL has lambda support for most operators, which can be beneficial for succinct transformations (the Processor API requires you to implement the <span class="keep-together"><code>Processor</code></span> interface, even for simple operations, which can be tedious).</p></li>
            <li>
              <p>Although we didn’t need to rekey any records in this tutorial, the method for doing this in the Processor API is much more tedious. You not only need to implement the <code>Processor</code> interface for a simple rekey operation, but you also have to handle the rewrite to an intermediate repartition topic (this involves adding an additional sink and source processor explicitly, which can lead to unnecessarily complex code).</p></li>
            <li>
              <p>The DSL operators give us a standard vocabulary for defining what happens at a given stream processing step. For example, we can infer that a <code>flatMap</code> operator may produce a different number of records than the input, without knowing anything else about the computational logic. On the other hand, the Processor API makes it easy to disguise the nature of a given <code>Processor</code> implementation, which hurts code readability and can have a negative impact on maintenance.</p></li>
            <li>
              <p>The DSL also gives us a common vocabulary for different types of streams. These include pure record streams, local aggregated streams (which we usually refer to as tables), and global aggregated streams (which we refer to as global tables).</p></li>
          </ul>
          <p>Therefore, I usually recommend leveraging the DSL’s special set of operators for using the Processor API whenever you need lower-level access, as opposed to implementing an application purely in the Processor API.<a data-type="indexterm" data-primary="IoT digital twin service tutorial" data-secondary="DSL refactor, putting it all together" data-startref="ix_IoTDTSDSLref" id="idm46281549344360" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="DSL (domain-specific language)" data-secondary="refactor of IoT digital twin service" data-startref="ix_DSLrefac" id="idm46281549343144" target="_blank" rel="noopener noreferrer"></a><a data-type="indexterm" data-primary="Processor API" data-secondary="IoT digital twin service tutorial" data-tertiary="DSL refactor, putting it all together" data-startref="ix_PrAPIDSLref" id="idm46281549341896" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <section data-type="sect1" data-pdf-bookmark="Summary">
        <div class="sect1" id="idm46281549340248">
          <h1>Summary</h1>
          <p>In this chapter, we learned how to use the Processor API to gain lower-level access to Kafka records and Kafka Streams’ processor context. We also discussed a useful feature of the Processor API that allows us to schedule periodic functions, and the various notions of time that can be used when defining a punctuator for these scheduled functions. Finally, we showed that combining the Processor API and high-level DSL is a great way to leverage the benefits of both APIs. In the next chapter, we will start exploring ksqlDB, which will take us to the opposite side of the spectrum in terms of simplicity (it is the simplest option for building stream processing applications that we will discuss in this book, and arguably the simplest option, period).<a data-type="indexterm" data-primary="Processor API" data-startref="ix_PrAPI" id="idm46281549338216" target="_blank" rel="noopener noreferrer"></a></p>
        </div>
      </section>
      <div data-type="footnotes">
        <p data-type="footnote" id="idm46281550242520"><sup><a href="ch07.html#idm46281550242520-marker" target="_blank" rel="noopener noreferrer">1</a></sup> The DSL is expressive and problem-framing is easy when we take advantage of its built-in operators. Losing some of this expressiveness and the operators themselves can make our problem-framing less standardized, and our solutions harder to communicate.</p>
        <p data-type="footnote" id="idm46281550241560"><sup><a href="ch07.html#idm46281550241560-marker" target="_blank" rel="noopener noreferrer">2</a></sup> For example, committing too aggressively or accessing state stores in a way that could impact performance.</p>
        <p data-type="footnote" id="idm46281550231368"><sup><a href="ch07.html#idm46281550231368-marker" target="_blank" rel="noopener noreferrer">3</a></sup> In this chapter, we use the broader term IoT, even when we’re talking about use cases that have industrial applications. Also, a notable example of where digital twins are used is Tesla Motors. “Tesla creates a digital twin of every car it sells. Tesla then updates software based on individual vehicles’ sensor data and uploads updates to its products.” See the following link for more information: <a href="https://oreil.ly/j6_mj" target="_blank" rel="noopener noreferrer"><em class="hyperlink">https://oreil.ly/j6_mj</em></a>.</p>
        <p data-type="footnote" id="idm46281550223608"><sup><a href="ch07.html#idm46281550223608-marker" target="_blank" rel="noopener noreferrer">4</a></sup> Usually, an interaction means either getting the latest state of a device, or changing the state of a device by updating one of the mutable state properties, like the power status.</p>
        <p data-type="footnote" id="idm46281550216936"><sup><a href="ch07.html#idm46281550216936-marker" target="_blank" rel="noopener noreferrer">5</a></sup> In IoT use cases, sensor data is often sent over a protocol called MQTT. One method for getting this data into Kafka would be to use <a href="https://oreil.ly/pTRmb" target="_blank" rel="noopener noreferrer">Confluent’s MQTT Kafka connector</a>.</p>
        <p data-type="footnote" id="idm46281550203368"><sup><a href="ch07.html#idm46281550203368-marker" target="_blank" rel="noopener noreferrer">6</a></sup> In practice, the wind speed threshold could be two-sided. While higher wind speeds introduce dangerous operating conditions, low wind speeds may not justify the cost of running the turbine.</p>
        <p data-type="footnote" id="idm46281550197448"><sup><a href="ch07.html#idm46281550197448-marker" target="_blank" rel="noopener noreferrer">7</a></sup> For another method, see our discussion of tombstones in <a data-type="xref" href="ch06.html#C6_TOMBSTONES" target="_blank" rel="noopener noreferrer">“Tombstones”</a>.</p>
      </div>
    </div>
    <script>
  document.body.parentElement.style.font = "16px/1.44 'Noto Serif'";
  document.body.parentElement.style.color = '#333';
  document.body.style.textRendering = 'optimizeLegibility';

  (function() {
    let toc = `<div id="toc">
      <label for="toc-checkbox" style="cursor: pointer">Table of Contents</label>
      <input id="toc-checkbox" type="checkbox" style="display: none">
      <div>`;
    let i = 1;
    document.querySelectorAll('h1').forEach(h => {
      h.setAttribute('id', 'ref' + i);
      if (h.parentElement.classList.contains('chapter'))
        toc += `<a href="#ref${i}">${h.innerText}</a>`;
      else
        toc += `<a href="#ref${i}" class="h2">${h.innerText}</a>`;
      i += 1;
    });
    toc += `</div></div>`;
    document.body.insertAdjacentHTML("beforeend", toc);
  })();
</script>
  </body>
</html>